{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OLIxEDq6VhvZ",
    "mdEditEnable": false
   },
   "source": [
    "## 嵌入层(Embedding)\n",
    "\n",
    "\n",
    "<img src=\"data/logo.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "迄今为止，我们使用了 一组独热编码过的n维数组来表示文本，每个索引都对应了一个token，每个索引处的值都表示了和对应单词在句子中出现的次数。这种方法会完全丢失输入中的结构性信息。\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
    " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
    " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
    " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]```\n",
    " \n",
    "我们还用过一种one-hot编码来表示输入，其中每个token都由一个n维数组表示。\n",
    " \n",
    " ```python\n",
    "[[0. 0. 0. ... 0. 0. 0.]\n",
    " [0. 0. 1. ... 0. 0. 0.]\n",
    " [0. 0. 0. ... 0. 0. 0.]\n",
    " ...\n",
    " [0. 0. 0. ... 0. 0. 0.]\n",
    " [0. 0. 0. ... 0. 0. 0.]\n",
    " [0. 0. 0. ... 0. 0. 0.]]\n",
    "```\n",
    "\n",
    "用这种方法，输入中的结构性信息得以保存，但它有两个主要缺点。当词汇表很大时，每个token的表示长度也会相应变得巨大，导致运算量的加大。同时，文本中的结构信息被保存了下来，但token的实际表示和其他token之间的关系并没有被存下来。\n",
    "\n",
    "在这个课程项目中，我们会了解学习 **嵌入层(embedding)** 以及它是如何解决我们目前接触的各种方法的问题。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qWro5T5qTJJL",
    "mdEditEnable": false
   },
   "source": [
    "## 概述\n",
    "\n",
    "* **目标:** 表示文本中包含内在结构关系的token。\n",
    "* **优点:**\n",
    "\t* 维度低，同时可以找到关系\n",
    "\t* 解释性强的token表示\n",
    "* **缺点：**\n",
    "\t* 无 !\n",
    "* **其他方面:** 有许多已经训练好的嵌入层可供选择。也可以训练自己的嵌入层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MlxioJLqx2Ls",
    "mdEditEnable": false
   },
   "source": [
    "\n",
    "## 嵌入层学习\n",
    "\n",
    "嵌入层的主要思想是将文本中词条的长度表示处理化为定值，而不再考虑词汇表中的词条数量。因此每个词条的表示就变成了 [1 X D] 而不是 [1 X V] (V 是词汇表的大小， D 是嵌入层的大小，通常为 50, 100, 200, 300 )。同时词条表示中的数字也不再是0和1，而是D维潜在空间中表示该词条的浮点数。如果嵌入层确实已经获取了词条之间的关系，那么我们就可以查看这个潜在空间然后确认这些已知的关系(我们会这么做的)。\n",
    "\n",
    "那么首先，应该如何学习嵌入层呢? 嵌入层的意义在于让词条的定义不再取决于它本身，而是它的上下文。这是几种不同的做法:\n",
    "\n",
    "1. 给出上下文中的单词，预测目标词 (CBOW - 连续词袋)\n",
    "2. 给定目标词，预测上下文的单词 (skip-gram)\n",
    "3. 给出一个词序列，预测下一个单词 (LM - 语言模型)\n",
    "\n",
    "所有这些方法都需要创建数据来训练模型。句子中的每个单词都会成为目标词，上下文的单词会由一个 **窗口(window)** 决定。下图显示了skip-gram 的过程，图中展示的窗口大小为 2。我们会对语料库中的每个句子都进行这个操作，从而生产出适用于非监督式学习的训练集。因为我们并没有对应上下文单词的正式标签，所以这会是一种非监督式学习技巧。核心思路是，相似的目标词汇会和相似的上下文一起出现，我们\n",
    "可以通过重复训练模型学习这种(上下文，目标词之间的)匹配关系。\n",
    "\n",
    "<img src=\"data/skipgram.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "我们可以用上面的任意一种方法学习嵌入层，他们互有胜负。你当然可以查看学习后的嵌入层进行比较，但实际上最有效的选择方法是在一个监督式学习的任务中直接验证性能，取最优。我们当然可以用PyTorch建模来学习嵌入层，但是现在我们会直接使用一个专门用于嵌入层和主题建模的库，它叫作 [Gensim](https://radimrehurek.com/gensim/)。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "colab_type": "code",
    "id": "uGDGEVvz41LL",
    "outputId": "90061167-dbee-45f4-986e-e47446a4cd4f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install gensim --upgrade -i https://pypi.tuna.tsinghua.edu.cn/simple --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "9wr9S6965DD7",
    "outputId": "78009f70-4ae5-436e-b4e9-065ff5e9a86b",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\i9233\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "import copy\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import json\n",
    "import nltk; nltk.download('punkt')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import urllib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5-sx-n9655TJ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    seed=1234,\n",
    "    data_file=\"data/harrypotter.txt\",\n",
    "    embedding_dim=100,\n",
    "    window=5,\n",
    "    min_count=3,\n",
    "    skip_gram=1, # 设为0使用CBOW\n",
    "    negative_sampling=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VaJSRgNEg_V_",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "url = \"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/data/harrypotter.txt\"\n",
    "response = urllib.request.urlopen(url)\n",
    "html = response.read()\n",
    "with open(args.data_file, 'wb') as fp:\n",
    "    fp.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "irvgngOG5yqk",
    "outputId": "086fa4d4-57d3-42fe-ba5e-6f15d1fa518a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15640\n",
      "Snape nodded, but did not elaborate.\n"
     ]
    }
   ],
   "source": [
    "# 文本分割成句子\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "with open(args.data_file, encoding='cp1252') as fp:\n",
    "    book = fp.read()\n",
    "sentences = tokenizer.tokenize(book)\n",
    "print (len(sentences))\n",
    "print (sentences[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VTREFDg47Vrx",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 预处理\n",
    "def preprocess_text(text):\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IX7pkCbg7WPK",
    "outputId": "95f9759f-6add-42c6-d4ab-62fcb7fbb58d",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snape nodded , but did not elaborate .\n"
     ]
    }
   ],
   "source": [
    "# 数据清洗\n",
    "sentences = [preprocess_text(sentence) for sentence in sentences]\n",
    "print (sentences[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dsmEEVzX5ytO",
    "outputId": "ba5cf8a6-106d-49ed-a9f8-063e04d58eeb",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['snape', 'nodded', ',', 'but', 'did', 'not', 'elaborate', '.']\n"
     ]
    }
   ],
   "source": [
    "# 为gensim对句子进行预处理\n",
    "sentences = [sentence.split(\" \") for sentence in sentences]\n",
    "print (sentences[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fa-DbjPW-KC3",
    "mdEditEnable": false
   },
   "source": [
    "当嵌入层的学习需要很大的词汇表时，事情很快就会变得复杂起来。之前我们提到，softmax 的反向传播会同时更新正确类和错误类的权重，这样每次反向传播得计算量就会变得巨大。一个变通方案是使用[负采样 negative sampling](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)，它只需要更新正确类和一些随机错误累的权重(negative_sampling=20)。我们有大量的训练数据，而训练数据中我们将多次看到和目标类相同的单词，所以我们可以使用负采样的方案而不会对模型性能产生明显影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wzkUkl-b5ywC",
    "outputId": "94d29f13-c47e-4c2c-915d-a4d930531346",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=4837, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# 由于底层是C语言编写优化，所以速度超级快\n",
    "model = Word2Vec(sentences=sentences, size=args.embedding_dim, \n",
    "                 window=args.window, min_count=args.min_count, \n",
    "                 sg=args.skip_gram, negative=args.negative_sampling)\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "gcMb9HUd7vS_",
    "outputId": "6be3ac88-a5c5-4614-8269-6c0547a6303d",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.38422042, -0.37165675,  0.03897004, -0.39572933, -0.06722818,\n",
       "        0.28262115, -0.18539344, -0.38252193,  0.04126768, -0.07177521,\n",
       "        0.4175501 ,  0.15910536,  0.02483768, -0.25743988, -0.06830274,\n",
       "       -0.85136706,  0.31464487, -0.09031256, -0.15029944, -0.27404675,\n",
       "       -0.1206075 , -0.0332078 ,  0.486565  ,  0.26153773, -0.2315495 ,\n",
       "        0.0435937 , -0.14099172,  0.05731924, -0.3005847 ,  0.08145553,\n",
       "        0.19418667,  0.03351159, -0.21810189, -0.4183487 ,  0.1069016 ,\n",
       "       -0.10239895,  0.11783896, -0.1575813 , -0.2918349 ,  0.44430414,\n",
       "        0.37383065, -0.3026512 , -0.03928424,  0.0907745 ,  0.10058812,\n",
       "        0.0709415 ,  0.3016943 , -0.00304371, -0.03840419,  0.41783455,\n",
       "       -0.20100603, -0.27276963,  0.11610165,  0.1100651 ,  0.72047746,\n",
       "       -0.4024304 , -0.05903339,  0.11686702, -0.24050266,  0.06404617,\n",
       "        0.05398972,  0.20258878, -0.17356241,  0.00923958, -0.19341443,\n",
       "        0.6215801 , -0.29480723, -0.03230208, -0.27822244,  0.39464992,\n",
       "        0.17439093,  0.28051412, -0.10131865,  0.16740386, -0.05840244,\n",
       "        0.0154103 , -0.0127101 ,  0.09744015, -0.15443113,  0.4706436 ,\n",
       "        0.23986171,  0.24745612,  0.1568045 , -0.09086382, -0.4436044 ,\n",
       "       -0.2695224 , -0.10642976,  0.27107126,  0.01049838,  0.20462516,\n",
       "       -0.0285094 ,  0.3016448 , -0.17886618, -0.06495321,  0.17792843,\n",
       "       -0.2597999 , -0.29897285,  0.15006647, -0.3106019 , -0.22431609],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对每个词进行向量化\n",
    "model.wv.get_vector(\"potter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "BozlP5WG70Ak",
    "outputId": "a82275b3-8d43-41ea-89ec-7be3ccdae930",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prickling', 0.9118848443031311),\n",
       " ('pain', 0.9056804776191711),\n",
       " ('heart', 0.9052410125732422),\n",
       " ('forehead', 0.9038321375846863),\n",
       " ('mouth', 0.8982157707214355)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找到距离最近的单词(排除自己)\n",
    "model.wv.most_similar(positive=\"scar\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q-wgfMnH68vQ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 保存权重\n",
    "model.wv.save_word2vec_format('data/model.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DHUU_-TJz_7K",
    "mdEditEnable": false
   },
   "source": [
    "## 预训练嵌入层\n",
    "\n",
    "我们可以使用上面的任意一种方法从头开始学习嵌入层，另外我们也可以使用已经在数百万文档上经过训练的预训练嵌入层。目前流行的嵌入层包括Word2Vec (skip-gram)或GloVe (global word-word co-occurrence)。我们可以通过确认这些嵌入层捕获的有意义的语义关系来进行验证操作。\n",
    "\n",
    "我们可以用之前提到的任意一种方法从头开始学习嵌入测过，但其实直接使用在数百万份文档上训练好的预训练嵌入层也是一种机智的选择。目前使用广泛的嵌入层包括 Word2Vec (skip-gram) 和 GloVe (global word-word co-occurrence)。 我们可以通过确认这些嵌入层所能发现的语义关系来进行验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vJNngmZjglVg",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "1830D18598DB4190B46C735241B5A869",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cp' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    }
   ],
   "source": [
    "!cp /home/kesci/input/glove1001480/glove.6B.100d.txt /home/kesci/work/glove.6B.100d.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CF4E41ABF03A42018F155C257E7C2E32",
    "mdEditEnable": false
   },
   "source": [
    "我们已经直接在K-Lab中挂载了相关文件，所以这里不需要再下载了，如果在本地环境使用本项目你可以uncomment相关代码运行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "NNacKuRNi8_x",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 解压文件 (可能需要三分钟)\n",
    "# resp = urlopen('http://nlp.stanford.edu/data/glove.6B.zip')\n",
    "# zipfile = ZipFile(BytesIO(resp.read()))\n",
    "# zipfile.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "739183BA1CEF423A844CC138C6E198AB",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 写入嵌入层\n",
    "embeddings_file = 'glove.6B.{0}d.txt'.format(args.embedding_dim)\n",
    "# zipfile.extract(embeddings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SA5Y1BGbbpPo",
    "outputId": "bf3bd287-47bf-442e-c78b-9ec7eee5f70c",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove.6B.100d.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-73dc587b634c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 保存word2vec格式的GloVe嵌入层到\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mword2vec_output_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'{0}.word2vec'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mglove2word2vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeddings_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2vec_output_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\Users\\i9233\\Anaconda3\\lib\\site-packages\\gensim\\scripts\\glove2word2vec.py\u001b[0m in \u001b[0;36mglove2word2vec\u001b[1;34m(glove_input_file, word2vec_output_file)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m     \"\"\"\n\u001b[1;32m--> 104\u001b[1;33m     \u001b[0mnum_lines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_glove_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_input_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"converting %i vectors from %s to %s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_lines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglove_input_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2vec_output_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2vec_output_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\i9233\\Anaconda3\\lib\\site-packages\\gensim\\scripts\\glove2word2vec.py\u001b[0m in \u001b[0;36mget_glove_info\u001b[1;34m(glove_file_name)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \"\"\"\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_file_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[0mnum_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglove_file_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\i9233\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[1;34m(uri, mode, **kw)\u001b[0m\n\u001b[0;32m    398\u001b[0m             \u001b[0mtransport_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_ext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_extension\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransport_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mscrubbed_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\i9233\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m         \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m     )\n\u001b[0;32m    302\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Users\\i9233\\Anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[1;34m(uri, mode, ignore_ext, buffering, encoding, errors)\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPY3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 459\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    460\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mopen_kwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove.6B.100d.txt'"
     ]
    }
   ],
   "source": [
    "# 保存word2vec格式的GloVe嵌入层到\n",
    "word2vec_output_file = '{0}.word2vec'.format(embeddings_file)\n",
    "glove2word2vec(embeddings_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bQp1pIJLiZw3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 加载嵌入层(可能需要一分钟)\n",
    "glove = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "HDpHnC0sbpVE",
    "outputId": "a19ed390-7c15-4897-d440-65137d170525",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# (king - man) + woman = ?\n",
    "glove.most_similar(positive=['woman', 'king'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "_1TatbmvbpYU",
    "outputId": "ec827143-0c7f-4c1b-f080-4d1458f53c1e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 找到距离最近的单词(排除自己)\n",
    "glove.wv.most_similar(positive=\"goku\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oMljB5MEbpdb",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 为绘图作降维\n",
    "X = glove[glove.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "pca_results = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Aok5pRqXqcCf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_embeddings(words, embeddings, pca_results):\n",
    "    for word in words:\n",
    "        index = embeddings.index2word.index(word)\n",
    "        plt.scatter(pca_results[index, 0], pca_results[index, 1])\n",
    "        plt.annotate(word, xy=(pca_results[index, 0], pca_results[index, 1]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "NOdLFgTOrrBd",
    "outputId": "3ec4bd22-6c24-46c3-889b-0dceda5d536e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_embeddings(words=[\"king\", \"queen\", \"man\", \"woman\"], embeddings=glove, \n",
    "                pca_results=pca_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "6667pZttboaj",
    "outputId": "d3bdf331-e90f-448f-899e-4d0dcd252878",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 嵌入层的偏差\n",
    "glove.most_similar(positive=['woman', 'doctor'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tu3Bl-_yw9LA",
    "mdEditEnable": false
   },
   "source": [
    "## 使用嵌入层 \n",
    "下面是几种使用嵌入层的方法:\n",
    "\n",
    "1. 使用自己训练的嵌入层（用非监督数据集训练）\n",
    "2. 使用预训练的嵌入层（GloVe，word2vec等）\n",
    "3. 随机初始化的嵌入层\n",
    "\n",
    "选好了使用的嵌入层，你可以选择冻结它还是用监督式数据继续训练 (可能会导致过拟合)。这里我们会使用 GloVe 并选择冻结。这次的任务是预测给定标题的文章类型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8OlZB0vc1Hbb",
    "mdEditEnable": false
   },
   "source": [
    "### 配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2EAFB01B886A4B45A5940D89CA11D670",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install torch --upgrade -i https://pypi.tuna.tsinghua.edu.cn/simple --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T3daO-e9wpXe",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "import collections\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ye64bXPrwpaQ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 设置Numpy和PyTorch的种子\n",
    "def set_seeds(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "# 创建目录\n",
    "def create_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "m5_jiImZ1NAr",
    "outputId": "238e95dd-3df4-45c6-e6dd-ebdeb33b8a88",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 参数\n",
    "args = Namespace(\n",
    "    seed=1234,\n",
    "    cuda=True,\n",
    "    shuffle=True,\n",
    "    data_file=\"data/news.csv\",\n",
    "    split_data_file=\"data/split_news.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"news\",\n",
    "    train_size=0.7,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    cutoff=25, # token must appear at least <cutoff> times to be in SequenceVocabulary\n",
    "    num_epochs=5,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=64,\n",
    "    num_filters=100,\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=100,\n",
    "    dropout_p=0.1,\n",
    ")\n",
    "\n",
    "# 设置种子\n",
    "set_seeds(seed=args.seed, cuda=args.cuda)\n",
    "\n",
    "# 创建保存目录\n",
    "create_dirs(args.save_dir)\n",
    "\n",
    "# 拓展路径\n",
    "args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "# 检查GPU可用性\n",
    "if torch.cuda.is_available():\n",
    "    args.cuda = True\n",
    "else:\n",
    "    args.cude = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GkNclM3I1XMg",
    "mdEditEnable": false
   },
   "source": [
    "### 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vk6ucvgp1NDD",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uVQNGEYa1NFu",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "url = \"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/data/news.csv\"\n",
    "response = urllib.request.urlopen(url)\n",
    "html = response.read()\n",
    "with open(args.data_file, 'wb') as fp:\n",
    "    fp.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "aTU6ENCu1NIc",
    "outputId": "6a7f2c48-4c8f-4283-accb-9d96a61ecbea",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 原始数据\n",
    "df = pd.read_csv(args.data_file, header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "xIouG0f71NK4",
    "outputId": "1f49fb96-8ef6-4772-89f7-6d9a605cda21",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 按类别拆分数据集\n",
    "by_category = collections.defaultdict(list)\n",
    "for _, row in df.iterrows():\n",
    "    by_category[row.category].append(row.to_dict())\n",
    "for category in by_category:\n",
    "    print (\"{0}: {1}\".format(category, len(by_category[category])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YZyFpi791NNP",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 新建切分数据集\n",
    "final_list = []\n",
    "for _, item_list in sorted(by_category.items()):\n",
    "    if args.shuffle:\n",
    "        np.random.shuffle(item_list)\n",
    "    n = len(item_list)\n",
    "    n_train = int(args.train_size*n)\n",
    "    n_val = int(args.val_size*n)\n",
    "    n_test = int(args.test_size*n)\n",
    "\n",
    "  # 给数据指定切分属性\n",
    "    for item in item_list[:n_train]:\n",
    "        item['split'] = 'train'\n",
    "    for item in item_list[n_train:n_train+n_val]:\n",
    "        item['split'] = 'val'\n",
    "    for item in item_list[n_train+n_val:]:\n",
    "        item['split'] = 'test'  \n",
    "\n",
    "    # 添加到列表\n",
    "    final_list.extend(item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "NLrQTVoO1NP4",
    "outputId": "c0800a8b-0c18-450c-87bf-04971bc92075",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 切分后的数据集dataframe\n",
    "split_df = pd.DataFrame(final_list)\n",
    "split_df[\"split\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v-HSMGwh1NUu",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 预处理\n",
    "def preprocess_text(text):\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text\n",
    "    \n",
    "split_df.title = split_df.title.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "gvNVV7n11NXX",
    "outputId": "bcb4a3dd-ba84-4475-dfe1-161f1f725684",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 存为CSV文件\n",
    "split_df.to_csv(args.split_data_file, index=False)\n",
    "split_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cPDWGdlE5vye",
    "mdEditEnable": false
   },
   "source": [
    "### 词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EMR5Y3D95v6P",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, token_to_idx=None):\n",
    "\n",
    "        # 词条转换为索引\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self.token_to_idx = token_to_idx\n",
    "\n",
    "        # 索引转换为词条\n",
    "        self.idx_to_token = {idx: token \\\n",
    "                             for token, idx in self.token_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self.token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        return [self.add_token[token] for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        return self.token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        if index not in self.idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self.idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "WKnfFKY95v81",
    "outputId": "03e03491-84aa-45f8-e004-1e9ba58c19c5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Vocabulary instance\n",
    "category_vocab = Vocabulary()\n",
    "for index, row in df.iterrows():\n",
    "    category_vocab.add_token(row.category)\n",
    "print (category_vocab) # __str__\n",
    "print (len(category_vocab)) # __len__\n",
    "index = category_vocab.lookup_token(\"Business\")\n",
    "print (index)\n",
    "print (category_vocab.lookup_index(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jxO9-6vhBCSO",
    "mdEditEnable": false
   },
   "source": [
    "### 序列词汇表\n",
    "\n",
    "接下来我们将为文章标题创建词汇表类，它由一系列词条构成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ip8ViI2v_7Y2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S2LTR5NjBDCG",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self.mask_token = mask_token\n",
    "        self.unk_token = unk_token\n",
    "        self.begin_seq_token = begin_seq_token\n",
    "        self.end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self.mask_token)\n",
    "        self.unk_index = self.add_token(self.unk_token)\n",
    "        self.begin_seq_index = self.add_token(self.begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self.end_seq_token)\n",
    "        \n",
    "        # 索引转换为词条\n",
    "        self.idx_to_token = {idx: token \\\n",
    "                             for token, idx in self.token_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self.unk_token,\n",
    "                         'mask_token': self.mask_token,\n",
    "                         'begin_seq_token': self.begin_seq_token,\n",
    "                         'end_seq_token': self.end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        return self.token_to_idx.get(token, self.unk_index)\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        if index not in self.idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the SequenceVocabulary\" % index)\n",
    "        return self.idx_to_token[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"<SequenceVocabulary(size=%d)>\" % len(self.token_to_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "DvDPVDbsBDE4",
    "outputId": "09913e32-1588-4410-9d22-6f4024ac3193",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 得到词数\n",
    "word_counts = Counter()\n",
    "for title in split_df.title:\n",
    "    for token in title.split(\" \"):\n",
    "        if token not in string.punctuation:\n",
    "            word_counts[token] += 1\n",
    "\n",
    "# 创建SequenceVocabulary实例\n",
    "title_vocab = SequenceVocabulary()\n",
    "for word, word_count in word_counts.items():\n",
    "    if word_count >= args.cutoff:\n",
    "        title_vocab.add_token(word)\n",
    "print (title_vocab) # __str__\n",
    "print (len(title_vocab)) # __len__\n",
    "index = title_vocab.lookup_token(\"general\")\n",
    "print (index)\n",
    "print (title_vocab.lookup_index(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6TuUHvlI6JGh",
    "mdEditEnable": false
   },
   "source": [
    "### 向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s8IsOPHT5v_i",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NewsVectorizer(object):\n",
    "    def __init__(self, title_vocab, category_vocab):\n",
    "        self.title_vocab = title_vocab\n",
    "        self.category_vocab = category_vocab\n",
    "\n",
    "    def vectorize(self, title):\n",
    "        indices = [self.title_vocab.lookup_token(token) for token in title.split(\" \")]\n",
    "        indices = [self.title_vocab.begin_seq_index] + indices + \\\n",
    "            [self.title_vocab.end_seq_index]\n",
    "        \n",
    "        # 创建向量\n",
    "        title_length = len(indices)\n",
    "        vector = np.zeros(title_length, dtype=np.int64)\n",
    "        vector[:len(indices)] = indices\n",
    "\n",
    "        return vector\n",
    "    \n",
    "    def unvectorize(self, vector):\n",
    "        tokens = [self.title_vocab.lookup_index(index) for index in vector]\n",
    "        title = \" \".join(token for token in tokens)\n",
    "        return title\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df, cutoff):\n",
    "        \n",
    "        # 创建分类的词汇表实例\n",
    "        category_vocab = Vocabulary()        \n",
    "        for category in sorted(set(df.category)):\n",
    "            category_vocab.add_token(category)\n",
    "\n",
    "        # 获取长度\n",
    "        word_counts = Counter()\n",
    "        for title in df.title:\n",
    "            for token in title.split(\" \"):\n",
    "                word_counts[token] += 1\n",
    "        \n",
    "        # 创建标题的词汇表实例\n",
    "        title_vocab = SequenceVocabulary()\n",
    "        for word, word_count in word_counts.items():\n",
    "            if word_count >= cutoff:\n",
    "                title_vocab.add_token(word)\n",
    "        \n",
    "        return cls(title_vocab, category_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        title_vocab = SequenceVocabulary.from_serializable(contents['title_vocab'])\n",
    "        category_vocab = Vocabulary.from_serializable(contents['category_vocab'])\n",
    "        return cls(title_vocab=title_vocab, category_vocab=category_vocab)\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {'title_vocab': self.title_vocab.to_serializable(),\n",
    "                'category_vocab': self.category_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "p2blo9sT5wCS",
    "outputId": "8a134781-caca-47b7-e7cc-913ca3a1d9fa",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 向量化实例\n",
    "vectorizer = NewsVectorizer.from_dataframe(split_df, cutoff=args.cutoff)\n",
    "print (vectorizer.title_vocab)\n",
    "print (vectorizer.category_vocab)\n",
    "vectorized_title = vectorizer.vectorize(preprocess_text(\n",
    "    \"Roger Federer wins the Wimbledon tennis tournament.\"))\n",
    "print (np.shape(vectorized_title))\n",
    "print (vectorized_title)\n",
    "print (vectorizer.unvectorize(vectorized_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jTpYV1FkHBTi",
    "mdEditEnable": false
   },
   "source": [
    "### 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_PcKZGbYIVA7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lrssjuxb1NZy",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, df, vectorizer):\n",
    "        self.df = df\n",
    "        self.vectorizer = vectorizer\n",
    "        \n",
    "        # 最大标题长度\n",
    "        get_length = lambda title: len(title.split(\" \"))\n",
    "        self.max_seq_length = max(map(get_length, df.title)) + 2 # (<BEGIN> + <END>)\n",
    "\n",
    "        # 切分数据\n",
    "        self.train_df = self.df[self.df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "        self.val_df = self.df[self.df.split=='val']\n",
    "        self.val_size = len(self.val_df)\n",
    "        self.test_df = self.df[self.df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "        self.lookup_dict = {'train': (self.train_df, self.train_size), \n",
    "                            'val': (self.val_df, self.val_size),\n",
    "                            'test': (self.test_df, self.test_size)}\n",
    "        self.set_split('train')\n",
    "\n",
    "        # 类权重(用于样本失衡)\n",
    "        class_counts = df.category.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self.vectorizer.category_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, split_data_file, cutoff):\n",
    "        df = pd.read_csv(split_data_file, header=0)\n",
    "        train_df = df[df.split=='train']\n",
    "        return cls(df, NewsVectorizer.from_dataframe(train_df, cutoff))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, split_data_file, vectorizer_filepath):\n",
    "        df = pd.read_csv(split_data_file, header=0)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(df, vectorizer)\n",
    "\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return NewsVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self.vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self.target_split = split\n",
    "        self.target_df, self.target_size = self.lookup_dict[split]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Dataset(split={0}, size={1})\".format(\n",
    "            self.target_split, self.target_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.target_df.iloc[index]\n",
    "        title_vector = self.vectorizer.vectorize(row.title)\n",
    "        category_index = self.vectorizer.category_vocab.lookup_token(row.category)\n",
    "        return {'title': title_vector, 'category': category_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "\n",
    "    def generate_batches(self, batch_size, collate_fn, shuffle=True, \n",
    "                         drop_last=False, device=\"cpu\"):\n",
    "        dataloader = DataLoader(dataset=self, batch_size=batch_size,\n",
    "                                collate_fn=collate_fn, shuffle=shuffle, \n",
    "                                drop_last=drop_last)\n",
    "        for data_dict in dataloader:\n",
    "            out_data_dict = {}\n",
    "            for name, tensor in data_dict.items():\n",
    "                out_data_dict[name] = data_dict[name].to(device)\n",
    "            yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "e9I-AFKsIP2e",
    "outputId": "913612d9-805a-4c5c-df5a-578fd1b8c935",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 数据集实例\n",
    "dataset = NewsDataset.load_dataset_and_make_vectorizer(args.split_data_file, \n",
    "                                                       cutoff=args.cutoff)\n",
    "print (dataset) # __str__\n",
    "title_vector = dataset[5]['title'] # __getitem__\n",
    "print (title_vector)\n",
    "print (dataset.vectorizer.unvectorize(title_vector))\n",
    "print (dataset.class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "202KnN2lLj61",
    "mdEditEnable": false
   },
   "source": [
    "### 模型\n",
    "\n",
    "input（输入层） → embedding（嵌入层） → conv （卷积层）→ FC （全连接层)\n",
    "\n",
    "尽管输入是各个单词，在这里我们将依旧会使用一维的卷积层([nn.Conv1D](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv1d))。我们并不会按字符表示他们，而会采用 shape ~$\\in \\mathbb{R}^{NXSXE}~$ 的输入形状。\n",
    "\n",
    "*其中:*\n",
    "* N = 每个批处理的训练样本数\n",
    "* S = 最大句子长度\n",
    "* E = 单词级别的词嵌入维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j-Xgp0F3INRA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lfw7csveHApA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NewsModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_embeddings, num_input_channels, \n",
    "                 num_channels, hidden_dim, num_classes, dropout_p, \n",
    "                 pretrained_embeddings=None, freeze_embeddings=False,\n",
    "                 padding_idx=0):\n",
    "        super(NewsModel, self).__init__()\n",
    "        \n",
    "        if pretrained_embeddings is None:\n",
    "            self.embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
    "                                          num_embeddings=num_embeddings,\n",
    "                                          padding_idx=padding_idx)\n",
    "        else:\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
    "                                           num_embeddings=num_embeddings,\n",
    "                                           padding_idx=padding_idx,\n",
    "                                           _weight=pretrained_embeddings)\n",
    "        \n",
    "        # 卷积层权重\n",
    "        self.conv = nn.ModuleList([nn.Conv1d(num_input_channels, num_channels, \n",
    "                                             kernel_size=f) for f in [2,3,4]])\n",
    "     \n",
    "        # 全连接层权重\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc1 = nn.Linear(num_channels*3, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "        if freeze_embeddings:\n",
    "            self.embeddings.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, x_in, channel_first=False, apply_softmax=False):\n",
    "        \n",
    "        # 嵌入\n",
    "        x_in = self.embeddings(x_in)\n",
    "\n",
    "        # 重置输入形状\n",
    "        if not channel_first:\n",
    "            x_in = x_in.transpose(1, 2)\n",
    "            \n",
    "        # 卷积层输出\n",
    "        z1 = self.conv[0](x_in)\n",
    "        z1 = F.max_pool1d(z1, z1.size(2)).squeeze(2)\n",
    "        z2 = self.conv[1](x_in)\n",
    "        z2 = F.max_pool1d(z2, z2.size(2)).squeeze(2)\n",
    "        z3 = self.conv[2](x_in)\n",
    "        z3 = F.max_pool1d(z3, z3.size(2)).squeeze(2)\n",
    "        \n",
    "        # 拼接卷积层输出\n",
    "        z = torch.cat([z1, z2, z3], 1)\n",
    "\n",
    "        # 全连接层\n",
    "        z = self.dropout(z)\n",
    "        z = self.fc1(z)\n",
    "        y_pred = self.fc2(z)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            y_pred = F.softmax(y_pred, dim=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d7_4SiLkKji8",
    "mdEditEnable": false
   },
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UQAIfKN2HAtN",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RwgZASWYHAwB",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, dataset, model, model_state_file, save_dir, device, shuffle, \n",
    "               num_epochs, batch_size, learning_rate, early_stopping_criteria):\n",
    "        self.dataset = dataset\n",
    "        self.class_weights = dataset.class_weights.to(device)\n",
    "        self.model = model.to(device)\n",
    "        self.save_dir = save_dir\n",
    "        self.device = device\n",
    "        self.shuffle = shuffle\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_func = nn.CrossEntropyLoss(self.class_weights)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer=self.optimizer, mode='min', factor=0.5, patience=1)\n",
    "        self.train_state = {\n",
    "            'stop_early': False, \n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'early_stopping_criteria': early_stopping_criteria,\n",
    "            'learning_rate': learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': model_state_file}\n",
    "    \n",
    "    def update_train_state(self):\n",
    "\n",
    "        # 打印checkpoint信息\n",
    "        print (\"[EPOCH]: {0:02d} | [LR]: {1} | [TRAIN LOSS]: {2:.2f} | [TRAIN ACC]: {3:.1f}% | [VAL LOSS]: {4:.2f} | [VAL ACC]: {5:.1f}%\".format(\n",
    "          self.train_state['epoch_index'], self.train_state['learning_rate'], \n",
    "            self.train_state['train_loss'][-1], self.train_state['train_acc'][-1], \n",
    "            self.train_state['val_loss'][-1], self.train_state['val_acc'][-1]))\n",
    "\n",
    "        # 至少保存一次模型\n",
    "        if self.train_state['epoch_index'] == 0:\n",
    "            torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
    "            self.train_state['stop_early'] = False\n",
    "\n",
    "        # 如果模型性能表现有提升，再次保存\n",
    "        elif self.train_state['epoch_index'] >= 1:\n",
    "            loss_tm1, loss_t = self.train_state['val_loss'][-2:]\n",
    "\n",
    "            # 如果损失增大\n",
    "            if loss_t >= self.train_state['early_stopping_best_val']:\n",
    "                # 更新步数\n",
    "                self.train_state['early_stopping_step'] += 1\n",
    "\n",
    "            # 损失变小\n",
    "            else:\n",
    "                # 保存最优的模型\n",
    "                if loss_t < self.train_state['early_stopping_best_val']:\n",
    "                    torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
    "\n",
    "                # 重置early stopping 的步数\n",
    "                self.train_state['early_stopping_step'] = 0\n",
    "\n",
    "            # 是否需要early stop?\n",
    "            self.train_state['stop_early'] = self.train_state['early_stopping_step'] \\\n",
    "              >= self.train_state['early_stopping_criteria']\n",
    "        return self.train_state\n",
    "  \n",
    "    def compute_accuracy(self, y_pred, y_target):\n",
    "        _, y_pred_indices = y_pred.max(dim=1)\n",
    "        n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "        return n_correct / len(y_pred_indices) * 100\n",
    "    \n",
    "    def pad_seq(self, seq, length):\n",
    "        vector = np.zeros(length, dtype=np.int64)\n",
    "        vector[:len(seq)] = seq\n",
    "        vector[len(seq):] = self.dataset.vectorizer.title_vocab.mask_index\n",
    "        return vector\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        \n",
    "        # 深度拷贝\n",
    "        batch_copy = copy.deepcopy(batch)\n",
    "        processed_batch = {\"title\": [], \"category\": []}\n",
    "        \n",
    "        # 得到最长序列长度\n",
    "        max_seq_len = max([len(sample[\"title\"]) for sample in batch_copy])\n",
    "        \n",
    "        # 填充\n",
    "        for i, sample in enumerate(batch_copy):\n",
    "            seq = sample[\"title\"]\n",
    "            category = sample[\"category\"]\n",
    "            padded_seq = self.pad_seq(seq, max_seq_len)\n",
    "            processed_batch[\"title\"].append(padded_seq)\n",
    "            processed_batch[\"category\"].append(category)\n",
    "            \n",
    "        # 转换为合适的tensor\n",
    "        processed_batch[\"title\"] = torch.LongTensor(\n",
    "            processed_batch[\"title\"])\n",
    "        processed_batch[\"category\"] = torch.LongTensor(\n",
    "            processed_batch[\"category\"])\n",
    "        \n",
    "        return processed_batch    \n",
    "  \n",
    "    def run_train_loop(self):\n",
    "        for epoch_index in range(self.num_epochs):\n",
    "            self.train_state['epoch_index'] = epoch_index\n",
    "      \n",
    "            # 编辑训练集\n",
    "            # 初始化批生成器, 将损失和准确率归零，设置为训练模式\n",
    "            self.dataset.set_split('train')\n",
    "            batch_generator = self.dataset.generate_batches(\n",
    "                batch_size=self.batch_size, collate_fn=self.collate_fn, \n",
    "                shuffle=self.shuffle, device=self.device)\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            self.model.train()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "                # 梯度归零\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # 计算输出\n",
    "                y_pred = self.model(batch_dict['title'])\n",
    "\n",
    "                # 计算损失\n",
    "                loss = self.loss_func(y_pred, batch_dict['category'])\n",
    "                loss_t = loss.item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # 反向传播\n",
    "                loss.backward()\n",
    "\n",
    "                # 更新梯度\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # 计算准确率\n",
    "                acc_t = self.compute_accuracy(y_pred, batch_dict['category'])\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            self.train_state['train_loss'].append(running_loss)\n",
    "            self.train_state['train_acc'].append(running_acc)\n",
    "\n",
    "            # 遍历验证集\n",
    "            # 初始化批生成器, 将损失和准确率归零，设置为运行模式\n",
    "            self.dataset.set_split('val')\n",
    "            batch_generator = self.dataset.generate_batches(\n",
    "                batch_size=self.batch_size, collate_fn=self.collate_fn, \n",
    "                shuffle=self.shuffle, device=self.device)\n",
    "            running_loss = 0.\n",
    "            running_acc = 0.\n",
    "            self.model.eval()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "                # 计算输出\n",
    "                y_pred =  self.model(batch_dict['title'])\n",
    "\n",
    "                # 计算损失\n",
    "                loss = self.loss_func(y_pred, batch_dict['category'])\n",
    "                loss_t = loss.to(\"cpu\").item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # 计算准确率\n",
    "                acc_t = self.compute_accuracy(y_pred, batch_dict['category'])\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            self.train_state['val_loss'].append(running_loss)\n",
    "            self.train_state['val_acc'].append(running_acc)\n",
    "\n",
    "            self.train_state = self.update_train_state()\n",
    "            self.scheduler.step(self.train_state['val_loss'][-1])\n",
    "            if self.train_state['stop_early']:\n",
    "                break\n",
    "          \n",
    "    def run_test_loop(self):\n",
    "        # 初始化批生成器, 将损失和准确率归零，设置为运行模式\n",
    "        self.dataset.set_split('test')\n",
    "        batch_generator = self.dataset.generate_batches(\n",
    "                batch_size=self.batch_size, collate_fn=self.collate_fn, \n",
    "                shuffle=self.shuffle, device=self.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        self.model.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # 计算输出\n",
    "            y_pred =  self.model(batch_dict['title'])\n",
    "\n",
    "            # 计算损失\n",
    "            loss = self.loss_func(y_pred, batch_dict['category'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # 计算准确率\n",
    "            acc_t = self.compute_accuracy(y_pred, batch_dict['category'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "        self.train_state['test_loss'] = running_loss\n",
    "        self.train_state['test_acc'] = running_acc\n",
    "    \n",
    "    def plot_performance(self):\n",
    "        # 设置图大小\n",
    "        plt.figure(figsize=(15,5))\n",
    "\n",
    "        # 画出损失\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Loss\")\n",
    "        plt.plot(trainer.train_state[\"train_loss\"], label=\"train\")\n",
    "        plt.plot(trainer.train_state[\"val_loss\"], label=\"val\")\n",
    "        plt.legend(loc='upper right')\n",
    "\n",
    "        # 画出准确率\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.plot(trainer.train_state[\"train_acc\"], label=\"train\")\n",
    "        plt.plot(trainer.train_state[\"val_acc\"], label=\"val\")\n",
    "        plt.legend(loc='lower right')\n",
    "\n",
    "        # 存图\n",
    "        plt.savefig(os.path.join(self.save_dir, \"performance.png\"))\n",
    "\n",
    "        # 展示图\n",
    "        plt.show()\n",
    "    \n",
    "    def save_train_state(self):\n",
    "        with open(os.path.join(self.save_dir, \"train_state.json\"), \"w\") as fp:\n",
    "            json.dump(self.train_state, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "LJfKGc9cHA0y",
    "outputId": "2185fea4-a2c6-4986-80d4-c781ab1ea394",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 初始化\n",
    "dataset = NewsDataset.load_dataset_and_make_vectorizer(args.split_data_file, \n",
    "                                                       cutoff=args.cutoff)\n",
    "dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.vectorizer\n",
    "model = NewsModel(embedding_dim=args.embedding_dim, \n",
    "                  num_embeddings=len(vectorizer.title_vocab), \n",
    "                  num_input_channels=args.embedding_dim, \n",
    "                  num_channels=args.num_filters, hidden_dim=args.hidden_dim, \n",
    "                  num_classes=len(vectorizer.category_vocab), \n",
    "                  dropout_p=args.dropout_p, pretrained_embeddings=None, \n",
    "                  padding_idx=vectorizer.title_vocab.mask_index)\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "wDxPyGIVKoUK",
    "outputId": "fabb05e9-25b3-43ea-ce72-713a0bae1426",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 训练\n",
    "trainer = Trainer(dataset=dataset, model=model, \n",
    "                  model_state_file=args.model_state_file, \n",
    "                  save_dir=args.save_dir, device=args.device,\n",
    "                  shuffle=args.shuffle, num_epochs=args.num_epochs, \n",
    "                  batch_size=args.batch_size, learning_rate=args.learning_rate, \n",
    "                  early_stopping_criteria=args.early_stopping_criteria)\n",
    "trainer.run_train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "V9Ruxpc8WT17",
    "outputId": "f776e4cb-0d62-4716-b51a-f5039e34d884",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 画出训练过程\n",
    "trainer.plot_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "xRphYPaVWTzY",
    "outputId": "a29f02c4-b004-4756-c06e-2438eb0d306e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 测试集上的性能\n",
    "trainer.run_test_loop()\n",
    "print(\"Test loss: {0:.2f}\".format(trainer.train_state['test_loss']))\n",
    "print(\"Test Accuracy: {0:.1f}%\".format(trainer.train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z_Zrw9rIWTwU",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 保存结果\n",
    "trainer.save_train_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HH_328MjWoM-",
    "mdEditEnable": false
   },
   "source": [
    "### 使用GloVe嵌入层\n",
    "\n",
    "\n",
    "上面的代码中，我们使用了随机初始化的嵌入层获得了还ok的性能。但是一定要记住，用这种方法生成的嵌入层在别的数据集上大概率会出现过拟合。\n",
    "现在我们要使用预训练的 GloVe 来初始化嵌入层，然后会先冻结嵌入层(这样他们在训练期间就不会有变化)在监督式学习任务里训练模型，接着不冻结嵌入层(这样会继续训练)再训练，这样来比较性能。\n",
    "\n",
    "```python\n",
    "pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "self.embeddings = nn.Embedding(embedding_dim=embedding_dim, \n",
    "                               num_embeddings=num_embeddings, \n",
    "                               padding_idx=padding_idx, \n",
    "                               _weight=pretrained_embeddings)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r1EAWPBEKoaS",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_glove_embeddings(embeddings_file):\n",
    "    word_to_idx = {}\n",
    "    embeddings = []\n",
    "\n",
    "    with open(embeddings_file, \"r\") as fp:\n",
    "        for index, line in enumerate(fp):\n",
    "            line = line.split(\" \")\n",
    "            word = line[0]\n",
    "            word_to_idx[word] = index\n",
    "            embedding_i = np.array([float(val) for val in line[1:]])\n",
    "            embeddings.append(embedding_i)\n",
    "\n",
    "    return word_to_idx, np.stack(embeddings)\n",
    "\n",
    "def make_embeddings_matrix(words):\n",
    "    word_to_idx, glove_embeddings = load_glove_embeddings(embeddings_file)\n",
    "    embedding_dim = glove_embeddings.shape[1]\n",
    "    embeddings = np.zeros((len(words), embedding_dim))\n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_to_idx:\n",
    "            embeddings[i, :] = glove_embeddings[word_to_idx[word]]\n",
    "        else:\n",
    "            embedding_i = torch.zeros(1, embedding_dim)\n",
    "            nn.init.xavier_uniform_(embedding_i)\n",
    "            embeddings[i, :] = embedding_i\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FZBLYjtWKoX5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "args.use_glove_embeddings = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Gj77_BmoHA3s",
    "outputId": "30f47b5b-310e-4bcc-d25c-07723dedb0b6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 初始化\n",
    "dataset = NewsDataset.load_dataset_and_make_vectorizer(args.split_data_file, \n",
    "                                                       cutoff=args.cutoff)\n",
    "dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.vectorizer\n",
    "\n",
    "# 创建嵌入层\n",
    "embeddings = None\n",
    "if args.use_glove_embeddings:\n",
    "    embeddings_file = 'glove.6B.{0}d.txt'.format(args.embedding_dim)\n",
    "    words = vectorizer.title_vocab.token_to_idx.keys()\n",
    "    embeddings = make_embeddings_matrix(words=words)\n",
    "    print (\"<Embeddings(words={0}, dim={1})>\".format(\n",
    "        np.shape(embeddings)[0], np.shape(embeddings)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "CC4ofLswt-D3",
    "outputId": "d183b8e5-f418-4b8c-f58e-20f06b31982e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "model = NewsModel(embedding_dim=args.embedding_dim, \n",
    "                  num_embeddings=len(vectorizer.title_vocab), \n",
    "                  num_input_channels=args.embedding_dim, \n",
    "                  num_channels=args.num_filters, hidden_dim=args.hidden_dim, \n",
    "                  num_classes=len(vectorizer.category_vocab), \n",
    "                  dropout_p=args.dropout_p, pretrained_embeddings=embeddings, \n",
    "                  padding_idx=vectorizer.title_vocab.mask_index)\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "44gVZbKbHA7R",
    "outputId": "05a65ccf-bf15-42dd-84a7-ffe120aca5cf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 训练\n",
    "trainer = Trainer(dataset=dataset, model=model, \n",
    "                  model_state_file=args.model_state_file, \n",
    "                  save_dir=args.save_dir, device=args.device,\n",
    "                  shuffle=args.shuffle, num_epochs=args.num_epochs, \n",
    "                  batch_size=args.batch_size, learning_rate=args.learning_rate, \n",
    "                  early_stopping_criteria=args.early_stopping_criteria)\n",
    "trainer.run_train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "98MqlEQ0sfTs",
    "outputId": "ef300406-fa50-41e2-ed77-f558520cc8c3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 查看性能\n",
    "trainer.plot_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "wYGUHK0GsfhP",
    "outputId": "7997fa4f-1633-4137-886b-ac10c5a31e4b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 测试性能\n",
    "trainer.run_test_loop()\n",
    "print(\"Test loss: {0:.2f}\".format(trainer.train_state['test_loss']))\n",
    "print(\"Test Accuracy: {0:.1f}%\".format(trainer.train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GO8v_uUisfd9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 保存所有结果\n",
    "trainer.save_train_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XrMm4K61t21h",
    "mdEditEnable": false
   },
   "source": [
    "### 冻结嵌入层\n",
    "\n",
    "\n",
    "现在冻结 GloVe 嵌入层并且在监督式学习任务上进行训练。模型代码里唯一需要改变的是打开 `freeze_embeddings`:\n",
    "\n",
    "```python\n",
    "if freeze_embeddings:\n",
    "    self.embeddings.weight.requires_grad = False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ilV_QbtktzH3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "args.freeze_embeddings = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "7NdD-iP6tzFQ",
    "outputId": "52f0faa1-9ec2-44de-8767-71b9e6f31c2c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "model = NewsModel(embedding_dim=args.embedding_dim, \n",
    "                  num_embeddings=len(vectorizer.title_vocab), \n",
    "                  num_input_channels=args.embedding_dim, \n",
    "                  num_channels=args.num_filters, hidden_dim=args.hidden_dim, \n",
    "                  num_classes=len(vectorizer.category_vocab), \n",
    "                  dropout_p=args.dropout_p, pretrained_embeddings=embeddings,\n",
    "                  freeze_embeddings=args.freeze_embeddings,\n",
    "                  padding_idx=vectorizer.title_vocab.mask_index)\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "6NJhm2EOtzCV",
    "outputId": "775c7f44-2346-459f-a0dd-d31244330b87",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 训练\n",
    "trainer = Trainer(dataset=dataset, model=model, \n",
    "                  model_state_file=args.model_state_file, \n",
    "                  save_dir=args.save_dir, device=args.device,\n",
    "                  shuffle=args.shuffle, num_epochs=args.num_epochs, \n",
    "                  batch_size=args.batch_size, learning_rate=args.learning_rate, \n",
    "                  early_stopping_criteria=args.early_stopping_criteria)\n",
    "trainer.run_train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "oojDHLowty_9",
    "outputId": "fffbfaf7-fa75-43d9-e70a-d3200ac80013",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 查看性能\n",
    "trainer.plot_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "rFYs9dH3ui66",
    "outputId": "89923358-2da3-4a32-f504-986136cc134b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 测试性能\n",
    "trainer.run_test_loop()\n",
    "print(\"Test loss: {0:.2f}\".format(trainer.train_state['test_loss']))\n",
    "print(\"Test Accuracy: {0:.1f}%\".format(trainer.train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vKDmGXAyukpV",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 保存结果\n",
    "trainer.save_train_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P4yiDxOavmJy",
    "mdEditEnable": false
   },
   "source": [
    "\n",
    "通过对比发现，使用没有冻结的 GloVe嵌入层 在测试集上取得了最好的性能。由于不同的任务会产生不同的结果，所以还是需要根据实验来选择是否进行冻结操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78C95A361E7E485A8847B8422194D24F",
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
