{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OLIxEDq6VhvZ",
    "mdEditEnable": false
   },
   "source": [
    "## 嵌入层(Embedding)\n",
    "\n",
    "\n",
    "<img src=\"data/logo.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "迄今为止，我们使用了 一组独热编码过的n维数组来表示文本，每个索引都对应了一个token，每个索引处的值都表示了和对应单词在句子中出现的次数。这种方法会完全丢失输入中的结构性信息。\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
    " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
    " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
    " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]```\n",
    " \n",
    "我们还用过一种one-hot编码来表示输入，其中每个token都由一个n维数组表示。\n",
    " \n",
    " ```python\n",
    "[[0. 0. 0. ... 0. 0. 0.]\n",
    " [0. 0. 1. ... 0. 0. 0.]\n",
    " [0. 0. 0. ... 0. 0. 0.]\n",
    " ...\n",
    " [0. 0. 0. ... 0. 0. 0.]\n",
    " [0. 0. 0. ... 0. 0. 0.]\n",
    " [0. 0. 0. ... 0. 0. 0.]]\n",
    "```\n",
    "\n",
    "用这种方法，输入中的结构性信息得以保存，但它有两个主要缺点。当词汇表很大时，每个token的表示长度也会相应变得巨大，导致运算量的加大。同时，文本中的结构信息被保存了下来，但token的实际表示和其他token之间的关系并没有被存下来。\n",
    "\n",
    "在这个课程项目中，我们会了解学习 **嵌入层(embedding)** 以及它是如何解决我们目前接触的各种方法的问题。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qWro5T5qTJJL",
    "mdEditEnable": false
   },
   "source": [
    "## 概述\n",
    "\n",
    "* **目标:** 表示文本中包含内在结构关系的token。\n",
    "* **优点:**\n",
    "\t* 维度低，同时可以找到关系\n",
    "\t* 解释性强的token表示\n",
    "* **缺点：**\n",
    "\t* 无 !\n",
    "* **其他方面:** 有许多已经训练好的嵌入层可供选择。也可以训练自己的嵌入层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MlxioJLqx2Ls",
    "mdEditEnable": false
   },
   "source": [
    "\n",
    "## 嵌入层学习\n",
    "\n",
    "嵌入层的主要思想是将文本中词条的长度表示处理化为定值，而不再考虑词汇表中的词条数量。因此每个词条的表示就变成了 [1 X D] 而不是 [1 X V] (V 是词汇表的大小， D 是嵌入层的大小，通常为 50, 100, 200, 300 )。同时词条表示中的数字也不再是0和1，而是D维潜在空间中表示该词条的浮点数。如果嵌入层确实已经获取了词条之间的关系，那么我们就可以查看这个潜在空间然后确认这些已知的关系(我们会这么做的)。\n",
    "\n",
    "那么首先，应该如何学习嵌入层呢? 嵌入层的意义在于让词条的定义不再取决于它本身，而是它的上下文。这是几种不同的做法:\n",
    "\n",
    "1. 给出上下文中的单词，预测目标词 (CBOW - 连续词袋)\n",
    "2. 给定目标词，预测上下文的单词 (skip-gram)\n",
    "3. 给出一个词序列，预测下一个单词 (LM - 语言模型)\n",
    "\n",
    "所有这些方法都需要创建数据来训练模型。句子中的每个单词都会成为目标词，上下文的单词会由一个 **窗口(window)** 决定。下图显示了skip-gram 的过程，图中展示的窗口大小为 2。我们会对语料库中的每个句子都进行这个操作，从而生产出适用于非监督式学习的训练集。因为我们并没有对应上下文单词的正式标签，所以这会是一种非监督式学习技巧。核心思路是，相似的目标词汇会和相似的上下文一起出现，我们\n",
    "可以通过重复训练模型学习这种(上下文，目标词之间的)匹配关系。\n",
    "\n",
    "<img src=\"data/skipgram.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "\n",
    "我们可以用上面的任意一种方法学习嵌入层，他们互有胜负。你当然可以查看学习后的嵌入层进行比较，但实际上最有效的选择方法是在一个监督式学习的任务中直接验证性能，取最优。我们当然可以用PyTorch建模来学习嵌入层，但是现在我们会直接使用一个专门用于嵌入层和主题建模的库，它叫作 [Gensim](https://radimrehurek.com/gensim/)。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "9wr9S6965DD7",
    "outputId": "78009f70-4ae5-436e-b4e9-065ff5e9a86b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "import copy\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import json\n",
    "import nltk; nltk.download('punkt')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import urllib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5-sx-n9655TJ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    seed=1234,\n",
    "    data_file=\"data/harrypotter.txt\",\n",
    "    embedding_dim=100,\n",
    "    window=5,\n",
    "    min_count=3,\n",
    "    skip_gram=1, # 设为0使用CBOW\n",
    "    negative_sampling=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "irvgngOG5yqk",
    "outputId": "086fa4d4-57d3-42fe-ba5e-6f15d1fa518a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 文本分割成句子\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "with open(args.data_file, encoding='cp1252') as fp:\n",
    "    book = fp.read()\n",
    "sentences = tokenizer.tokenize(book)\n",
    "print (len(sentences))\n",
    "print (sentences[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VTREFDg47Vrx",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 预处理\n",
    "def preprocess_text(text):\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IX7pkCbg7WPK",
    "outputId": "95f9759f-6add-42c6-d4ab-62fcb7fbb58d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 数据清洗\n",
    "sentences = [preprocess_text(sentence) for sentence in sentences]\n",
    "print (sentences[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dsmEEVzX5ytO",
    "outputId": "ba5cf8a6-106d-49ed-a9f8-063e04d58eeb",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 为gensim对句子进行预处理\n",
    "sentences = [sentence.split(\" \") for sentence in sentences]\n",
    "print (sentences[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fa-DbjPW-KC3",
    "mdEditEnable": false
   },
   "source": [
    "当嵌入层的学习需要很大的词汇表时，事情很快就会变得复杂起来。之前我们提到，softmax 的反向传播会同时更新正确类和错误类的权重，这样每次反向传播得计算量就会变得巨大。一个变通方案是使用[负采样 negative sampling](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)，它只需要更新正确类和一些随机错误累的权重(negative_sampling=20)。我们有大量的训练数据，而训练数据中我们将多次看到和目标类相同的单词，所以我们可以使用负采样的方案而不会对模型性能产生明显影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wzkUkl-b5ywC",
    "outputId": "94d29f13-c47e-4c2c-915d-a4d930531346",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 由于底层是C语言编写优化，所以速度超级快\n",
    "model = Word2Vec(sentences=sentences, size=args.embedding_dim, \n",
    "                 window=args.window, min_count=args.min_count, \n",
    "                 sg=args.skip_gram, negative=args.negative_sampling)\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "gcMb9HUd7vS_",
    "outputId": "6be3ac88-a5c5-4614-8269-6c0547a6303d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 对每个词进行向量化\n",
    "model.wv.get_vector(\"potter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "BozlP5WG70Ak",
    "outputId": "a82275b3-8d43-41ea-89ec-7be3ccdae930",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 找到距离最近的单词(排除自己)\n",
    "model.wv.most_similar(positive=\"scar\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q-wgfMnH68vQ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 保存权重\n",
    "model.wv.save_word2vec_format('data/model.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DHUU_-TJz_7K",
    "mdEditEnable": false
   },
   "source": [
    "## 预训练嵌入层\n",
    "\n",
    "我们可以使用上面的任意一种方法从头开始学习嵌入层，另外我们也可以使用已经在数百万文档上经过训练的预训练嵌入层。目前流行的嵌入层包括Word2Vec (skip-gram)或GloVe (global word-word co-occurrence)。我们可以通过确认这些嵌入层捕获的有意义的语义关系来进行验证操作。\n",
    "\n",
    "我们可以用之前提到的任意一种方法从头开始学习嵌入测过，但其实直接使用在数百万份文档上训练好的预训练嵌入层也是一种机智的选择。目前使用广泛的嵌入层包括 Word2Vec (skip-gram) 和 GloVe (global word-word co-occurrence)。 我们可以通过确认这些嵌入层所能发现的语义关系来进行验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vJNngmZjglVg",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNacKuRNi8_x",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 解压文件 (可能需要三分钟)\n",
    "resp = urlopen('http://nlp.stanford.edu/data/glove.6B.zip')\n",
    "zipfile = ZipFile(BytesIO(resp.read()))\n",
    "zipfile.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "739183BA1CEF423A844CC138C6E198AB",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 写入嵌入层\n",
    "embeddings_file = 'glove.6B.{0}d.txt'.format(args.embedding_dim)\n",
    "# zipfile.extract(embeddings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SA5Y1BGbbpPo",
    "outputId": "bf3bd287-47bf-442e-c78b-9ec7eee5f70c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 保存word2vec格式的GloVe嵌入层到\n",
    "word2vec_output_file = '{0}.word2vec'.format(embeddings_file)\n",
    "glove2word2vec(embeddings_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bQp1pIJLiZw3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 加载嵌入层(可能需要一分钟)\n",
    "glove = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "HDpHnC0sbpVE",
    "outputId": "a19ed390-7c15-4897-d440-65137d170525",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# (king - man) + woman = ?\n",
    "glove.most_similar(positive=['woman', 'king'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "_1TatbmvbpYU",
    "outputId": "ec827143-0c7f-4c1b-f080-4d1458f53c1e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 找到距离最近的单词(排除自己)\n",
    "glove.wv.most_similar(positive=\"goku\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oMljB5MEbpdb",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 为绘图作降维\n",
    "X = glove[glove.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "pca_results = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Aok5pRqXqcCf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_embeddings(words, embeddings, pca_results):\n",
    "    for word in words:\n",
    "        index = embeddings.index2word.index(word)\n",
    "        plt.scatter(pca_results[index, 0], pca_results[index, 1])\n",
    "        plt.annotate(word, xy=(pca_results[index, 0], pca_results[index, 1]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 347
    },
    "colab_type": "code",
    "id": "NOdLFgTOrrBd",
    "outputId": "3ec4bd22-6c24-46c3-889b-0dceda5d536e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_embeddings(words=[\"king\", \"queen\", \"man\", \"woman\"], embeddings=glove, \n",
    "                pca_results=pca_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "6667pZttboaj",
    "outputId": "d3bdf331-e90f-448f-899e-4d0dcd252878",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 嵌入层的偏差\n",
    "glove.most_similar(positive=['woman', 'doctor'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tu3Bl-_yw9LA",
    "mdEditEnable": false
   },
   "source": [
    "## 使用嵌入层 \n",
    "下面是几种使用嵌入层的方法:\n",
    "\n",
    "1. 使用自己训练的嵌入层（用非监督数据集训练）\n",
    "2. 使用预训练的嵌入层（GloVe，word2vec等）\n",
    "3. 随机初始化的嵌入层\n",
    "\n",
    "选好了使用的嵌入层，你可以选择冻结它还是用监督式数据继续训练 (可能会导致过拟合)。这里我们会使用 GloVe 并选择冻结。这次的任务是预测给定标题的文章类型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8OlZB0vc1Hbb",
    "mdEditEnable": false
   },
   "source": [
    "### 配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2EAFB01B886A4B45A5940D89CA11D670",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install torch --upgrade -i https://pypi.tuna.tsinghua.edu.cn/simple --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T3daO-e9wpXe",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "import collections\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ye64bXPrwpaQ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 设置Numpy和PyTorch的种子\n",
    "def set_seeds(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "# 创建目录\n",
    "def create_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "m5_jiImZ1NAr",
    "outputId": "238e95dd-3df4-45c6-e6dd-ebdeb33b8a88",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 参数\n",
    "args = Namespace(\n",
    "    seed=1234,\n",
    "    cuda=True,\n",
    "    shuffle=True,\n",
    "    data_file=\"data/news.csv\",\n",
    "    split_data_file=\"data/split_news.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"news\",\n",
    "    train_size=0.7,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    cutoff=25, # token must appear at least <cutoff> times to be in SequenceVocabulary\n",
    "    num_epochs=5,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=64,\n",
    "    num_filters=100,\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=100,\n",
    "    dropout_p=0.1,\n",
    ")\n",
    "\n",
    "# 设置种子\n",
    "set_seeds(seed=args.seed, cuda=args.cuda)\n",
    "\n",
    "# 创建保存目录\n",
    "create_dirs(args.save_dir)\n",
    "\n",
    "# 拓展路径\n",
    "args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "# 检查GPU可用性\n",
    "if torch.cuda.is_available():\n",
    "    args.cuda = True\n",
    "else:\n",
    "    args.cude = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GkNclM3I1XMg",
    "mdEditEnable": false
   },
   "source": [
    "### 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vk6ucvgp1NDD",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uVQNGEYa1NFu",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "url = \"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/data/news.csv\"\n",
    "response = urllib.request.urlopen(url)\n",
    "html = response.read()\n",
    "with open(args.data_file, 'wb') as fp:\n",
    "    fp.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "aTU6ENCu1NIc",
    "outputId": "6a7f2c48-4c8f-4283-accb-9d96a61ecbea",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 原始数据\n",
    "df = pd.read_csv(args.data_file, header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "xIouG0f71NK4",
    "outputId": "1f49fb96-8ef6-4772-89f7-6d9a605cda21",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 按类别拆分数据集\n",
    "by_category = collections.defaultdict(list)\n",
    "for _, row in df.iterrows():\n",
    "    by_category[row.category].append(row.to_dict())\n",
    "for category in by_category:\n",
    "    print (\"{0}: {1}\".format(category, len(by_category[category])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YZyFpi791NNP",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 新建切分数据集\n",
    "final_list = []\n",
    "for _, item_list in sorted(by_category.items()):\n",
    "    if args.shuffle:\n",
    "        np.random.shuffle(item_list)\n",
    "    n = len(item_list)\n",
    "    n_train = int(args.train_size*n)\n",
    "    n_val = int(args.val_size*n)\n",
    "    n_test = int(args.test_size*n)\n",
    "\n",
    "  # 给数据指定切分属性\n",
    "    for item in item_list[:n_train]:\n",
    "        item['split'] = 'train'\n",
    "    for item in item_list[n_train:n_train+n_val]:\n",
    "        item['split'] = 'val'\n",
    "    for item in item_list[n_train+n_val:]:\n",
    "        item['split'] = 'test'  \n",
    "\n",
    "    # 添加到列表\n",
    "    final_list.extend(item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "NLrQTVoO1NP4",
    "outputId": "c0800a8b-0c18-450c-87bf-04971bc92075",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 切分后的数据集dataframe\n",
    "split_df = pd.DataFrame(final_list)\n",
    "split_df[\"split\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v-HSMGwh1NUu",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 预处理\n",
    "def preprocess_text(text):\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text\n",
    "    \n",
    "split_df.title = split_df.title.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "gvNVV7n11NXX",
    "outputId": "bcb4a3dd-ba84-4475-dfe1-161f1f725684",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 存为CSV文件\n",
    "split_df.to_csv(args.split_data_file, index=False)\n",
    "split_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cPDWGdlE5vye",
    "mdEditEnable": false
   },
   "source": [
    "### 词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EMR5Y3D95v6P",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, token_to_idx=None):\n",
    "\n",
    "        # 词条转换为索引\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self.token_to_idx = token_to_idx\n",
    "\n",
    "        # 索引转换为词条\n",
    "        self.idx_to_token = {idx: token \\\n",
    "                             for token, idx in self.token_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self.token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        return [self.add_token[token] for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        return self.token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        if index not in self.idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self.idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "WKnfFKY95v81",
    "outputId": "03e03491-84aa-45f8-e004-1e9ba58c19c5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Vocabulary instance\n",
    "category_vocab = Vocabulary()\n",
    "for index, row in df.iterrows():\n",
    "    category_vocab.add_token(row.category)\n",
    "print (category_vocab) # __str__\n",
    "print (len(category_vocab)) # __len__\n",
    "index = category_vocab.lookup_token(\"Business\")\n",
    "print (index)\n",
    "print (category_vocab.lookup_index(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jxO9-6vhBCSO",
    "mdEditEnable": false
   },
   "source": [
    "### 序列词汇表\n",
    "\n",
    "接下来我们将为文章标题创建词汇表类，它由一系列词条构成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ip8ViI2v_7Y2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S2LTR5NjBDCG",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self.mask_token = mask_token\n",
    "        self.unk_token = unk_token\n",
    "        self.begin_seq_token = begin_seq_token\n",
    "        self.end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self.mask_token)\n",
    "        self.unk_index = self.add_token(self.unk_token)\n",
    "        self.begin_seq_index = self.add_token(self.begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self.end_seq_token)\n",
    "        \n",
    "        # 索引转换为词条\n",
    "        self.idx_to_token = {idx: token \\\n",
    "                             for token, idx in self.token_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self.unk_token,\n",
    "                         'mask_token': self.mask_token,\n",
    "                         'begin_seq_token': self.begin_seq_token,\n",
    "                         'end_seq_token': self.end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        return self.token_to_idx.get(token, self.unk_index)\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        if index not in self.idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the SequenceVocabulary\" % index)\n",
    "        return self.idx_to_token[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"<SequenceVocabulary(size=%d)>\" % len(self.token_to_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "DvDPVDbsBDE4",
    "outputId": "09913e32-1588-4410-9d22-6f4024ac3193",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 得到词数\n",
    "word_counts = Counter()\n",
    "for title in split_df.title:\n",
    "    for token in title.split(\" \"):\n",
    "        if token not in string.punctuation:\n",
    "            word_counts[token] += 1\n",
    "\n",
    "# 创建SequenceVocabulary实例\n",
    "title_vocab = SequenceVocabulary()\n",
    "for word, word_count in word_counts.items():\n",
    "    if word_count >= args.cutoff:\n",
    "        title_vocab.add_token(word)\n",
    "print (title_vocab) # __str__\n",
    "print (len(title_vocab)) # __len__\n",
    "index = title_vocab.lookup_token(\"general\")\n",
    "print (index)\n",
    "print (title_vocab.lookup_index(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6TuUHvlI6JGh",
    "mdEditEnable": false
   },
   "source": [
    "### 向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s8IsOPHT5v_i",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NewsVectorizer(object):\n",
    "    def __init__(self, title_vocab, category_vocab):\n",
    "        self.title_vocab = title_vocab\n",
    "        self.category_vocab = category_vocab\n",
    "\n",
    "    def vectorize(self, title):\n",
    "        indices = [self.title_vocab.lookup_token(token) for token in title.split(\" \")]\n",
    "        indices = [self.title_vocab.begin_seq_index] + indices + \\\n",
    "            [self.title_vocab.end_seq_index]\n",
    "        \n",
    "        # 创建向量\n",
    "        title_length = len(indices)\n",
    "        vector = np.zeros(title_length, dtype=np.int64)\n",
    "        vector[:len(indices)] = indices\n",
    "\n",
    "        return vector\n",
    "    \n",
    "    def unvectorize(self, vector):\n",
    "        tokens = [self.title_vocab.lookup_index(index) for index in vector]\n",
    "        title = \" \".join(token for token in tokens)\n",
    "        return title\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df, cutoff):\n",
    "        \n",
    "        # 创建分类的词汇表实例\n",
    "        category_vocab = Vocabulary()        \n",
    "        for category in sorted(set(df.category)):\n",
    "            category_vocab.add_token(category)\n",
    "\n",
    "        # 获取长度\n",
    "        word_counts = Counter()\n",
    "        for title in df.title:\n",
    "            for token in title.split(\" \"):\n",
    "                word_counts[token] += 1\n",
    "        \n",
    "        # 创建标题的词汇表实例\n",
    "        title_vocab = SequenceVocabulary()\n",
    "        for word, word_count in word_counts.items():\n",
    "            if word_count >= cutoff:\n",
    "                title_vocab.add_token(word)\n",
    "        \n",
    "        return cls(title_vocab, category_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        title_vocab = SequenceVocabulary.from_serializable(contents['title_vocab'])\n",
    "        category_vocab = Vocabulary.from_serializable(contents['category_vocab'])\n",
    "        return cls(title_vocab=title_vocab, category_vocab=category_vocab)\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {'title_vocab': self.title_vocab.to_serializable(),\n",
    "                'category_vocab': self.category_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "p2blo9sT5wCS",
    "outputId": "8a134781-caca-47b7-e7cc-913ca3a1d9fa",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 向量化实例\n",
    "vectorizer = NewsVectorizer.from_dataframe(split_df, cutoff=args.cutoff)\n",
    "print (vectorizer.title_vocab)\n",
    "print (vectorizer.category_vocab)\n",
    "vectorized_title = vectorizer.vectorize(preprocess_text(\n",
    "    \"Roger Federer wins the Wimbledon tennis tournament.\"))\n",
    "print (np.shape(vectorized_title))\n",
    "print (vectorized_title)\n",
    "print (vectorizer.unvectorize(vectorized_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jTpYV1FkHBTi",
    "mdEditEnable": false
   },
   "source": [
    "### 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_PcKZGbYIVA7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lrssjuxb1NZy",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, df, vectorizer):\n",
    "        self.df = df\n",
    "        self.vectorizer = vectorizer\n",
    "        \n",
    "        # 最大标题长度\n",
    "        get_length = lambda title: len(title.split(\" \"))\n",
    "        self.max_seq_length = max(map(get_length, df.title)) + 2 # (<BEGIN> + <END>)\n",
    "\n",
    "        # 切分数据\n",
    "        self.train_df = self.df[self.df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "        self.val_df = self.df[self.df.split=='val']\n",
    "        self.val_size = len(self.val_df)\n",
    "        self.test_df = self.df[self.df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "        self.lookup_dict = {'train': (self.train_df, self.train_size), \n",
    "                            'val': (self.val_df, self.val_size),\n",
    "                            'test': (self.test_df, self.test_size)}\n",
    "        self.set_split('train')\n",
    "\n",
    "        # 类权重(用于样本失衡)\n",
    "        class_counts = df.category.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self.vectorizer.category_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, split_data_file, cutoff):\n",
    "        df = pd.read_csv(split_data_file, header=0)\n",
    "        train_df = df[df.split=='train']\n",
    "        return cls(df, NewsVectorizer.from_dataframe(train_df, cutoff))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, split_data_file, vectorizer_filepath):\n",
    "        df = pd.read_csv(split_data_file, header=0)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(df, vectorizer)\n",
    "\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return NewsVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self.vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self.target_split = split\n",
    "        self.target_df, self.target_size = self.lookup_dict[split]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Dataset(split={0}, size={1})\".format(\n",
    "            self.target_split, self.target_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.target_df.iloc[index]\n",
    "        title_vector = self.vectorizer.vectorize(row.title)\n",
    "        category_index = self.vectorizer.category_vocab.lookup_token(row.category)\n",
    "        return {'title': title_vector, 'category': category_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "\n",
    "    def generate_batches(self, batch_size, collate_fn, shuffle=True, \n",
    "                         drop_last=False, device=\"cpu\"):\n",
    "        dataloader = DataLoader(dataset=self, batch_size=batch_size,\n",
    "                                collate_fn=collate_fn, shuffle=shuffle, \n",
    "                                drop_last=drop_last)\n",
    "        for data_dict in dataloader:\n",
    "            out_data_dict = {}\n",
    "            for name, tensor in data_dict.items():\n",
    "                out_data_dict[name] = data_dict[name].to(device)\n",
    "            yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "e9I-AFKsIP2e",
    "outputId": "913612d9-805a-4c5c-df5a-578fd1b8c935",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 数据集实例\n",
    "dataset = NewsDataset.load_dataset_and_make_vectorizer(args.split_data_file, \n",
    "                                                       cutoff=args.cutoff)\n",
    "print (dataset) # __str__\n",
    "title_vector = dataset[5]['title'] # __getitem__\n",
    "print (title_vector)\n",
    "print (dataset.vectorizer.unvectorize(title_vector))\n",
    "print (dataset.class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "202KnN2lLj61",
    "mdEditEnable": false
   },
   "source": [
    "### 模型\n",
    "\n",
    "input（输入层） → embedding（嵌入层） → conv （卷积层）→ FC （全连接层)\n",
    "\n",
    "尽管输入是各个单词，在这里我们将依旧会使用一维的卷积层([nn.Conv1D](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv1d))。我们并不会按字符表示他们，而会采用 shape ~$\\in \\mathbb{R}^{NXSXE}~$ 的输入形状。\n",
    "\n",
    "*其中:*\n",
    "* N = 每个批处理的训练样本数\n",
    "* S = 最大句子长度\n",
    "* E = 单词级别的词嵌入维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j-Xgp0F3INRA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lfw7csveHApA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NewsModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_embeddings, num_input_channels, \n",
    "                 num_channels, hidden_dim, num_classes, dropout_p, \n",
    "                 pretrained_embeddings=None, freeze_embeddings=False,\n",
    "                 padding_idx=0):\n",
    "        super(NewsModel, self).__init__()\n",
    "        \n",
    "        if pretrained_embeddings is None:\n",
    "            self.embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
    "                                          num_embeddings=num_embeddings,\n",
    "                                          padding_idx=padding_idx)\n",
    "        else:\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
    "                                           num_embeddings=num_embeddings,\n",
    "                                           padding_idx=padding_idx,\n",
    "                                           _weight=pretrained_embeddings)\n",
    "        \n",
    "        # 卷积层权重\n",
    "        self.conv = nn.ModuleList([nn.Conv1d(num_input_channels, num_channels, \n",
    "                                             kernel_size=f) for f in [2,3,4]])\n",
    "     \n",
    "        # 全连接层权重\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc1 = nn.Linear(num_channels*3, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "        if freeze_embeddings:\n",
    "            self.embeddings.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, x_in, channel_first=False, apply_softmax=False):\n",
    "        \n",
    "        # 嵌入\n",
    "        x_in = self.embeddings(x_in)\n",
    "\n",
    "        # 重置输入形状\n",
    "        if not channel_first:\n",
    "            x_in = x_in.transpose(1, 2)\n",
    "            \n",
    "        # 卷积层输出\n",
    "        z1 = self.conv[0](x_in)\n",
    "        z1 = F.max_pool1d(z1, z1.size(2)).squeeze(2)\n",
    "        z2 = self.conv[1](x_in)\n",
    "        z2 = F.max_pool1d(z2, z2.size(2)).squeeze(2)\n",
    "        z3 = self.conv[2](x_in)\n",
    "        z3 = F.max_pool1d(z3, z3.size(2)).squeeze(2)\n",
    "        \n",
    "        # 拼接卷积层输出\n",
    "        z = torch.cat([z1, z2, z3], 1)\n",
    "\n",
    "        # 全连接层\n",
    "        z = self.dropout(z)\n",
    "        z = self.fc1(z)\n",
    "        y_pred = self.fc2(z)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            y_pred = F.softmax(y_pred, dim=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d7_4SiLkKji8",
    "mdEditEnable": false
   },
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UQAIfKN2HAtN",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RwgZASWYHAwB",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, dataset, model, model_state_file, save_dir, device, shuffle, \n",
    "               num_epochs, batch_size, learning_rate, early_stopping_criteria):\n",
    "        self.dataset = dataset\n",
    "        self.class_weights = dataset.class_weights.to(device)\n",
    "        self.model = model.to(device)\n",
    "        self.save_dir = save_dir\n",
    "        self.device = device\n",
    "        self.shuffle = shuffle\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_func = nn.CrossEntropyLoss(self.class_weights)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer=self.optimizer, mode='min', factor=0.5, patience=1)\n",
    "        self.train_state = {\n",
    "            'stop_early': False, \n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'early_stopping_criteria': early_stopping_criteria,\n",
    "            'learning_rate': learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': model_state_file}\n",
    "    \n",
    "    def update_train_state(self):\n",
    "\n",
    "        # 打印checkpoint信息\n",
    "        print (\"[EPOCH]: {0:02d} | [LR]: {1} | [TRAIN LOSS]: {2:.2f} | [TRAIN ACC]: {3:.1f}% | [VAL LOSS]: {4:.2f} | [VAL ACC]: {5:.1f}%\".format(\n",
    "          self.train_state['epoch_index'], self.train_state['learning_rate'], \n",
    "            self.train_state['train_loss'][-1], self.train_state['train_acc'][-1], \n",
    "            self.train_state['val_loss'][-1], self.train_state['val_acc'][-1]))\n",
    "\n",
    "        # 至少保存一次模型\n",
    "        if self.train_state['epoch_index'] == 0:\n",
    "            torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
    "            self.train_state['stop_early'] = False\n",
    "\n",
    "        # 如果模型性能表现有提升，再次保存\n",
    "        elif self.train_state['epoch_index'] >= 1:\n",
    "            loss_tm1, loss_t = self.train_state['val_loss'][-2:]\n",
    "\n",
    "            # 如果损失增大\n",
    "            if loss_t >= self.train_state['early_stopping_best_val']:\n",
    "                # 更新步数\n",
    "                self.train_state['early_stopping_step'] += 1\n",
    "\n",
    "            # 损失变小\n",
    "            else:\n",
    "                # 保存最优的模型\n",
    "                if loss_t < self.train_state['early_stopping_best_val']:\n",
    "                    torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
    "\n",
    "                # 重置early stopping 的步数\n",
    "                self.train_state['early_stopping_step'] = 0\n",
    "\n",
    "            # 是否需要early stop?\n",
    "            self.train_state['stop_early'] = self.train_state['early_stopping_step'] \\\n",
    "              >= self.train_state['early_stopping_criteria']\n",
    "        return self.train_state\n",
    "  \n",
    "    def compute_accuracy(self, y_pred, y_target):\n",
    "        _, y_pred_indices = y_pred.max(dim=1)\n",
    "        n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "        return n_correct / len(y_pred_indices) * 100\n",
    "    \n",
    "    def pad_seq(self, seq, length):\n",
    "        vector = np.zeros(length, dtype=np.int64)\n",
    "        vector[:len(seq)] = seq\n",
    "        vector[len(seq):] = self.dataset.vectorizer.title_vocab.mask_index\n",
    "        return vector\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        \n",
    "        # 深度拷贝\n",
    "        batch_copy = copy.deepcopy(batch)\n",
    "        processed_batch = {\"title\": [], \"category\": []}\n",
    "        \n",
    "        # 得到最长序列长度\n",
    "        max_seq_len = max([len(sample[\"title\"]) for sample in batch_copy])\n",
    "        \n",
    "        # 填充\n",
    "        for i, sample in enumerate(batch_copy):\n",
    "            seq = sample[\"title\"]\n",
    "            category = sample[\"category\"]\n",
    "            padded_seq = self.pad_seq(seq, max_seq_len)\n",
    "            processed_batch[\"title\"].append(padded_seq)\n",
    "            processed_batch[\"category\"].append(category)\n",
    "            \n",
    "        # 转换为合适的tensor\n",
    "        processed_batch[\"title\"] = torch.LongTensor(\n",
    "            processed_batch[\"title\"])\n",
    "        processed_batch[\"category\"] = torch.LongTensor(\n",
    "            processed_batch[\"category\"])\n",
    "        \n",
    "        return processed_batch    \n",
    "  \n",
    "    def run_train_loop(self):\n",
    "        for epoch_index in range(self.num_epochs):\n",
    "            self.train_state['epoch_index'] = epoch_index\n",
    "      \n",
    "            # 编辑训练集\n",
    "            # 初始化批生成器, 将损失和准确率归零，设置为训练模式\n",
    "            self.dataset.set_split('train')\n",
    "            batch_generator = self.dataset.generate_batches(\n",
    "                batch_size=self.batch_size, collate_fn=self.collate_fn, \n",
    "                shuffle=self.shuffle, device=self.device)\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            self.model.train()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "                # 梯度归零\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # 计算输出\n",
    "                y_pred = self.model(batch_dict['title'])\n",
    "\n",
    "                # 计算损失\n",
    "                loss = self.loss_func(y_pred, batch_dict['category'])\n",
    "                loss_t = loss.item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # 反向传播\n",
    "                loss.backward()\n",
    "\n",
    "                # 更新梯度\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # 计算准确率\n",
    "                acc_t = self.compute_accuracy(y_pred, batch_dict['category'])\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            self.train_state['train_loss'].append(running_loss)\n",
    "            self.train_state['train_acc'].append(running_acc)\n",
    "\n",
    "            # 遍历验证集\n",
    "            # 初始化批生成器, 将损失和准确率归零，设置为运行模式\n",
    "            self.dataset.set_split('val')\n",
    "            batch_generator = self.dataset.generate_batches(\n",
    "                batch_size=self.batch_size, collate_fn=self.collate_fn, \n",
    "                shuffle=self.shuffle, device=self.device)\n",
    "            running_loss = 0.\n",
    "            running_acc = 0.\n",
    "            self.model.eval()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "                # 计算输出\n",
    "                y_pred =  self.model(batch_dict['title'])\n",
    "\n",
    "                # 计算损失\n",
    "                loss = self.loss_func(y_pred, batch_dict['category'])\n",
    "                loss_t = loss.to(\"cpu\").item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # 计算准确率\n",
    "                acc_t = self.compute_accuracy(y_pred, batch_dict['category'])\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            self.train_state['val_loss'].append(running_loss)\n",
    "            self.train_state['val_acc'].append(running_acc)\n",
    "\n",
    "            self.train_state = self.update_train_state()\n",
    "            self.scheduler.step(self.train_state['val_loss'][-1])\n",
    "            if self.train_state['stop_early']:\n",
    "                break\n",
    "          \n",
    "    def run_test_loop(self):\n",
    "        # 初始化批生成器, 将损失和准确率归零，设置为运行模式\n",
    "        self.dataset.set_split('test')\n",
    "        batch_generator = self.dataset.generate_batches(\n",
    "                batch_size=self.batch_size, collate_fn=self.collate_fn, \n",
    "                shuffle=self.shuffle, device=self.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        self.model.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # 计算输出\n",
    "            y_pred =  self.model(batch_dict['title'])\n",
    "\n",
    "            # 计算损失\n",
    "            loss = self.loss_func(y_pred, batch_dict['category'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # 计算准确率\n",
    "            acc_t = self.compute_accuracy(y_pred, batch_dict['category'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "        self.train_state['test_loss'] = running_loss\n",
    "        self.train_state['test_acc'] = running_acc\n",
    "    \n",
    "    def plot_performance(self):\n",
    "        # 设置图大小\n",
    "        plt.figure(figsize=(15,5))\n",
    "\n",
    "        # 画出损失\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Loss\")\n",
    "        plt.plot(trainer.train_state[\"train_loss\"], label=\"train\")\n",
    "        plt.plot(trainer.train_state[\"val_loss\"], label=\"val\")\n",
    "        plt.legend(loc='upper right')\n",
    "\n",
    "        # 画出准确率\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.plot(trainer.train_state[\"train_acc\"], label=\"train\")\n",
    "        plt.plot(trainer.train_state[\"val_acc\"], label=\"val\")\n",
    "        plt.legend(loc='lower right')\n",
    "\n",
    "        # 存图\n",
    "        plt.savefig(os.path.join(self.save_dir, \"performance.png\"))\n",
    "\n",
    "        # 展示图\n",
    "        plt.show()\n",
    "    \n",
    "    def save_train_state(self):\n",
    "        with open(os.path.join(self.save_dir, \"train_state.json\"), \"w\") as fp:\n",
    "            json.dump(self.train_state, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "LJfKGc9cHA0y",
    "outputId": "2185fea4-a2c6-4986-80d4-c781ab1ea394",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 初始化\n",
    "dataset = NewsDataset.load_dataset_and_make_vectorizer(args.split_data_file, \n",
    "                                                       cutoff=args.cutoff)\n",
    "dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.vectorizer\n",
    "model = NewsModel(embedding_dim=args.embedding_dim, \n",
    "                  num_embeddings=len(vectorizer.title_vocab), \n",
    "                  num_input_channels=args.embedding_dim, \n",
    "                  num_channels=args.num_filters, hidden_dim=args.hidden_dim, \n",
    "                  num_classes=len(vectorizer.category_vocab), \n",
    "                  dropout_p=args.dropout_p, pretrained_embeddings=None, \n",
    "                  padding_idx=vectorizer.title_vocab.mask_index)\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "wDxPyGIVKoUK",
    "outputId": "fabb05e9-25b3-43ea-ce72-713a0bae1426",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 训练\n",
    "trainer = Trainer(dataset=dataset, model=model, \n",
    "                  model_state_file=args.model_state_file, \n",
    "                  save_dir=args.save_dir, device=args.device,\n",
    "                  shuffle=args.shuffle, num_epochs=args.num_epochs, \n",
    "                  batch_size=args.batch_size, learning_rate=args.learning_rate, \n",
    "                  early_stopping_criteria=args.early_stopping_criteria)\n",
    "trainer.run_train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "V9Ruxpc8WT17",
    "outputId": "f776e4cb-0d62-4716-b51a-f5039e34d884",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 画出训练过程\n",
    "trainer.plot_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "xRphYPaVWTzY",
    "outputId": "a29f02c4-b004-4756-c06e-2438eb0d306e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 测试集上的性能\n",
    "trainer.run_test_loop()\n",
    "print(\"Test loss: {0:.2f}\".format(trainer.train_state['test_loss']))\n",
    "print(\"Test Accuracy: {0:.1f}%\".format(trainer.train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z_Zrw9rIWTwU",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 保存结果\n",
    "trainer.save_train_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HH_328MjWoM-",
    "mdEditEnable": false
   },
   "source": [
    "### 使用GloVe嵌入层\n",
    "\n",
    "\n",
    "上面的代码中，我们使用了随机初始化的嵌入层获得了还ok的性能。但是一定要记住，用这种方法生成的嵌入层在别的数据集上大概率会出现过拟合。\n",
    "现在我们要使用预训练的 GloVe 来初始化嵌入层，然后会先冻结嵌入层(这样他们在训练期间就不会有变化)在监督式学习任务里训练模型，接着不冻结嵌入层(这样会继续训练)再训练，这样来比较性能。\n",
    "\n",
    "```python\n",
    "pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "self.embeddings = nn.Embedding(embedding_dim=embedding_dim, \n",
    "                               num_embeddings=num_embeddings, \n",
    "                               padding_idx=padding_idx, \n",
    "                               _weight=pretrained_embeddings)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r1EAWPBEKoaS",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_glove_embeddings(embeddings_file):\n",
    "    word_to_idx = {}\n",
    "    embeddings = []\n",
    "\n",
    "    with open(embeddings_file, \"r\") as fp:\n",
    "        for index, line in enumerate(fp):\n",
    "            line = line.split(\" \")\n",
    "            word = line[0]\n",
    "            word_to_idx[word] = index\n",
    "            embedding_i = np.array([float(val) for val in line[1:]])\n",
    "            embeddings.append(embedding_i)\n",
    "\n",
    "    return word_to_idx, np.stack(embeddings)\n",
    "\n",
    "def make_embeddings_matrix(words):\n",
    "    word_to_idx, glove_embeddings = load_glove_embeddings(embeddings_file)\n",
    "    embedding_dim = glove_embeddings.shape[1]\n",
    "    embeddings = np.zeros((len(words), embedding_dim))\n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_to_idx:\n",
    "            embeddings[i, :] = glove_embeddings[word_to_idx[word]]\n",
    "        else:\n",
    "            embedding_i = torch.zeros(1, embedding_dim)\n",
    "            nn.init.xavier_uniform_(embedding_i)\n",
    "            embeddings[i, :] = embedding_i\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FZBLYjtWKoX5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "args.use_glove_embeddings = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Gj77_BmoHA3s",
    "outputId": "30f47b5b-310e-4bcc-d25c-07723dedb0b6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 初始化\n",
    "dataset = NewsDataset.load_dataset_and_make_vectorizer(args.split_data_file, \n",
    "                                                       cutoff=args.cutoff)\n",
    "dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.vectorizer\n",
    "\n",
    "# 创建嵌入层\n",
    "embeddings = None\n",
    "if args.use_glove_embeddings:\n",
    "    embeddings_file = 'glove.6B.{0}d.txt'.format(args.embedding_dim)\n",
    "    words = vectorizer.title_vocab.token_to_idx.keys()\n",
    "    embeddings = make_embeddings_matrix(words=words)\n",
    "    print (\"<Embeddings(words={0}, dim={1})>\".format(\n",
    "        np.shape(embeddings)[0], np.shape(embeddings)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "CC4ofLswt-D3",
    "outputId": "d183b8e5-f418-4b8c-f58e-20f06b31982e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "model = NewsModel(embedding_dim=args.embedding_dim, \n",
    "                  num_embeddings=len(vectorizer.title_vocab), \n",
    "                  num_input_channels=args.embedding_dim, \n",
    "                  num_channels=args.num_filters, hidden_dim=args.hidden_dim, \n",
    "                  num_classes=len(vectorizer.category_vocab), \n",
    "                  dropout_p=args.dropout_p, pretrained_embeddings=embeddings, \n",
    "                  padding_idx=vectorizer.title_vocab.mask_index)\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "44gVZbKbHA7R",
    "outputId": "05a65ccf-bf15-42dd-84a7-ffe120aca5cf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 训练\n",
    "trainer = Trainer(dataset=dataset, model=model, \n",
    "                  model_state_file=args.model_state_file, \n",
    "                  save_dir=args.save_dir, device=args.device,\n",
    "                  shuffle=args.shuffle, num_epochs=args.num_epochs, \n",
    "                  batch_size=args.batch_size, learning_rate=args.learning_rate, \n",
    "                  early_stopping_criteria=args.early_stopping_criteria)\n",
    "trainer.run_train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "98MqlEQ0sfTs",
    "outputId": "ef300406-fa50-41e2-ed77-f558520cc8c3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 查看性能\n",
    "trainer.plot_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "wYGUHK0GsfhP",
    "outputId": "7997fa4f-1633-4137-886b-ac10c5a31e4b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 测试性能\n",
    "trainer.run_test_loop()\n",
    "print(\"Test loss: {0:.2f}\".format(trainer.train_state['test_loss']))\n",
    "print(\"Test Accuracy: {0:.1f}%\".format(trainer.train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GO8v_uUisfd9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 保存所有结果\n",
    "trainer.save_train_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XrMm4K61t21h",
    "mdEditEnable": false
   },
   "source": [
    "### 冻结嵌入层\n",
    "\n",
    "\n",
    "现在冻结 GloVe 嵌入层并且在监督式学习任务上进行训练。模型代码里唯一需要改变的是打开 `freeze_embeddings`:\n",
    "\n",
    "```python\n",
    "if freeze_embeddings:\n",
    "    self.embeddings.weight.requires_grad = False\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ilV_QbtktzH3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "args.freeze_embeddings = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "7NdD-iP6tzFQ",
    "outputId": "52f0faa1-9ec2-44de-8767-71b9e6f31c2c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 初始化模型\n",
    "model = NewsModel(embedding_dim=args.embedding_dim, \n",
    "                  num_embeddings=len(vectorizer.title_vocab), \n",
    "                  num_input_channels=args.embedding_dim, \n",
    "                  num_channels=args.num_filters, hidden_dim=args.hidden_dim, \n",
    "                  num_classes=len(vectorizer.category_vocab), \n",
    "                  dropout_p=args.dropout_p, pretrained_embeddings=embeddings,\n",
    "                  freeze_embeddings=args.freeze_embeddings,\n",
    "                  padding_idx=vectorizer.title_vocab.mask_index)\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "6NJhm2EOtzCV",
    "outputId": "775c7f44-2346-459f-a0dd-d31244330b87",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 训练\n",
    "trainer = Trainer(dataset=dataset, model=model, \n",
    "                  model_state_file=args.model_state_file, \n",
    "                  save_dir=args.save_dir, device=args.device,\n",
    "                  shuffle=args.shuffle, num_epochs=args.num_epochs, \n",
    "                  batch_size=args.batch_size, learning_rate=args.learning_rate, \n",
    "                  early_stopping_criteria=args.early_stopping_criteria)\n",
    "trainer.run_train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "oojDHLowty_9",
    "outputId": "fffbfaf7-fa75-43d9-e70a-d3200ac80013",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 查看性能\n",
    "trainer.plot_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "rFYs9dH3ui66",
    "outputId": "89923358-2da3-4a32-f504-986136cc134b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 测试性能\n",
    "trainer.run_test_loop()\n",
    "print(\"Test loss: {0:.2f}\".format(trainer.train_state['test_loss']))\n",
    "print(\"Test Accuracy: {0:.1f}%\".format(trainer.train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vKDmGXAyukpV",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 保存结果\n",
    "trainer.save_train_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P4yiDxOavmJy",
    "mdEditEnable": false
   },
   "source": [
    "\n",
    "通过对比发现，使用没有冻结的 GloVe嵌入层 在测试集上取得了最好的性能。由于不同的任务会产生不同的结果，所以还是需要根据实验来选择是否进行冻结操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78C95A361E7E485A8847B8422194D24F",
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
