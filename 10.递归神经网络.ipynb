{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OLIxEDq6VhvZ",
    "mdEditEnable": false
   },
   "source": [
    "## 递归神经网络 Recurrent Neural Networks\n",
    "\n",
    "<img src=\"data/logo.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "当处理序列化数据时(时间序列，句子，等等) 输入的顺训对于任务是否能顺利完成至关重要。递归神经网络(RNN)会把从之前的输入学习到的信息和新的输入结合处理，这个课程项目中我们将会学习如何创建，并使用序列化数据用RNN建模。\n",
    "\n",
    "\n",
    "<img src=\"data/rnn.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dIXlGMExJD6w",
    "mdEditEnable": false
   },
   "source": [
    "## 概览\n",
    "\n",
    "* **目标:** 从之前的输入学习到的信息和新的输入结合处理, 从而处理序列化数据\n",
    "\n",
    "* **优点:**\n",
    "\t* 把序列信息和前置输入有机结合处理\n",
    "\t* 生成序列时进行条件判断。\n",
    "* **缺点:**\n",
    "\t\t* 每一时间步输出的预测都依赖于上一步的预测输出，所以RNN很难并行计算。\n",
    "\t\t* 处理很长的序列数据时，会出现内存和运算问题。\n",
    "\t\t* 模型的可解释性比较困难，但是有一些好用的[工具和技巧](https://arxiv.org/abs/1506.02078)会检查RNN中的激活函数来推断哪一部分数据正在被处理。\n",
    "* **其他:**\n",
    "\t\t* 如何进行构架变化从而让RNN变得更快，解释性更好，其实一直是一个热门的研究课题。\t\t\n",
    "\n",
    "<img src=\"data/rnn2.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "RNN 每一时间步的前馈网络 ~$X_t~$:\n",
    "\n",
    "~$h_t = tanh(W_{hh}h_{t-1} + W_{xh}X_t+b_h)~$\n",
    "\n",
    "~$y_t = W_{hy}h_t + b_y~$\n",
    "\n",
    "~$P(y) = softmax(y_t) = \\frac{e^y}{\\sum e^y}~$\n",
    "\n",
    "*其中*:\n",
    "* ~$X_t~$ = 时间步t时刻的输入 | ~$\\in \\mathbb{R}^{NXE}~$ (~$N~$ 批次大小, ~$E~$ 是嵌入层维度)\n",
    "* ~$W_{hh}~$ = 隐藏单元权重| ~$\\in \\mathbb{R}^{HXH}~$ (~$H~$ 隐藏单元维度)\n",
    "* ~$h_{t-1}~$ = 上一个时间步的隐藏状态 ~$\\in \\mathbb{R}^{NXH}~$\n",
    "* ~$W_{xh}~$ = 输入权重| $\\in \\mathbb{R}^{EXH}~$\n",
    "* ~$b_h~$ = 隐藏单元偏差 ~$\\in \\mathbb{R}^{HX1}~$\n",
    "* ~$W_{hy}~$ = 输出权重 | ~$\\in \\mathbb{R}^{HXC}~$ (~$C~$ 类别数量)\n",
    "* ~$b_y~$ = 输出偏差 ~$\\in \\mathbb{R}^{CX1}~$\n",
    "\n",
    "对每个时间步的输入 ~$(X_{t+1}, X_{t+2}, ..., X_{N})~$ 都进行这一套操作来得到每个时间步的预测输出。\n",
    "\n",
    "**注意:** 在每个时间步的开始，前一个隐藏状态~$h_{t-1}~$可能是一个零值向量(没有条件)或是已被初始化(有条件)。如果RNN有前置条件，第一个隐藏状态 ~$h_0~$ 属于一个特定的条件，或者在每一个时间步将特定的条件和随机初始化的零值向量拼接。关于RNN的更多细节我们会在下一个 项目课程里进行讲解。下面来看一个假设情景，就是我们需要处理一些评价数据然后判断这些评价是好评还是差评，在这个情景里RNN的前馈网络是什么样的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RcWE5cw0_cKA",
    "outputId": "a44156b9-b43f-409c-f0ce-4a4bd871d6a0",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already up-to-date: torch in /opt/conda/lib/python3.5/site-packages (1.0.0)\n",
      "Requirement already up-to-date: torchvision in /opt/conda/lib/python3.5/site-packages (0.2.1)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.5/site-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /opt/conda/lib/python3.5/site-packages (from torchvision) (4.2.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/conda/lib/python3.5/site-packages (from torchvision) (1.14.2)\n",
      "Requirement already satisfied, skipping upgrade: olefile in /opt/conda/lib/python3.5/site-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 安装PyTorch\n",
    "!pip install torch torchvision --upgrade -i https://pypi.tuna.tsinghua.edu.cn/simple --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o6eEK1wM_dXG",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qi9hIEV6COLF",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "seq_size = 10 # 输入最长长度 (对不等于最长序列的序列使用遮罩)\n",
    "x_lengths = [8, 5, 4, 10, 5] # 每个输入数据序列的长度\n",
    "embedding_dim = 100\n",
    "rnn_hidden_dim = 256\n",
    "output_dim = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bLEzfxjhB94C",
    "outputId": "f2feefbf-8635-4b23-ef53-b5713cf2cdb2",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "# 初始化数据\n",
    "x_in = torch.randn(batch_size, seq_size, embedding_dim)\n",
    "x_lengths = torch.tensor(x_lengths)\n",
    "print (x_in.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dr6oLqtXB98N",
    "outputId": "9817e88d-6e73-414a-dfa6-2386f40db0d9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 256])\n"
     ]
    }
   ],
   "source": [
    "# 初始化隐藏层\n",
    "hidden_t = torch.zeros((batch_size, rnn_hidden_dim))\n",
    "print (hidden_t.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ryZMOLLgB9-v",
    "outputId": "14ec0a2a-bf37-4e03-b69b-099180f8f149",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNCell(100, 256)\n"
     ]
    }
   ],
   "source": [
    "# 初始化RNN Cell\n",
    "rnn_cell = nn.RNNCell(embedding_dim, rnn_hidden_dim)\n",
    "print (rnn_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "rlbZ7ujxExXb",
    "outputId": "6c83ba2b-94c5-4f76-c8fb-ef0c1ccdeb37",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 256])\n"
     ]
    }
   ],
   "source": [
    "# RNN前馈网络\n",
    "x_in = x_in.permute(1, 0, 2) # RNN需要把batchsize数据放在第一维\n",
    "\n",
    "# 对时间步循环\n",
    "hiddens = []\n",
    "for t in range(seq_size):\n",
    "    hidden_t = rnn_cell(x_in[t], hidden_t)\n",
    "    hiddens.append(hidden_t)\n",
    "hiddens = torch.stack(hiddens)\n",
    "hiddens = hiddens.permute(1, 0, 2) # 把batchsize数据置回第0维\n",
    "print (hiddens.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "3TTL-jmg-MHa",
    "outputId": "3fae323f-c37d-4dac-c8a8-7fea7a45c95c",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out:  torch.Size([5, 10, 256])\n",
      "h_n:  torch.Size([1, 5, 256])\n"
     ]
    }
   ],
   "source": [
    "# 其实可以用更抽象的网络层\n",
    "x_in = torch.randn(batch_size, seq_size, embedding_dim)\n",
    "rnn = nn.RNN(embedding_dim, rnn_hidden_dim, batch_first=True)\n",
    "out, h_n = rnn(x_in) #h_n 是前一个隐藏状态\n",
    "print (\"out: \", out.size())\n",
    "print (\"h_n: \", h_n.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iAsyRNnbHwcT",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def gather_last_relevant_hidden(hiddens, x_lengths):\n",
    "    x_lengths = x_lengths.long().detach().cpu().numpy() - 1\n",
    "    out = []\n",
    "    for batch_index, column_index in enumerate(x_lengths):\n",
    "        out.append(hiddens[batch_index, column_index])\n",
    "    return torch.stack(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PVhp1KLqHqpA",
    "outputId": "d04be3ef-c2d6-48b9-f0f5-a93f619ec594",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 256])\n"
     ]
    }
   ],
   "source": [
    "# 获取最后一个相关的隐藏状态\n",
    "z = gather_last_relevant_hidden(hiddens, x_lengths)\n",
    "print (z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "yGk_iZ5cITZl",
    "outputId": "84749ff2-1e45-4599-a38d-8c83cee116a9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4])\n",
      "tensor([[0.2734, 0.2128, 0.2010, 0.3129],\n",
      "        [0.1772, 0.2352, 0.2647, 0.3228],\n",
      "        [0.2697, 0.2114, 0.2957, 0.2231],\n",
      "        [0.1680, 0.2455, 0.2652, 0.3212],\n",
      "        [0.2690, 0.2353, 0.1680, 0.3277]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 通过全连接层向前传播\n",
    "fc1 = nn.Linear(rnn_hidden_dim, output_dim)\n",
    "y_pred = fc1(z)\n",
    "y_pred = F.softmax(y_pred, dim=1)\n",
    "print (y_pred.size())\n",
    "print (y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kP1awuluoCSr",
    "mdEditEnable": false
   },
   "source": [
    "## 序列化数据\n",
    "\n",
    "RNN 可以帮助处理许多不同的序列化任务。\n",
    "\n",
    "1. **一对一 (One to one)**: 有一个输入，产出一个输出。\n",
    "\t* 举个例子: 输入一个单词，给出类别(它是动词，名字，等等)\n",
    "2. **一对多 (One to many)**: 有一个输入，产出多个输出。\n",
    "\t* 举个例子: 输入一种评价分类(差评还是好评)，输出一个评价文案\n",
    "3. **多对一 (Many to one)**: 多个输入，产出一个输出\n",
    "\t* 举个例子: 输入输出一个评价文案, 输出一种评价分类(差评还是好评)\n",
    "4. **多对多 (Many to many)**: 多个输入序列化处理生成多个输出\n",
    "\t* 举个例子: 输入一句法语，处理后输出英语翻译\n",
    "\t* 再举个例子: 输入一组时间序列数据，在每一时间步输出某个事件的概率(疾病风险)\n",
    "\n",
    "\n",
    "<img src=\"data/seq2seq.jpeg\" alt=\"Drawing\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uMx2s93VLUTt",
    "mdEditEnable": false
   },
   "source": [
    "## 普通RNN的问题\n",
    "\n",
    "到现在为止我们看到的其实都属于 普通RNN(Vanills RNN), 其中存在一些问题。\n",
    "\n",
    "1. 当输入序列包含的时间步越多, 在处理后置的数据同时保存前置已经学习到的信息的难度就会越来越大。模型的目的确实是要保存之前已处理的数据中的有效信息，但是当时间步很多的时候这个操作就会显得异常笨重。\n",
    "\n",
    "2. 进行反向传播时，损失梯度需要传播到最开始的第一个时间步。当梯度比1大(~${1.01}^{1000} = 20959~$)或比1小(~${0.99}^{1000} = 4.31e-5~$)，且序列中时间步很多时，反向传播就会成为灾难。\n",
    "\n",
    "为了解决这些问题，**门控(Gating)** 的机制被引进了RNN. 这一机制可以使网络得以控制时间步之间的信息流，从而优化模型。选择性地向下一个时间步传可以递信息让模型可以轻松处理含有许多时间步的数据。类似的网络变种有长短期记忆网络 Long Short Term Memory ([LSTM](https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM)), 和 门控递归神经网络 Gated Recurrent Units ([GRUs](https://pytorch.org/docs/stable/nn.html#torch.nn.GRU))。 大家可以在[here](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)学习具体信息。\n",
    "\n",
    "\n",
    "<img src=\"data/gates.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tirko0kwp-9J",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 用PyTorch实现GRU\n",
    "gru = nn.GRU(input_size=embedding_dim, hidden_size=rnn_hidden_dim, \n",
    "             batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "UZjUhh4VBWxM",
    "outputId": "9fe275fe-c8d9-42f0-e5d0-0295268ed83d",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "# 初始化人造数据\n",
    "x_in = torch.randn(batch_size, seq_size, embedding_dim)\n",
    "print (x_in.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "xJ_SE7AvBfa4",
    "outputId": "b9411aaa-fab1-4104-aee7-8f9a423332ab",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: torch.Size([5, 10, 256])\n",
      "h_n: torch.Size([1, 5, 256])\n"
     ]
    }
   ],
   "source": [
    "# 向前传播\n",
    "out, h_n = gru(x_in)\n",
    "print (\"out:\", out.size())\n",
    "print (\"h_n:\", h_n.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ij_GA2Rr9BbA",
    "mdEditEnable": false
   },
   "source": [
    "\n",
    "**注意**: 数据特性和性能决定了我们到底是选择 GRU 还是 LSTM。GRU在参数较少时性能客观，LSTM 效率更高，不过性能随实际情况变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xck0n-KpmXkV",
    "mdEditEnable": false
   },
   "source": [
    "## 双向递归神经网络\n",
    "我们会在接下来的项目中看到 RNN 的优化和提升 ([注意](https://www.oreilly.com/ideas/interpretability-via-attentional-and-memory-based-interfaces-using-tensorflow), Quasi RNNs, etc.)。顾名思义，双向递归神经网络(bidirectional RNNs, Bi-RNNs) 会从输入的两个方向来处理数据。分别从两头开始处理数据结构提供给输出层输入序列中每一个点的完整的过去和未来的上下文信息，有助于性能的提升。Bi-RNN的一种常用情景是 翻译，从两头分别开始处理整句对将一种语言译成另一种语言的任务非常有帮助。\n",
    "\n",
    "\n",
    "<img src=\"data/birnn.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gSk_5XrvApCd",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PyTorch实现Bi-GRU\n",
    "bi_gru = nn.GRU(input_size=embedding_dim, hidden_size=rnn_hidden_dim, \n",
    "                batch_first=True, bidirectional=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Fx7-GTptBCtZ",
    "outputId": "f0242cc5-534a-460b-ebe0-4e8c504fab22",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out: torch.Size([5, 10, 512])\n",
      "h_n: torch.Size([2, 5, 256])\n"
     ]
    }
   ],
   "source": [
    "# 向前传播\n",
    "out, h_n = bi_gru(x_in)\n",
    "print (\"out:\", out.size()) # collection of all hidden states from the RNN for each time step\n",
    "print (\"h_n:\", h_n.size()) # last hidden state from the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k5lvJirLBjI6",
    "mdEditEnable": false
   },
   "source": [
    "注意每个样本的输出的大小是512，就是隐藏维度的两倍。因为同时包含了向前传播和反向传播的数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JgYdEZmHlmft",
    "mdEditEnable": false
   },
   "source": [
    "## 使用 RNN 作文本分类\n",
    "让我们把RNN应用在[嵌入层](https://www.kesci.com/home/project/share/50c13cecc0bf7ba1)的项目中，我们当时想通过标题预测文章的分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eIvXqvPQEiDC",
    "mdEditEnable": false
   },
   "source": [
    "### 配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "muTcvMynlmAu",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "import collections\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "00ESjecep-_y",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 设置Numpy和PyTorch随机种子\n",
    "def set_seeds(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "# 创建字典\n",
    "def create_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "m67THDvxEl1e",
    "outputId": "7118c77b-cbf9-4d7e-ff7a-b9dc1fb63cbb",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "# 参数\n",
    "args = Namespace(\n",
    "    seed=1234,\n",
    "    cuda=True,\n",
    "    shuffle=True,\n",
    "    data_file=\"data/news.csv\",\n",
    "    split_data_file=\"data/split_news.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"news\",\n",
    "    train_size=0.7,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    pretrained_embeddings=None,\n",
    "    cutoff=25, # token must appear at least <cutoff> times to be in SequenceVocabulary\n",
    "    num_epochs=5,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=64,\n",
    "    embedding_dim=100,\n",
    "    rnn_hidden_dim=128,\n",
    "    hidden_dim=100,\n",
    "    num_layers=1,\n",
    "    bidirectional=False,\n",
    "    dropout_p=0.1,\n",
    ")\n",
    "\n",
    "# 设置种子\n",
    "set_seeds(seed=args.seed, cuda=args.cuda)\n",
    "\n",
    "# 创建保存目录\n",
    "create_dirs(args.save_dir)\n",
    "\n",
    "# 拓展路径\n",
    "args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "# 检查GPU可用性\n",
    "if torch.cuda.is_available():\n",
    "    args.cuda = True\n",
    "else:\n",
    "    args.cude = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s7T-_kGvExVW",
    "mdEditEnable": false
   },
   "source": [
    "### 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XVyK25xOEwjN",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M_gclwECEwll",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "url = \"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/data/news.csv\"\n",
    "response = urllib.request.urlopen(url)\n",
    "html = response.read()\n",
    "with open(args.data_file, 'wb') as fp:\n",
    "    fp.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "V244zOIPEwoP",
    "outputId": "ab8b5cab-4e25-436e-9cb3-0db6f524eb9a",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                              title\n",
       "0  Business  Wall St. Bears Claw Back Into the Black (Reuters)\n",
       "1  Business  Carlyle Looks Toward Commercial Aerospace (Reu...\n",
       "2  Business    Oil and Economy Cloud Stocks' Outlook (Reuters)\n",
       "3  Business  Iraq Halts Oil Exports from Main Southern Pipe...\n",
       "4  Business  Oil prices soar to all-time record, posing new..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原始数据\n",
    "df = pd.read_csv(args.data_file, header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "ICl2MNK4EwrL",
    "outputId": "d2073597-71e5-40b1-a845-90bf4913ea7a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business: 30000\n",
      "Sports: 30000\n",
      "World: 30000\n",
      "Sci/Tech: 30000\n"
     ]
    }
   ],
   "source": [
    "# 按类别拆分数据集\n",
    "by_category = collections.defaultdict(list)\n",
    "for _, row in df.iterrows():\n",
    "    by_category[row.category].append(row.to_dict())\n",
    "for category in by_category:\n",
    "    print (\"{0}: {1}\".format(category, len(by_category[category])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "76PwKQHLEww5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 新建切分数据集\n",
    "final_list = []\n",
    "for _, item_list in sorted(by_category.items()):\n",
    "    if args.shuffle:\n",
    "        np.random.shuffle(item_list)\n",
    "    n = len(item_list)\n",
    "    n_train = int(args.train_size*n)\n",
    "    n_val = int(args.val_size*n)\n",
    "    n_test = int(args.test_size*n)\n",
    "\n",
    "  # 给数据指定切分属性\n",
    "    for item in item_list[:n_train]:\n",
    "        item['split'] = 'train'\n",
    "    for item in item_list[n_train:n_train+n_val]:\n",
    "        item['split'] = 'val'\n",
    "    for item in item_list[n_train+n_val:]:\n",
    "        item['split'] = 'test'  \n",
    "\n",
    "    # 添加到列表\n",
    "    final_list.extend(item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "CQeS0KHOEwzm",
    "outputId": "93c9aadb-25c4-4029-f002-8a43f3956045",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    84000\n",
       "test     18000\n",
       "val      18000\n",
       "Name: split, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 切分后的数据集dataframe\n",
    "split_df = pd.DataFrame(final_list)\n",
    "split_df[\"split\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pPJDyVusEw3-",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 预处理\n",
    "def preprocess_text(text):\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "    \n",
    "split_df.title = split_df.title.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "IAetKendEw6b",
    "outputId": "d5946f7e-840e-4a0b-e492-d3da68cefd44",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>split</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>general electric posts higher rd quarter profit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>lilly to eliminate up to us jobs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>s amp p lowers america west outlook to negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>does rand walk the talk on labor policy ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>housekeeper advocates for changes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category  split                                            title\n",
       "0  Business  train  general electric posts higher rd quarter profit\n",
       "1  Business  train                 lilly to eliminate up to us jobs\n",
       "2  Business  train  s amp p lowers america west outlook to negative\n",
       "3  Business  train        does rand walk the talk on labor policy ?\n",
       "4  Business  train                housekeeper advocates for changes"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 存为CSV文件\n",
    "split_df.to_csv(args.split_data_file, index=False)\n",
    "split_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NHzGXAI3E7lF",
    "mdEditEnable": false
   },
   "source": [
    "### 词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZIRUjX0MEw88",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, token_to_idx=None):\n",
    "\n",
    "        # 词条转换为索引\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self.token_to_idx = token_to_idx\n",
    "\n",
    "        # 索引转换为词条\n",
    "        self.idx_to_token = {idx: token \\\n",
    "                             for token, idx in self.token_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self.token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        return [self.add_token[token] for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        return self.token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        if index not in self.idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self.idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "1LtYf3lpExBb",
    "outputId": "617297a7-3fdb-4789-bbca-dea82d06c8ce",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary(size=4)>\n",
      "4\n",
      "0\n",
      "Business\n"
     ]
    }
   ],
   "source": [
    "# 词汇表实例\n",
    "category_vocab = Vocabulary()\n",
    "for index, row in df.iterrows():\n",
    "    category_vocab.add_token(row.category)\n",
    "print (category_vocab) # __str__\n",
    "print (len(category_vocab)) # __len__\n",
    "index = category_vocab.lookup_token(\"Business\")\n",
    "print (index)\n",
    "print (category_vocab.lookup_index(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QtntaISyE_1c",
    "mdEditEnable": false
   },
   "source": [
    "### 序列词汇表\n",
    "\n",
    "接下来我们将为文章标题创建词汇表类，它由一系列词条构成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ovI8QRefEw_p",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4W3ZouuTEw1_",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self.mask_token = mask_token\n",
    "        self.unk_token = unk_token\n",
    "        self.begin_seq_token = begin_seq_token\n",
    "        self.end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self.mask_token)\n",
    "        self.unk_index = self.add_token(self.unk_token)\n",
    "        self.begin_seq_index = self.add_token(self.begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self.end_seq_token)\n",
    "        \n",
    "        # 索引转换为词条\n",
    "        self.idx_to_token = {idx: token \\\n",
    "                             for token, idx in self.token_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self.unk_token,\n",
    "                         'mask_token': self.mask_token,\n",
    "                         'begin_seq_token': self.begin_seq_token,\n",
    "                         'end_seq_token': self.end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        return self.token_to_idx.get(token, self.unk_index)\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        if index not in self.idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the SequenceVocabulary\" % index)\n",
    "        return self.idx_to_token[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"<SequenceVocabulary(size=%d)>\" % len(self.token_to_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "g5UHjpi3El37",
    "outputId": "cb20aa34-2bd5-4178-b219-d845fdc4968e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SequenceVocabulary(size=4400)>\n",
      "4400\n",
      "641\n",
      "general\n"
     ]
    }
   ],
   "source": [
    "# 得到词数\n",
    "word_counts = Counter()\n",
    "for title in split_df.title:\n",
    "    for token in title.split(\" \"):\n",
    "        if token not in string.punctuation:\n",
    "            word_counts[token] += 1\n",
    "\n",
    "# 创建SequenceVocabulary实例\n",
    "title_vocab = SequenceVocabulary()\n",
    "for word, word_count in word_counts.items():\n",
    "    if word_count >= args.cutoff:\n",
    "        title_vocab.add_token(word)\n",
    "print (title_vocab) # __str__\n",
    "print (len(title_vocab)) # __len__\n",
    "index = title_vocab.lookup_token(\"general\")\n",
    "print (index)\n",
    "print (title_vocab.lookup_index(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VQIfxcUuKwzz",
    "mdEditEnable": false
   },
   "source": [
    "### 向量化\n",
    "\n",
    "在向量化中这次我们会引入新的操作: 计算输入序列的长度。我们会在为每个输入序列提取最新的相关隐藏状态时用到它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tsNtEnhBEl6s",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NewsVectorizer(object):\n",
    "    def __init__(self, title_vocab, category_vocab):\n",
    "        self.title_vocab = title_vocab\n",
    "        self.category_vocab = category_vocab\n",
    "\n",
    "    def vectorize(self, title):\n",
    "        indices = [self.title_vocab.lookup_token(token) for token in title.split(\" \")]\n",
    "        indices = [self.title_vocab.begin_seq_index] + indices + \\\n",
    "            [self.title_vocab.end_seq_index]\n",
    "        \n",
    "        # 创建向量\n",
    "        title_length = len(indices)\n",
    "        vector = np.zeros(title_length, dtype=np.int64)\n",
    "        vector[:len(indices)] = indices\n",
    "\n",
    "        return vector, title_length\n",
    "    \n",
    "    def unvectorize(self, vector):\n",
    "        tokens = [self.title_vocab.lookup_index(index) for index in vector]\n",
    "        title = \" \".join(token for token in tokens)\n",
    "        return title\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df, cutoff):\n",
    "        \n",
    "        # 创建分类的词汇表实例\n",
    "        category_vocab = Vocabulary()        \n",
    "        for category in sorted(set(df.category)):\n",
    "            category_vocab.add_token(category)\n",
    "\n",
    "        # 获取词数\n",
    "        word_counts = Counter()\n",
    "        for title in df.title:\n",
    "            for token in title.split(\" \"):\n",
    "                word_counts[token] += 1\n",
    "        \n",
    "        # 创建标题的词汇表实例\n",
    "        title_vocab = SequenceVocabulary()\n",
    "        for word, word_count in word_counts.items():\n",
    "            if word_count >= cutoff:\n",
    "                title_vocab.add_token(word)\n",
    "        \n",
    "        return cls(title_vocab, category_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        title_vocab = SequenceVocabulary.from_serializable(contents['title_vocab'])\n",
    "        category_vocab = Vocabulary.from_serializable(contents['category_vocab'])\n",
    "        return cls(title_vocab=title_vocab, category_vocab=category_vocab)\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {'title_vocab': self.title_vocab.to_serializable(),\n",
    "                'category_vocab': self.category_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "JtRRXU53El9Y",
    "outputId": "ba63f1e4-d50e-458c-cb38-da4cc69e5dfa",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SequenceVocabulary(size=4404)>\n",
      "<Vocabulary(size=4)>\n",
      "(10,)\n",
      "title_length: 10\n",
      "[   2    1 2032  115 1075    1 2590 4387 3927    3]\n",
      "<BEGIN> <UNK> federer wins the <UNK> tennis tournament . <END>\n"
     ]
    }
   ],
   "source": [
    "# 向量化实例\n",
    "vectorizer = NewsVectorizer.from_dataframe(split_df, cutoff=args.cutoff)\n",
    "print (vectorizer.title_vocab)\n",
    "print (vectorizer.category_vocab)\n",
    "vectorized_title, title_length = vectorizer.vectorize(preprocess_text(\n",
    "    \"Roger Federer wins the Wimbledon tennis tournament.\"))\n",
    "print (np.shape(vectorized_title))\n",
    "print (\"title_length:\", title_length)\n",
    "print (vectorized_title)\n",
    "print (vectorizer.unvectorize(vectorized_title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uk_QvpVfFM0S",
    "mdEditEnable": false
   },
   "source": [
    "### 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oU7oDdelFMR9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pB7FHmiSFMXA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, df, vectorizer):\n",
    "        self.df = df\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "        # 切分数据\n",
    "        self.train_df = self.df[self.df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "        self.val_df = self.df[self.df.split=='val']\n",
    "        self.val_size = len(self.val_df)\n",
    "        self.test_df = self.df[self.df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "        self.lookup_dict = {'train': (self.train_df, self.train_size), \n",
    "                            'val': (self.val_df, self.val_size),\n",
    "                            'test': (self.test_df, self.test_size)}\n",
    "        self.set_split('train')\n",
    "\n",
    "        # 类权重(用于样本失衡)\n",
    "        class_counts = df.category.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self.vectorizer.category_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, split_data_file, cutoff):\n",
    "        df = pd.read_csv(split_data_file, header=0)\n",
    "        train_df = df[df.split=='train']\n",
    "        return cls(df, NewsVectorizer.from_dataframe(train_df, cutoff))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, split_data_file, vectorizer_filepath):\n",
    "        df = pd.read_csv(split_data_file, header=0)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(df, vectorizer)\n",
    "\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return NewsVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self.vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self.target_split = split\n",
    "        self.target_df, self.target_size = self.lookup_dict[split]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Dataset(split={0}, size={1})\".format(\n",
    "            self.target_split, self.target_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.target_df.iloc[index]\n",
    "        title_vector, title_length = self.vectorizer.vectorize(row.title)\n",
    "        category_index = self.vectorizer.category_vocab.lookup_token(row.category)\n",
    "        return {'title': title_vector, 'title_length': title_length, \n",
    "                'category': category_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "\n",
    "    def generate_batches(self, batch_size, collate_fn, shuffle=True, \n",
    "                         drop_last=False, device=\"cpu\"):\n",
    "        dataloader = DataLoader(dataset=self, batch_size=batch_size,\n",
    "                                collate_fn=collate_fn, shuffle=shuffle, \n",
    "                                drop_last=drop_last)\n",
    "        for data_dict in dataloader:\n",
    "            out_data_dict = {}\n",
    "            for name, tensor in data_dict.items():\n",
    "                out_data_dict[name] = data_dict[name].to(device)\n",
    "            yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "_Dpb6ZHJFMeb",
    "outputId": "2283eeae-4015-44a5-a2a5-1fe2278769ba",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Dataset(split=train, size=84000)\n",
      "[   2 1182 1955  816 3138 2806    3] 7 0\n",
      "<BEGIN> software firm to cut jobs <END>\n",
      "tensor([3.3333e-05, 3.3333e-05, 3.3333e-05, 3.3333e-05])\n"
     ]
    }
   ],
   "source": [
    "# 数据集实例\n",
    "dataset = NewsDataset.load_dataset_and_make_vectorizer(args.split_data_file,\n",
    "                                                       args.cutoff)\n",
    "print (dataset) # __str__\n",
    "input_ = dataset[5] # __getitem__\n",
    "print (input_['title'], input_['title_length'], input_['category'])\n",
    "print (dataset.vectorizer.unvectorize(input_['title']))\n",
    "print (dataset.class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xJV5WlDiFVVz",
    "mdEditEnable": false
   },
   "source": [
    "### 模型\n",
    "input → embedding → RNN → FC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rZCzdZZ9FMhm",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wbWO4lZcIdqZ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def gather_last_relevant_hidden(hiddens, x_lengths):\n",
    "    x_lengths = x_lengths.long().detach().cpu().numpy() - 1\n",
    "    out = []\n",
    "    for batch_index, column_index in enumerate(x_lengths):\n",
    "        out.append(hiddens[batch_index, column_index])\n",
    "    return torch.stack(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9TT66Y-UFMcZ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NewsModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_embeddings, rnn_hidden_dim, \n",
    "                 hidden_dim, output_dim, num_layers, bidirectional, dropout_p, \n",
    "                 pretrained_embeddings=None, freeze_embeddings=False, \n",
    "                 padding_idx=0):\n",
    "        super(NewsModel, self).__init__()\n",
    "        \n",
    "        if pretrained_embeddings is None:\n",
    "            self.embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
    "                                          num_embeddings=num_embeddings,\n",
    "                                          padding_idx=padding_idx)\n",
    "        else:\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
    "                                           num_embeddings=num_embeddings,\n",
    "                                           padding_idx=padding_idx,\n",
    "                                           _weight=pretrained_embeddings)\n",
    "        \n",
    "        # 卷积层权重\n",
    "        self.gru = nn.GRU(input_size=embedding_dim, hidden_size=rnn_hidden_dim, \n",
    "                          num_layers=num_layers, batch_first=True, \n",
    "                          bidirectional=bidirectional)\n",
    "     \n",
    "        # 全连接层权重\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        if freeze_embeddings:\n",
    "            self.embeddings.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, x_in, x_lengths, apply_softmax=False):\n",
    "        \n",
    "        # 嵌入\n",
    "        x_in = self.embeddings(x_in)\n",
    "            \n",
    "        # 传入RNN\n",
    "        out, h_n = self.gru(x_in)\n",
    "        \n",
    "        # 获取上一个相关隐藏状态\n",
    "        out = gather_last_relevant_hidden(out, x_lengths)\n",
    "\n",
    "        # 全连接层\n",
    "        z = self.dropout(out)\n",
    "        z = self.fc1(z)\n",
    "        z = self.dropout(z)\n",
    "        y_pred = self.fc2(z)\n",
    "\n",
    "        if apply_softmax:\n",
    "            y_pred = F.softmax(y_pred, dim=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jHPYCPd7Fl3M",
    "mdEditEnable": false
   },
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D3seBMA7FlcC",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HnRKWLekFlnM",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, dataset, model, model_state_file, save_dir, device, shuffle, \n",
    "               num_epochs, batch_size, learning_rate, early_stopping_criteria):\n",
    "        self.dataset = dataset\n",
    "        self.class_weights = dataset.class_weights.to(device)\n",
    "        self.model = model.to(device)\n",
    "        self.save_dir = save_dir\n",
    "        self.device = device\n",
    "        self.shuffle = shuffle\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_func = nn.CrossEntropyLoss(self.class_weights)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer=self.optimizer, mode='min', factor=0.5, patience=1)\n",
    "        self.train_state = {\n",
    "            'stop_early': False, \n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'early_stopping_criteria': early_stopping_criteria,\n",
    "            'learning_rate': learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': model_state_file}\n",
    "    \n",
    "    def update_train_state(self):\n",
    "\n",
    "        # 打印checkpoint信息\n",
    "        print (\"[EPOCH]: {0:02d} | [LR]: {1} | [TRAIN LOSS]: {2:.2f} | [TRAIN ACC]: {3:.1f}% | [VAL LOSS]: {4:.2f} | [VAL ACC]: {5:.1f}%\".format(\n",
    "          self.train_state['epoch_index'], self.train_state['learning_rate'], \n",
    "            self.train_state['train_loss'][-1], self.train_state['train_acc'][-1], \n",
    "            self.train_state['val_loss'][-1], self.train_state['val_acc'][-1]))\n",
    "\n",
    "        # 至少保存一次模型\n",
    "        if self.train_state['epoch_index'] == 0:\n",
    "            torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
    "            self.train_state['stop_early'] = False\n",
    "\n",
    "        # 如果模型性能表现有提升，再次保存\n",
    "        elif self.train_state['epoch_index'] >= 1:\n",
    "            loss_tm1, loss_t = self.train_state['val_loss'][-2:]\n",
    "\n",
    "            # 如果损失增大\n",
    "            if loss_t >= self.train_state['early_stopping_best_val']:\n",
    "                # 更新步数\n",
    "                self.train_state['early_stopping_step'] += 1\n",
    "\n",
    "            # 损失变小\n",
    "            else:\n",
    "                # 保存最优的模型\n",
    "                if loss_t < self.train_state['early_stopping_best_val']:\n",
    "                    torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
    "\n",
    "                # 重置early stopping 的步数\n",
    "                self.train_state['early_stopping_step'] = 0\n",
    "\n",
    "            # 是否需要early stop?\n",
    "            self.train_state['stop_early'] = self.train_state['early_stopping_step'] \\\n",
    "              >= self.train_state['early_stopping_criteria']\n",
    "        return self.train_state\n",
    "  \n",
    "    def compute_accuracy(self, y_pred, y_target):\n",
    "        _, y_pred_indices = y_pred.max(dim=1)\n",
    "        n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "        return n_correct / len(y_pred_indices) * 100\n",
    "    \n",
    "    def pad_seq(self, seq, length):\n",
    "        vector = np.zeros(length, dtype=np.int64)\n",
    "        vector[:len(seq)] = seq\n",
    "        vector[len(seq):] = self.dataset.vectorizer.title_vocab.mask_index\n",
    "        return vector\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        \n",
    "        # 深度拷贝\n",
    "        batch_copy = copy.deepcopy(batch)\n",
    "        processed_batch = {\"title\": [], \"title_length\": [], \"category\": []}\n",
    "        \n",
    "        # 得到最长序列长度\n",
    "        get_length = lambda sample: len(sample[\"title\"])\n",
    "        max_seq_length = max(map(get_length, batch))\n",
    "        \n",
    "        # 填充\n",
    "        for i, sample in enumerate(batch_copy):\n",
    "            padded_seq = self.pad_seq(sample[\"title\"], max_seq_length)\n",
    "            processed_batch[\"title\"].append(padded_seq)\n",
    "            processed_batch[\"title_length\"].append(sample[\"title_length\"])\n",
    "            processed_batch[\"category\"].append(sample[\"category\"])\n",
    "            \n",
    "        # 转换为合适的tensor\n",
    "        processed_batch[\"title\"] = torch.LongTensor(\n",
    "            processed_batch[\"title\"])\n",
    "        processed_batch[\"title_length\"] = torch.LongTensor(\n",
    "            processed_batch[\"title_length\"])\n",
    "        processed_batch[\"category\"] = torch.LongTensor(\n",
    "            processed_batch[\"category\"])\n",
    "        \n",
    "        return processed_batch   \n",
    "  \n",
    "    def run_train_loop(self):\n",
    "        for epoch_index in range(self.num_epochs):\n",
    "            self.train_state['epoch_index'] = epoch_index\n",
    "      \n",
    "            # 遍历训练集\n",
    "            # 初始化批生成器, 将损失和准确率归零，设置为训练模式\n",
    "            self.dataset.set_split('train')\n",
    "            batch_generator = self.dataset.generate_batches(\n",
    "                batch_size=self.batch_size, collate_fn=self.collate_fn, \n",
    "                shuffle=self.shuffle, device=self.device)\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            self.model.train()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "                # 梯度归零\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # 计算输出\n",
    "                y_pred = self.model(batch_dict['title'], batch_dict['title_length'])\n",
    "\n",
    "                # 计算损失\n",
    "                loss = self.loss_func(y_pred, batch_dict['category'])\n",
    "                loss_t = loss.item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # 反向传播\n",
    "                loss.backward()\n",
    "\n",
    "                # 更新梯度\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # 计算准确率\n",
    "                acc_t = self.compute_accuracy(y_pred, batch_dict['category'])\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            self.train_state['train_loss'].append(running_loss)\n",
    "            self.train_state['train_acc'].append(running_acc)\n",
    "\n",
    "            # 遍历验证集\n",
    "            # 初始化批生成器, 将损失和准确率归零，设置为运行模式\n",
    "            self.dataset.set_split('val')\n",
    "            batch_generator = self.dataset.generate_batches(\n",
    "                batch_size=self.batch_size, collate_fn=self.collate_fn, \n",
    "                shuffle=self.shuffle, device=self.device)\n",
    "            running_loss = 0.\n",
    "            running_acc = 0.\n",
    "            self.model.eval()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "                # 计算输出\n",
    "                y_pred =  self.model(batch_dict['title'], batch_dict['title_length'])\n",
    "\n",
    "                # 计算损失\n",
    "                loss = self.loss_func(y_pred, batch_dict['category'])\n",
    "                loss_t = loss.to(\"cpu\").item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # 计算准确率\n",
    "                acc_t = self.compute_accuracy(y_pred, batch_dict['category'])\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            self.train_state['val_loss'].append(running_loss)\n",
    "            self.train_state['val_acc'].append(running_acc)\n",
    "\n",
    "            self.train_state = self.update_train_state()\n",
    "            self.scheduler.step(self.train_state['val_loss'][-1])\n",
    "            if self.train_state['stop_early']:\n",
    "                break\n",
    "          \n",
    "    def run_test_loop(self):\n",
    "        # 初始化批生成器, 将损失和准确率归零，设置为运行模式\n",
    "        self.dataset.set_split('test')\n",
    "        batch_generator = self.dataset.generate_batches(\n",
    "            batch_size=self.batch_size, collate_fn=self.collate_fn, \n",
    "            shuffle=self.shuffle, device=self.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        self.model.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # 计算输出\n",
    "            y_pred =  self.model(batch_dict['title'], batch_dict['title_length'])\n",
    "\n",
    "            # 计算损失\n",
    "            loss = self.loss_func(y_pred, batch_dict['category'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # 计算准确率\n",
    "            acc_t = self.compute_accuracy(y_pred, batch_dict['category'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "        self.train_state['test_loss'] = running_loss\n",
    "        self.train_state['test_acc'] = running_acc\n",
    "    \n",
    "    def plot_performance(self):\n",
    "        # 设置图大小\n",
    "        plt.figure(figsize=(15,5))\n",
    "\n",
    "        # 画出损失\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Loss\")\n",
    "        plt.plot(trainer.train_state[\"train_loss\"], label=\"train\")\n",
    "        plt.plot(trainer.train_state[\"val_loss\"], label=\"val\")\n",
    "        plt.legend(loc='upper right')\n",
    "\n",
    "        # 画出准确率\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.plot(trainer.train_state[\"train_acc\"], label=\"train\")\n",
    "        plt.plot(trainer.train_state[\"val_acc\"], label=\"val\")\n",
    "        plt.legend(loc='lower right')\n",
    "\n",
    "        # 存图\n",
    "        plt.savefig(os.path.join(self.save_dir, \"performance.png\"))\n",
    "\n",
    "        # 展示图\n",
    "        plt.show()\n",
    "    \n",
    "    def save_train_state(self):\n",
    "        with open(os.path.join(self.save_dir, \"train_state.json\"), \"w\") as fp:\n",
    "            json.dump(self.train_state, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "ICkiOaGtFlk-",
    "outputId": "57f7f143-7899-407a-acbd-17f767eb56c3",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.named_modules of NewsModel(\n",
      "  (embeddings): Embedding(3406, 100, padding_idx=0)\n",
      "  (gru): GRU(100, 128, batch_first=True)\n",
      "  (dropout): Dropout(p=0.1)\n",
      "  (fc1): Linear(in_features=128, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# 初始化\n",
    "dataset = NewsDataset.load_dataset_and_make_vectorizer(args.split_data_file,\n",
    "                                                       args.cutoff)\n",
    "dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.vectorizer\n",
    "model = NewsModel(embedding_dim=args.embedding_dim, \n",
    "                  num_embeddings=len(vectorizer.title_vocab), \n",
    "                  rnn_hidden_dim=args.rnn_hidden_dim,\n",
    "                  hidden_dim=args.hidden_dim,\n",
    "                  output_dim=len(vectorizer.category_vocab),\n",
    "                  num_layers=args.num_layers,\n",
    "                  bidirectional=args.bidirectional,\n",
    "                  dropout_p=args.dropout_p, \n",
    "                  pretrained_embeddings=None, \n",
    "                  padding_idx=vectorizer.title_vocab.mask_index)\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "tuaRZ4DiFlh1",
    "outputId": "fba7ac04-7e1d-4372-b358-7340a013960d",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH]: 00 | [LR]: 0.001 | [TRAIN LOSS]: 0.75 | [TRAIN ACC]: 70.4% | [VAL LOSS]: 0.54 | [VAL ACC]: 80.3%\n",
      "[EPOCH]: 01 | [LR]: 0.001 | [TRAIN LOSS]: 0.48 | [TRAIN ACC]: 82.8% | [VAL LOSS]: 0.49 | [VAL ACC]: 82.3%\n",
      "[EPOCH]: 02 | [LR]: 0.001 | [TRAIN LOSS]: 0.41 | [TRAIN ACC]: 85.1% | [VAL LOSS]: 0.46 | [VAL ACC]: 83.3%\n",
      "[EPOCH]: 03 | [LR]: 0.001 | [TRAIN LOSS]: 0.37 | [TRAIN ACC]: 86.6% | [VAL LOSS]: 0.47 | [VAL ACC]: 83.3%\n",
      "[EPOCH]: 04 | [LR]: 0.001 | [TRAIN LOSS]: 0.32 | [TRAIN ACC]: 88.2% | [VAL LOSS]: 0.47 | [VAL ACC]: 83.3%\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "trainer = Trainer(dataset=dataset, model=model, \n",
    "                  model_state_file=args.model_state_file, \n",
    "                  save_dir=args.save_dir, device=args.device,\n",
    "                  shuffle=args.shuffle, num_epochs=args.num_epochs, \n",
    "                  batch_size=args.batch_size, learning_rate=args.learning_rate, \n",
    "                  early_stopping_criteria=args.early_stopping_criteria)\n",
    "trainer.run_train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "mzRJIz88Flfe",
    "outputId": "a7ac8786-01ea-4421-e70c-d79c22c7ed4a",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/mzRJIz88Flfe/pk82kvgceo.png\">"
      ],
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 画出训练过程\n",
    "trainer.plot_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4EmFhiX-FMaV",
    "outputId": "c689c3e6-972b-4499-81b6-8812a25076d1",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.47\n",
      "Test Accuracy: 83.3%\n"
     ]
    }
   ],
   "source": [
    "# 测试集上的性能\n",
    "trainer.run_test_loop()\n",
    "print(\"Test loss: {0:.2f}\".format(trainer.train_state['test_loss']))\n",
    "print(\"Test Accuracy: {0:.1f}%\".format(trainer.train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zVU1zakYFMVF",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 保存结果\n",
    "trainer.save_train_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qLoKfjSpFw7t",
    "mdEditEnable": false
   },
   "source": [
    "### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ANrPcS7Hp_CP",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Inference(object):\n",
    "    def __init__(self, model, vectorizer):\n",
    "        self.model = model\n",
    "        self.vectorizer = vectorizer\n",
    "  \n",
    "    def predict_category(self, title):\n",
    "        # 向量化\n",
    "        vectorized_title, title_length = self.vectorizer.vectorize(title)\n",
    "        vectorized_title = torch.tensor(vectorized_title).unsqueeze(0)\n",
    "        title_length = torch.tensor([title_length]).long()\n",
    "        \n",
    "        # 向前传播\n",
    "        self.model.eval()\n",
    "        y_pred = self.model(x_in=vectorized_title, x_lengths=title_length, \n",
    "                            apply_softmax=True)\n",
    "\n",
    "        # 可能性最高的分类\n",
    "        y_prob, indices = y_pred.max(dim=1)\n",
    "        index = indices.item()\n",
    "\n",
    "        # 预测出的分类\n",
    "        category = vectorizer.category_vocab.lookup_index(index)\n",
    "        probability = y_prob.item()\n",
    "        return {'category': category, 'probability': probability}\n",
    "    \n",
    "    def predict_top_k(self, title, k):\n",
    "        # 向量化\n",
    "        vectorized_title, title_length = self.vectorizer.vectorize(title)\n",
    "        vectorized_title = torch.tensor(vectorized_title).unsqueeze(0)\n",
    "        title_length = torch.tensor([title_length]).long()\n",
    "        \n",
    "        # 向前传播\n",
    "        self.model.eval()\n",
    "        y_pred = self.model(x_in=vectorized_title, x_lengths=title_length, \n",
    "                            apply_softmax=True)\n",
    "        \n",
    "        # 最有可能的K种分类\n",
    "        y_prob, indices = torch.topk(y_pred, k=k)\n",
    "        probabilities = y_prob.detach().numpy()[0]\n",
    "        indices = indices.detach().numpy()[0]\n",
    "\n",
    "        # 结果\n",
    "        results = []\n",
    "        for probability, index in zip(probabilities, indices):\n",
    "            category = self.vectorizer.category_vocab.lookup_index(index)\n",
    "            results.append({'category': category, 'probability': probability})\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "W6wr68o2p_Eh",
    "outputId": "3e94c736-3ad3-4c70-b24c-591edbe069ad",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.named_modules of NewsModel(\n",
      "  (embeddings): Embedding(3406, 100, padding_idx=0)\n",
      "  (gru): GRU(100, 128, batch_first=True)\n",
      "  (dropout): Dropout(p=0.1)\n",
      "  (fc1): Linear(in_features=128, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "dataset = NewsDataset.load_dataset_and_load_vectorizer(\n",
    "    args.split_data_file, args.vectorizer_file)\n",
    "vectorizer = dataset.vectorizer\n",
    "model = NewsModel(embedding_dim=args.embedding_dim, \n",
    "                  num_embeddings=len(vectorizer.title_vocab), \n",
    "                  rnn_hidden_dim=args.rnn_hidden_dim,\n",
    "                  hidden_dim=args.hidden_dim,\n",
    "                  output_dim=len(vectorizer.category_vocab),\n",
    "                  num_layers=args.num_layers,\n",
    "                  bidirectional=args.bidirectional,\n",
    "                  dropout_p=args.dropout_p, \n",
    "                  pretrained_embeddings=None, \n",
    "                  padding_idx=vectorizer.title_vocab.mask_index)\n",
    "model.load_state_dict(torch.load(args.model_state_file))\n",
    "model = model.to(\"cpu\")\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "JPKgHxsfN954",
    "outputId": "c9f21a76-8307-4737-c785-01f1004891b6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stream",
     "output_type": "stream",
     "text": [
      "Enter a title to classify: : shanghai fashion"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shanghai fashion → Sports (p=0.83)\n"
     ]
    }
   ],
   "source": [
    "# 预测\n",
    "inference = Inference(model=model, vectorizer=vectorizer)\n",
    "title = input(\"Enter a title to classify: \")\n",
    "prediction = inference.predict_category(preprocess_text(title))\n",
    "print(\"{} → {} (p={:0.2f})\".format(title, prediction['category'], \n",
    "                                   prediction['probability']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "JRdz4wzuQR4N",
    "outputId": "9a349bf0-16ba-402d-9133-11ce27e1ec59",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shanghai fashion\n",
      "Sports (p=0.83)\n",
      "Business (p=0.10)\n",
      "World (p=0.04)\n",
      "Sci/Tech (p=0.03)\n"
     ]
    }
   ],
   "source": [
    "# 可能性最高的K个结果\n",
    "top_k = inference.predict_top_k(preprocess_text(title), k=len(vectorizer.category_vocab))\n",
    "print (\"{}\".format(title))\n",
    "for result in top_k:\n",
    "    print (\"{} (p={:0.2f})\".format(result['category'], \n",
    "                                   result['probability']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oAhAHcgZBMFe",
    "mdEditEnable": false
   },
   "source": [
    "## 层标准化\n",
    "\n",
    "在 [卷积神经网络](https://www.kesci.com/home/project/share/5377343963c1a400) 中我们用了批标准化来解决 internal covariate shift 问题。RNN的激活也会遇到类似的问题，这里我们会用到 [层标准化(layer normalization)](https://arxiv.org/abs/1607.06450) 来维持0均值单位方差。\n",
    "\n",
    "层标准化当然和批标准化不同。这里我们会分别计算每个样本(而不是每一隐藏维)的均值和方差，然后再非线性化前进行操作。PyTorch 的 [LayerNorm 类](https://pytorch.org/docs/stable/nn.html#torch.nn.LayerNorm) 已经帮我们完成了大部分工作。\n",
    "\n",
    "~$LN = \\frac{a - \\mu_{L}}{\\sqrt{\\sigma^2_{L} + \\epsilon}}  * \\gamma + \\beta~$\n",
    "\n",
    "*其中* :\n",
    "* ~$a~$ = 激活 | ~$\\in \\mathbb{R}^{NXH}~$ (~$N~$ 样本数量, ~$H~$ 隐藏维度)\n",
    "* ~$\\mu_{L}~$ = 输入均值| ~$\\in \\mathbb{R}^{NX1}~$\n",
    "* ~$\\sigma^2_{L}~$ = 输入方差 | ~$\\in \\mathbb{R}^{NX1}~$\n",
    "* ~$epsilon~$ = 噪声\n",
    "* ~$\\gamma~$ = 规模参数 (通过学习得到)\n",
    "* ~$\\beta~$ = 偏移参数 (通过学习得到)\n",
    "\n",
    "\n",
    "<img src=\"data/batchnorm.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "进行层标准化操作最好的为止是在激活时，并且在做非线性操作之前。\n",
    "但是 PyTorch的[LayerNorm类](https://pytorch.org/docs/stable/nn.html#torch.nn.LayerNorm) 还并没有被默认加入 RNN 相关的代码种。所以你需要用下面的方法自己实现。\n",
    "\n",
    "```python\n",
    "# Layernorm\n",
    "for t in range(seq_size):\n",
    "    # Normalize over hidden dim\n",
    "    layernorm = nn.LayerNorm(args.hidden_dim)\n",
    "    # Activating the module\n",
    "    a = layernorm(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
