{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OLIxEDq6VhvZ",
    "mdEditEnable": false
   },
   "source": [
    "## 卷积神经网络 Convolutional Neural Networks\n",
    "\n",
    "<img src=\"data/logo.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "在这个课程项目中，我们接触学习卷积神经网络(CNNs)并将它运用到自然语言处理(NLP)的任务中。CNN一般被使用于图像相关的机器学习任务中, [这里]((https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) 有一些相关教程。我们还是会先做一些文本处理任务，CNN的表现也足够惊艳。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qWro5T5qTJJL",
    "mdEditEnable": false
   },
   "source": [
    "## 概览\n",
    "\n",
    "这篇 [论文](https://arxiv.org/abs/1510.03820) 中的这张图显示了1维卷积是怎么应用于句子中的词语的。 \n",
    "\n",
    "<img src=\"data/cnn_text.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "* **目的:** 从输入数据中检测空间数据\n",
    "\n",
    "* **优点:**\n",
    "\t* 权重数量比较少(被分享)\n",
    "\t* 可平行的\n",
    "\t* 可检测出多维空间子结构(特征提取)\n",
    "\t* 应用过滤器可以有较好解释性\n",
    "\t* 可应用在图像处理，文字处理，时间序列处理 等等\n",
    "* **缺点:**\n",
    "\t* 较多超参数 (卷积核大小, 步长, 等等)\n",
    "\t* 输入的数据形状应该一样 (图片维度, 文本长度等等)\n",
    "* **其他:**\n",
    "\t* 许多深度卷积神经构架经常会为了达到最好的性能(state of the art)做更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lxpgRzIjiVHv",
    "mdEditEnable": false
   },
   "source": [
    "## 过滤器\n",
    "\n",
    "卷积神经网络的核心是过滤器(或者权重，卷积核，等等叫法)会在输入数据上进行卷积操作(滑动)来获取相关特征。过滤器也是随机初始化的，但之后随着训练的过程 \"学会\" 从输入数据提取帮助优化模型的有效特征。这里会用较为非常规的方法讲解CNN，因为我们只会将它运用于2维的文本数据。每个输入都由词语组成，然后我们会把每个词语都做独热编码处理，从而得到二维的输入。这么做是因为，每个过滤器都会代表一种特征，我们会将这个过滤器运用到别的输入上从而获取相同的特征。这也叫做共享参数(parameter sharing)。\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"data/conv.gif\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "1kTABJyYj91S",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting torch\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/86/78/a113ef4e76dfb539e5c73be2aa010008b88fe1d83e324b5a3b16011bb245/torch-1.0.0-cp35-cp35m-manylinux1_x86_64.whl (591.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 591.8MB 72kB/s \n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "  Found existing installation: torch 0.4.0\n",
      "    Uninstalling torch-0.4.0:\n",
      "      Successfully uninstalled torch-0.4.0\n",
      "Successfully installed torch-1.0.0\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 更新PyTorch\n",
    "!pip3 install torch --upgrade -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kz9D2rrdmSl9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1q1FiiIHXjI_",
    "mdEditEnable": false
   },
   "source": [
    "\n",
    "这里输入是一批二维文本数据。让我们先来制造一组有64个采样的输入，其中每个采样有8个词，而每个词又由长度为10的数组表示(词大小为10的独热编码)。这就构成了大小是(64, 8, 10)的输入。[PyTorch的CNN模块](https://pytorch.org/docs/stable/nn.html#convolution-functions) 默认的输入中通道维度(channel dim)，在我们的例子中就是独热向量的维度，是在第二位的，所以输入的形状就是(64, 10, 8)。\n",
    "\n",
    "\n",
    "<img src=\"data/cnn_text1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "b6G2nBvOxR-e",
    "outputId": "3b422df4-d2c6-4f2f-cb71-925453c54948",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: torch.Size([64, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "# 假设所有输入都有同样多的词语数量\n",
    "batch_size = 64\n",
    "sequence_size = 8 # 每个输入的词语数\n",
    "one_hot_size = 10 # vocab size (num_input_channels)\n",
    "x = torch.randn(batch_size, one_hot_size, sequence_size)\n",
    "print(\"Size: {}\".format(x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZJF0l88qb-21",
    "mdEditEnable": false
   },
   "source": [
    "然后我们就会开始用过滤器对输入进行卷积操作。参考下面的图片，为了更加简明扼要，这里只用了5个大小是 (1,2) 的过滤器，而且深度和信道数一样 (都是10)，所以过滤器的形状是 (5, 2, 10), 和前面一样，我们将它变形为 (5, 10, 2)\n",
    "\n",
    "<img src=\"data/cnn_text2.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "WMK2TzgDxR8B",
    "outputId": "59562322-13b9-43cf-ec5f-04d692566364",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: torch.Size([5, 10, 2])\n",
      "Filter size: 2\n",
      "Padding: 0\n",
      "Stride: 1\n"
     ]
    }
   ],
   "source": [
    "# 创建过滤器\n",
    "out_channels = 5 # 过滤器数量\n",
    "kernel_size = 2 # 大小\n",
    "conv1 = nn.Conv1d(in_channels=one_hot_size, out_channels=out_channels, kernel_size=kernel_size)\n",
    "print(\"Size: {}\".format(conv1.weight.shape))\n",
    "print(\"Filter size: {}\".format(conv1.kernel_size[0]))\n",
    "print(\"Padding: {}\".format(conv1.padding[0]))\n",
    "print(\"Stride: {}\".format(conv1.stride[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2c_KKtP4hrJx",
    "mdEditEnable": false
   },
   "source": [
    "应用这个过滤器，我们会得到大小是 (64, 5, 7) 的输出。64 是这批数据大小，5 是信道维度因为我们用了5个过滤器，7 是卷积输出，算法如下:\n",
    "\n",
    "~$\\frac{W - F + 2P}{S} + 1 = \\frac{8 - 2 + 2(0)}{1} + 1 = 7~$\n",
    "\n",
    "其中 :\n",
    "  * W: 输入宽度\n",
    "  * F: 过滤器大小\n",
    "  * P: 内边距\n",
    "  * S: 步长\n",
    "\n",
    "\n",
    "<img src=\"data/cnn_text3.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "yjxtrM89xR5a",
    "outputId": "5892ff1d-595f-4528-b84a-dcd4295181f3",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: torch.Size([64, 5, 7])\n"
     ]
    }
   ],
   "source": [
    "# 使用过滤器进行卷积\n",
    "conv_output = conv1(x)\n",
    "print(\"Size: {}\".format(conv_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VXBbKPs1ua9G",
    "mdEditEnable": false
   },
   "source": [
    "## 池化\n",
    "我们将卷积的结果成为 **特征图(feature map)** 。由于卷积和交叉的特性，特征图里会携带许多冗余信息。 **池化(Pooling)** 操作可以讲高维的特征图进行有效降维。\n",
    "\n",
    "<img src=\"data/pool.jpeg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VCag6lk2mSwU",
    "outputId": "27fd0640-c1c5-467b-a77e-c19eb50e6749",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: torch.Size([64, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "# 最大池化\n",
    "kernel_size = 2\n",
    "pool1 = nn.MaxPool1d(kernel_size=kernel_size, stride=2, padding=0)\n",
    "pool_output = pool1(conv_output)\n",
    "print(\"Size: {}\".format(pool_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c_e4QRFwvTt8",
    "mdEditEnable": false
   },
   "source": [
    "~$\\frac{W-F}{S} + 1 = \\frac{7-2}{2} + 1 =  \\text{floor }(2.5) + 1 = 3~$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aWtHDOJgHZvk",
    "mdEditEnable": false
   },
   "source": [
    "## 使用卷积神经网络处理文本信息\n",
    "\n",
    "我们将在文本数据上使用卷积神经网络，核心概念是在表示单个字符的向量组上进行卷积来获取有意义的 [n-gram](https://blog.csdn.net/songbinxu/article/details/80209197#n-gram%E7%AE%80%E4%BB%8B) 信息。\n",
    "\n",
    "你可以简单的直接应用这样的配置来处理 [时间序列数据](https://arxiv.org/abs/1807.10707) 或和其他网络 [结合](https://arxiv.org/abs/1808.04928)。 对于文本数据来说，我们会创建不同大小的过滤器，(1,2), (1,3), (1,4) 作为不同大小 n-gram 的特征选择器。他们的输出会被拼接并输入一个全连接层(fully-connected-layer)用作类别判断。我们在这里将会对词语中的字母使用一维卷积，而在 [嵌入层](https://www.kesci.com/home/project/share/50c13cecc0bf7ba1) 这个项目中，我们会对句子中的单词作一维卷积。\n",
    "\n",
    "**词嵌入:** 找到相邻词条(token)中的暂时关系，这些暂时关系暗示了某些词语代表了相似的意义。比如 \"New Jersey\" 和 \"NJ\" 和 \"Garden State\" 有相似意思，他们都指新泽信这个地方。\n",
    "**字符嵌入:** 创建词语在字符级别的关联。比如 \"toy\" 和 \"toys\" 意思相近。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bVBZxbaAtS9u",
    "mdEditEnable": false
   },
   "source": [
    "## 配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y8QSdEcDtXUs",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "import collections\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VADCXjMwtXYN",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 设置Numpy和PyTorch随机种子\n",
    "def set_seeds(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "# 创建字典\n",
    "def create_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mpiCYECstXbT",
    "outputId": "d836f55c-3fce-4b3e-ea6b-bc32c5fb5b10",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 参数\n",
    "args = Namespace(\n",
    "    seed=1234,\n",
    "    cuda=False,\n",
    "    shuffle=True,\n",
    "    data_file=\"data/names.csv\",\n",
    "    split_data_file=\"data/split_names.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"names\",\n",
    "    train_size=0.7,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    num_epochs=20,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=64,\n",
    "    num_filters=100,\n",
    "    dropout_p=0.1,\n",
    ")\n",
    "\n",
    "# 设置种子\n",
    "set_seeds(seed=args.seed, cuda=args.cuda)\n",
    "\n",
    "# 创建保存文件夹\n",
    "create_dirs(args.save_dir)\n",
    "\n",
    "# 拓展文件路径\n",
    "args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "# 检查GPU可用性\n",
    "if torch.cuda.is_available():\n",
    "    args.cuda = True\n",
    "else:\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda), \"device: {}\".format(args.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ptb4hJ4Bw8YU",
    "mdEditEnable": false
   },
   "source": [
    "## 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bNxZQUqfmS0B",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBdQpUTQtMgu",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "url = \"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/data/surnames.csv\"\n",
    "response = urllib.request.urlopen(url)\n",
    "html = response.read()\n",
    "with open(args.data_file, 'wb') as fp:\n",
    "    fp.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "6PYCeGrStMj7",
    "outputId": "b72eb44f-a6a4-4c78-f167-48b52e2e35dc",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surname</th>\n",
       "      <th>nationality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Woodford</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coté</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kore</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Koury</td>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lebzak</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    surname nationality\n",
       "0  Woodford     English\n",
       "1      Coté      French\n",
       "2      Kore     English\n",
       "3     Koury      Arabic\n",
       "4    Lebzak     Russian"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 未处理数据\n",
    "df = pd.read_csv(args.data_file, header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "pbfVM-YatMnD",
    "outputId": "a583b5d1-3125-4b7a-d9d0-dab540c806a8",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: 2972\n",
      "French: 229\n",
      "Scottish: 75\n",
      "German: 576\n",
      "Italian: 600\n",
      "Vietnamese: 58\n",
      "Russian: 2373\n",
      "Arabic: 1603\n",
      "Chinese: 220\n",
      "Portuguese: 55\n",
      "Polish: 120\n",
      "Korean: 77\n",
      "Irish: 183\n",
      "Dutch: 236\n",
      "Japanese: 775\n",
      "Czech: 414\n",
      "Spanish: 258\n",
      "Greek: 156\n"
     ]
    }
   ],
   "source": [
    "# 按国籍切分数据集\n",
    "by_nationality = collections.defaultdict(list)\n",
    "for _, row in df.iterrows():\n",
    "    by_nationality[row.nationality].append(row.to_dict())\n",
    "for nationality in by_nationality:\n",
    "    print (\"{0}: {1}\".format(nationality, len(by_nationality[nationality])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KdGOoKFjtMpz",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 新建切分数据集\n",
    "final_list = []\n",
    "for _, item_list in sorted(by_nationality.items()):\n",
    "    if args.shuffle:\n",
    "        np.random.shuffle(item_list)\n",
    "    n = len(item_list)\n",
    "    n_train = int(args.train_size*n)\n",
    "    n_val = int(args.val_size*n)\n",
    "    n_test = int(args.test_size*n)\n",
    "\n",
    "  # 给每个数据点添加子集属性\n",
    "    for item in item_list[:n_train]:\n",
    "        item['split'] = 'train'\n",
    "    for item in item_list[n_train:n_train+n_val]:\n",
    "        item['split'] = 'val'\n",
    "    for item in item_list[n_train+n_val:]:\n",
    "        item['split'] = 'test'  \n",
    "\n",
    "    #添加到最新的列表\n",
    "    final_list.extend(item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "DyDwlzzKtMsz",
    "outputId": "2f21d2e1-ffab-4c09-e9e1-bd103cecaca3",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    7680\n",
       "test     1660\n",
       "val      1640\n",
       "Name: split, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 切分后的数据集dataframe\n",
    "split_df = pd.DataFrame(final_list)\n",
    "split_df[\"split\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "17aHMQOwtMvh",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 预处理\n",
    "def preprocess_text(text):\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text\n",
    "    \n",
    "split_df.surname = split_df.surname.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "wh6D8qfQmS2c",
    "outputId": "c28ae3d6-706d-441d-eb92-d447f7295abf",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nationality</th>\n",
       "      <th>split</th>\n",
       "      <th>surname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "      <td>bishara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "      <td>nahas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "      <td>ghanem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "      <td>tannous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "      <td>mikhail</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  nationality  split  surname\n",
       "0      Arabic  train  bishara\n",
       "1      Arabic  train    nahas\n",
       "2      Arabic  train   ghanem\n",
       "3      Arabic  train  tannous\n",
       "4      Arabic  train  mikhail"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 存入CSV\n",
    "split_df.to_csv(args.split_data_file, index=False)\n",
    "split_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6nZBgfQTuAA8",
    "mdEditEnable": false
   },
   "source": [
    "## 词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TeRVQlRZuBgA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "\n",
    "        # 词条转换为索引\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self.token_to_idx = token_to_idx\n",
    "\n",
    "        # 索引转换为词条\n",
    "        self.idx_to_token = {idx: token \\\n",
    "                             for token, idx in self.token_to_idx.items()}\n",
    "        \n",
    "        # 添加未知词条\n",
    "        self.add_unk = add_unk\n",
    "        self.unk_token = unk_token\n",
    "        if self.add_unk:\n",
    "            self.unk_index = self.add_token(self.unk_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self.token_to_idx,\n",
    "                'add_unk': self.add_unk, 'unk_token': self.unk_token}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        return [self.add_token[token] for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        if self.add_unk:\n",
    "            index = self.token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            index =  self.token_to_idx[token]\n",
    "        return index\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        if index not in self.idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self.idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "bH8LMH9wuBi9",
    "outputId": "1a9cde73-19a1-4db2-a789-9ecab9e53fa2",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary(size=18)>\n",
      "0\n",
      "English\n"
     ]
    }
   ],
   "source": [
    "# 词汇表实例\n",
    "nationality_vocab = Vocabulary(add_unk=False)\n",
    "for index, row in df.iterrows():\n",
    "    nationality_vocab.add_token(row.nationality)\n",
    "print (nationality_vocab) # __str__\n",
    "index = nationality_vocab.lookup_token(\"English\")\n",
    "print (index)\n",
    "print (nationality_vocab.lookup_index(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "57a1lzHPuHHm",
    "mdEditEnable": false
   },
   "source": [
    "## 向量器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MwS5BEV-uBlt",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class SurnameVectorizer(object):\n",
    "    def __init__(self, surname_vocab, nationality_vocab):\n",
    "        self.surname_vocab = surname_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "\n",
    "    def vectorize(self, surname):\n",
    "        one_hot_matrix_size = (len(surname), len(self.surname_vocab))\n",
    "        one_hot_matrix = np.zeros(one_hot_matrix_size, dtype=np.float32)\n",
    "                               \n",
    "        for position_index, character in enumerate(surname):\n",
    "            character_index = self.surname_vocab.lookup_token(character)\n",
    "            one_hot_matrix[position_index][character_index] = 1\n",
    "        \n",
    "        return one_hot_matrix\n",
    "    \n",
    "    def unvectorize(self, one_hot_matrix):\n",
    "        len_name = len(one_hot_matrix)\n",
    "        indices = np.zeros(len_name)\n",
    "        for i in range(len_name):\n",
    "            indices[i] = np.where(one_hot_matrix[i]==1)[0][0]\n",
    "        surname = [self.surname_vocab.lookup_index(index) for index in indices]\n",
    "        return surname\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df):\n",
    "        surname_vocab = Vocabulary(add_unk=True)\n",
    "        nationality_vocab = Vocabulary(add_unk=False)\n",
    "\n",
    "        # 创建词汇表\n",
    "        for index, row in df.iterrows():\n",
    "            for letter in row.surname: # 字符级词条化\n",
    "                surname_vocab.add_token(letter)\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "        return cls(surname_vocab, nationality_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        surname_vocab = Vocabulary.from_serializable(contents['surname_vocab'])\n",
    "        nationality_vocab =  Vocabulary.from_serializable(contents['nationality_vocab'])\n",
    "        return cls(surname_vocab, nationality_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'surname_vocab': self.surname_vocab.to_serializable(),\n",
    "                'nationality_vocab': self.nationality_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "zq7RoFAXuBo9",
    "outputId": "a8e8f739-5d23-4164-e0e2-fac1b13fe177",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary(size=28)>\n",
      "<Vocabulary(size=18)>\n",
      "(4, 28)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]]\n",
      "['g', 'o', 'k', 'u']\n"
     ]
    }
   ],
   "source": [
    "# 向量器实例\n",
    "vectorizer = SurnameVectorizer.from_dataframe(split_df)\n",
    "print (vectorizer.surname_vocab)\n",
    "print (vectorizer.nationality_vocab)\n",
    "vectorized_surname = vectorizer.vectorize(preprocess_text(\"goku\"))\n",
    "print (np.shape(vectorized_surname))\n",
    "print (vectorized_surname)\n",
    "print (vectorizer.unvectorize(vectorized_surname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wwQ8MNp5ZfeG",
    "mdEditEnable": false
   },
   "source": [
    "**注意**: 和MLP时的独热编码不同，这里我们成功保存了和姓相关的结构信息。这是因为我们这次用的是字符，并且有很大的词汇表来处理文本，这个方法并不永远可靠。在接下来的课程项目中里我们将会探索 **嵌入法**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mnf7gXgKuOgp",
    "mdEditEnable": false
   },
   "source": [
    "## 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YYqzM53fuBrf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gjolk855uPrA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, df, vectorizer):\n",
    "        self.df = df\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "        # Data splits\n",
    "        self.train_df = self.df[self.df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "        self.val_df = self.df[self.df.split=='val']\n",
    "        self.val_size = len(self.val_df)\n",
    "        self.test_df = self.df[self.df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "        self.lookup_dict = {'train': (self.train_df, self.train_size), \n",
    "                            'val': (self.val_df, self.val_size),\n",
    "                            'test': (self.test_df, self.test_size)}\n",
    "        self.set_split('train')\n",
    "\n",
    "        # Class weights (for imbalances)\n",
    "        class_counts = df.nationality.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self.vectorizer.nationality_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, split_data_file):\n",
    "        df = pd.read_csv(split_data_file, header=0)\n",
    "        train_df = df[df.split=='train']\n",
    "        return cls(df, SurnameVectorizer.from_dataframe(train_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, split_data_file, vectorizer_filepath):\n",
    "        df = pd.read_csv(split_data_file, header=0)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(df, vectorizer)\n",
    "\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self.vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self.target_split = split\n",
    "        self.target_df, self.target_size = self.lookup_dict[split]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Dataset(split={0}, size={1})\".format(\n",
    "            self.target_split, self.target_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.target_df.iloc[index]\n",
    "        surname_vector = self.vectorizer.vectorize(row.surname)\n",
    "        nationality_index = self.vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
    "        return {'surname': surname_vector, 'nationality': nationality_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "\n",
    "    def generate_batches(self, batch_size, collate_fn, shuffle=True, \n",
    "                         drop_last=True, device=\"cpu\"):\n",
    "        dataloader = DataLoader(dataset=self, batch_size=batch_size,\n",
    "                                collate_fn=collate_fn, shuffle=shuffle, \n",
    "                                drop_last=drop_last)\n",
    "        for data_dict in dataloader:\n",
    "            out_data_dict = {}\n",
    "            for name, tensor in data_dict.items():\n",
    "                out_data_dict[name] = data_dict[name].to(device)\n",
    "            yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "hvy-CJVSuPuS",
    "outputId": "962f8b0d-116b-4827-b9be-6ca25104858d",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Dataset(split=train, size=7680)\n",
      "(6, 28)\n",
      "tensor([0.0006, 0.0045, 0.0024, 0.0042, 0.0003, 0.0044, 0.0017, 0.0064, 0.0055,\n",
      "        0.0017, 0.0013, 0.0130, 0.0083, 0.0182, 0.0004, 0.0133, 0.0039, 0.0172])\n"
     ]
    }
   ],
   "source": [
    "# 数据集实例\n",
    "dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.split_data_file)\n",
    "print (dataset) # __str__\n",
    "print (np.shape(dataset[5]['surname'])) # __getitem__\n",
    "print (dataset.class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XY0CqM2Rd3Im",
    "mdEditEnable": false
   },
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pWGpAzKPd32f",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d7Q0_nkjd30L",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class SurnameModel(nn.Module):\n",
    "    def __init__(self, num_input_channels, num_output_channels, num_classes, dropout_p):\n",
    "        super(SurnameModel, self).__init__()\n",
    "        \n",
    "        # 卷积权重\n",
    "        self.conv = nn.ModuleList([nn.Conv1d(num_input_channels, num_output_channels, \n",
    "                                             kernel_size=f) for f in [2,3,4]])\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "       \n",
    "        # 全连接层权重\n",
    "        self.fc1 = nn.Linear(num_output_channels*3, num_classes)\n",
    "\n",
    "    def forward(self, x, channel_first=False, apply_softmax=False):\n",
    "        \n",
    "        # 重新\n",
    "        if not channel_first:\n",
    "            x = x.transpose(1, 2)\n",
    "            \n",
    "        # 卷积输出\n",
    "        z = [conv(x) for conv in self.conv]\n",
    "        z = [F.max_pool1d(zz, zz.size(2)).squeeze(2) for zz in z]\n",
    "        z = [F.relu(zz) for zz in z]\n",
    "        \n",
    "        # 连接卷积输出\n",
    "        z = torch.cat(z, 1)\n",
    "        z = self.dropout(z)\n",
    "\n",
    "        # 全连接层\n",
    "        y_pred = self.fc1(z)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            y_pred = F.softmax(y_pred, dim=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rh_1heUNSUYN",
    "mdEditEnable": false
   },
   "source": [
    "## 训练\n",
    "\n",
    "**内距:** 同一批的输入信息需要有相同的形状大小。向量器将token转化为向量形式，但是在某一个特别的数据批次中，我们可能会有不同大小的数据输入。解决方案是找到最大的数据，并且将其他数据填充至一样的大小，一般我们会对较小的数据填充 0 值向量。\n",
    "\n",
    "训练时，训练类的 批生成器(batch generater) `generate_batches` 函数生成数据采样，我们用 `collate_fn` 找到最大的输入，用`pad_seq`函数进行填充操作，直到所有数据都有相同的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wLLmIuKRkNYW",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sV-Dc_5ykNgS",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, dataset, model, model_state_file, save_dir, device, shuffle, \n",
    "               num_epochs, batch_size, learning_rate, early_stopping_criteria):\n",
    "        self.dataset = dataset\n",
    "        self.class_weights = dataset.class_weights.to(device)\n",
    "        self.model = model.to(device)\n",
    "        self.save_dir = save_dir\n",
    "        self.device = device\n",
    "        self.shuffle = shuffle\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_func = nn.CrossEntropyLoss(self.class_weights)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer=self.optimizer, mode='min', factor=0.5, patience=1)\n",
    "        self.train_state = {\n",
    "            'stop_early': False, \n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'early_stopping_criteria': early_stopping_criteria,\n",
    "            'learning_rate': learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': model_state_file}\n",
    "    \n",
    "    def update_train_state(self):\n",
    "\n",
    "        # 打印训练checkpoint\n",
    "        print (\"[EPOCH]: {0:02d} | [LR]: {1} | [TRAIN LOSS]: {2:.2f} | [TRAIN ACC]: {3:.1f}% | [VAL LOSS]: {4:.2f} | [VAL ACC]: {5:.1f}%\".format(\n",
    "          self.train_state['epoch_index'], self.train_state['learning_rate'], \n",
    "            self.train_state['train_loss'][-1], self.train_state['train_acc'][-1], \n",
    "            self.train_state['val_loss'][-1], self.train_state['val_acc'][-1]))\n",
    "\n",
    "        # 至少保存一次模型\n",
    "        if self.train_state['epoch_index'] == 0:\n",
    "            torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
    "            self.train_state['stop_early'] = False\n",
    "\n",
    "        # 如果模型性能表现有提升，再次保存\n",
    "        elif self.train_state['epoch_index'] >= 1:\n",
    "            loss_tm1, loss_t = self.train_state['val_loss'][-2:]\n",
    "\n",
    "            # 如果损失增大\n",
    "            if loss_t >= self.train_state['early_stopping_best_val']:\n",
    "                # 更新early stop的步数\n",
    "                self.train_state['early_stopping_step'] += 1\n",
    "\n",
    "            # 损失变小\n",
    "            else:\n",
    "                # 保存最优的模型\n",
    "                if loss_t < self.train_state['early_stopping_best_val']:\n",
    "                    torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
    "\n",
    "                # 重置early stopping 的步数\n",
    "                self.train_state['early_stopping_step'] = 0\n",
    "\n",
    "            # 是否需要early stop ?\n",
    "            self.train_state['stop_early'] = self.train_state['early_stopping_step'] \\\n",
    "              >= self.train_state['early_stopping_criteria']\n",
    "        return self.train_state\n",
    "  \n",
    "    def compute_accuracy(self, y_pred, y_target):\n",
    "        _, y_pred_indices = y_pred.max(dim=1)\n",
    "        n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "        return n_correct / len(y_pred_indices) * 100\n",
    "    \n",
    "    def pad_seq(self, seq, length):\n",
    "        vector = np.zeros((length, len(self.dataset.vectorizer.surname_vocab)),\n",
    "                          dtype=np.int64)\n",
    "        for i in range(len(seq)):\n",
    "            vector[i] = seq[i]\n",
    "        return vector\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        \n",
    "        # 深度拷贝\n",
    "        batch_copy = copy.deepcopy(batch)\n",
    "        processed_batch = {\"surname\": [], \"nationality\": []}\n",
    "        \n",
    "        # 获取最长数据\n",
    "        max_seq_len = max([len(sample[\"surname\"]) for sample in batch_copy])\n",
    "        \n",
    "        # 填充\n",
    "        for i, sample in enumerate(batch_copy):\n",
    "            seq = sample[\"surname\"]\n",
    "            nationality = sample[\"nationality\"]\n",
    "            padded_seq = self.pad_seq(seq, max_seq_len)\n",
    "            processed_batch[\"surname\"].append(padded_seq)\n",
    "            processed_batch[\"nationality\"].append(nationality)\n",
    "            \n",
    "        # 转化为合适的tensor\n",
    "        processed_batch[\"surname\"] = torch.FloatTensor(\n",
    "            processed_batch[\"surname\"]) # 这里需要浮点数\n",
    "        processed_batch[\"nationality\"] = torch.LongTensor(\n",
    "            processed_batch[\"nationality\"])\n",
    "        \n",
    "        return processed_batch    \n",
    "  \n",
    "    def run_train_loop(self):\n",
    "        for epoch_index in range(self.num_epochs):\n",
    "            self.train_state['epoch_index'] = epoch_index\n",
    "      \n",
    "            # 在训练集上遍历\n",
    "\n",
    "            # 初始化批生成器, 将损失和准确率归零，设置为训练模式\n",
    "            self.dataset.set_split('train')\n",
    "            batch_generator = self.dataset.generate_batches(\n",
    "                batch_size=self.batch_size, collate_fn=self.collate_fn,\n",
    "                shuffle=self.shuffle, device=self.device)\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            self.model.train()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "                # 梯度归零\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # 计算输出\n",
    "                y_pred = self.model(batch_dict['surname'])\n",
    "\n",
    "                # 计算损失\n",
    "                loss = self.loss_func(y_pred, batch_dict['nationality'])\n",
    "                loss_t = loss.item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # 反向传播\n",
    "                loss.backward()\n",
    "\n",
    "                # 更新梯度\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # 计算准确率\n",
    "                acc_t = self.compute_accuracy(y_pred, batch_dict['nationality'])\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            self.train_state['train_loss'].append(running_loss)\n",
    "            self.train_state['train_acc'].append(running_acc)\n",
    "\n",
    "            # 遍历 val 集\n",
    "\n",
    "            # 初始化批生成器, 将损失和准确率归零，设置为训练模式\n",
    "            self.dataset.set_split('val')\n",
    "            batch_generator = self.dataset.generate_batches(\n",
    "                batch_size=self.batch_size, collate_fn=self.collate_fn, \n",
    "                shuffle=self.shuffle, device=self.device)\n",
    "            running_loss = 0.\n",
    "            running_acc = 0.\n",
    "            self.model.eval()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "                # 计算输出\n",
    "                y_pred =  self.model(batch_dict['surname'])\n",
    "\n",
    "                # 计算损失\n",
    "                loss = self.loss_func(y_pred, batch_dict['nationality'])\n",
    "                loss_t = loss.to(\"cpu\").item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # 计算准确率\n",
    "                acc_t = self.compute_accuracy(y_pred, batch_dict['nationality'])\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            self.train_state['val_loss'].append(running_loss)\n",
    "            self.train_state['val_acc'].append(running_acc)\n",
    "\n",
    "            self.train_state = self.update_train_state()\n",
    "            self.scheduler.step(self.train_state['val_loss'][-1])\n",
    "            if self.train_state['stop_early']:\n",
    "                break\n",
    "          \n",
    "    def run_test_loop(self):\n",
    "        # 始化批生成器, 将损失和准确率归零，设置为运行模式\n",
    "        self.dataset.set_split('test')\n",
    "        batch_generator = self.dataset.generate_batches(\n",
    "            batch_size=self.batch_size, collate_fn=self.collate_fn,\n",
    "            shuffle=self.shuffle, device=self.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        self.model.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # 计算输出\n",
    "            y_pred =  self.model(batch_dict['surname'])\n",
    "\n",
    "            # 计算损失\n",
    "            loss = self.loss_func(y_pred, batch_dict['nationality'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # 计算准确率\n",
    "            acc_t = self.compute_accuracy(y_pred, batch_dict['nationality'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "        self.train_state['test_loss'] = running_loss\n",
    "        self.train_state['test_acc'] = running_acc\n",
    "    \n",
    "    def plot_performance(self):\n",
    "        # 设置图大小\n",
    "        plt.figure(figsize=(15,5))\n",
    "\n",
    "        # 画出损失\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Loss\")\n",
    "        plt.plot(trainer.train_state[\"train_loss\"], label=\"train\")\n",
    "        plt.plot(trainer.train_state[\"val_loss\"], label=\"val\")\n",
    "        plt.legend(loc='upper right')\n",
    "\n",
    "        # 画出准确率\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.plot(trainer.train_state[\"train_acc\"], label=\"train\")\n",
    "        plt.plot(trainer.train_state[\"val_acc\"], label=\"val\")\n",
    "        plt.legend(loc='lower right')\n",
    "\n",
    "        # 存图\n",
    "        plt.savefig(os.path.join(self.save_dir, \"performance.png\"))\n",
    "\n",
    "        # 展示图\n",
    "        plt.show()\n",
    "    \n",
    "    def save_train_state(self):\n",
    "        with open(os.path.join(self.save_dir, \"train_state.json\"), \"w\") as fp:\n",
    "            json.dump(self.train_state, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "OkeOQRwckNd1",
    "outputId": "92bf0944-5cae-4e91-8d9d-ae0ebccec738",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.named_modules of SurnameModel(\n",
      "  (conv): ModuleList(\n",
      "    (0): Conv1d(28, 100, kernel_size=(2,), stride=(1,))\n",
      "    (1): Conv1d(28, 100, kernel_size=(3,), stride=(1,))\n",
      "    (2): Conv1d(28, 100, kernel_size=(4,), stride=(1,))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1)\n",
      "  (fc1): Linear(in_features=300, out_features=18, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# 初始化\n",
    "dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.split_data_file)\n",
    "dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.vectorizer\n",
    "model = SurnameModel(num_input_channels=len(vectorizer.surname_vocab),\n",
    "                     num_output_channels=args.num_filters,\n",
    "                     num_classes=len(vectorizer.nationality_vocab),\n",
    "                     dropout_p=args.dropout_p)\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "3JJdOO4ZkNb3",
    "outputId": "1bc45f26-92d9-421b-bf1d-53971fa177de",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH]: 00 | [LR]: 0.001 | [TRAIN LOSS]: 2.82 | [TRAIN ACC]: 20.4% | [VAL LOSS]: 2.71 | [VAL ACC]: 36.2%\n",
      "[EPOCH]: 01 | [LR]: 0.001 | [TRAIN LOSS]: 2.54 | [TRAIN ACC]: 43.6% | [VAL LOSS]: 2.40 | [VAL ACC]: 43.6%\n",
      "[EPOCH]: 02 | [LR]: 0.001 | [TRAIN LOSS]: 2.18 | [TRAIN ACC]: 48.3% | [VAL LOSS]: 2.12 | [VAL ACC]: 48.6%\n",
      "[EPOCH]: 03 | [LR]: 0.001 | [TRAIN LOSS]: 1.88 | [TRAIN ACC]: 50.8% | [VAL LOSS]: 1.90 | [VAL ACC]: 51.6%\n",
      "[EPOCH]: 04 | [LR]: 0.001 | [TRAIN LOSS]: 1.67 | [TRAIN ACC]: 55.0% | [VAL LOSS]: 1.75 | [VAL ACC]: 54.4%\n",
      "[EPOCH]: 05 | [LR]: 0.001 | [TRAIN LOSS]: 1.51 | [TRAIN ACC]: 57.7% | [VAL LOSS]: 1.64 | [VAL ACC]: 58.1%\n",
      "[EPOCH]: 06 | [LR]: 0.001 | [TRAIN LOSS]: 1.40 | [TRAIN ACC]: 59.7% | [VAL LOSS]: 1.58 | [VAL ACC]: 57.1%\n",
      "[EPOCH]: 07 | [LR]: 0.001 | [TRAIN LOSS]: 1.29 | [TRAIN ACC]: 62.1% | [VAL LOSS]: 1.51 | [VAL ACC]: 58.0%\n",
      "[EPOCH]: 08 | [LR]: 0.001 | [TRAIN LOSS]: 1.21 | [TRAIN ACC]: 62.7% | [VAL LOSS]: 1.49 | [VAL ACC]: 63.9%\n",
      "[EPOCH]: 09 | [LR]: 0.001 | [TRAIN LOSS]: 1.12 | [TRAIN ACC]: 64.7% | [VAL LOSS]: 1.45 | [VAL ACC]: 64.3%\n",
      "[EPOCH]: 10 | [LR]: 0.001 | [TRAIN LOSS]: 1.06 | [TRAIN ACC]: 66.1% | [VAL LOSS]: 1.39 | [VAL ACC]: 63.6%\n",
      "[EPOCH]: 11 | [LR]: 0.001 | [TRAIN LOSS]: 1.00 | [TRAIN ACC]: 67.2% | [VAL LOSS]: 1.37 | [VAL ACC]: 65.4%\n",
      "[EPOCH]: 12 | [LR]: 0.001 | [TRAIN LOSS]: 0.94 | [TRAIN ACC]: 67.9% | [VAL LOSS]: 1.35 | [VAL ACC]: 65.2%\n",
      "[EPOCH]: 13 | [LR]: 0.001 | [TRAIN LOSS]: 0.90 | [TRAIN ACC]: 69.0% | [VAL LOSS]: 1.33 | [VAL ACC]: 66.0%\n",
      "[EPOCH]: 14 | [LR]: 0.001 | [TRAIN LOSS]: 0.85 | [TRAIN ACC]: 70.0% | [VAL LOSS]: 1.31 | [VAL ACC]: 66.4%\n",
      "[EPOCH]: 15 | [LR]: 0.001 | [TRAIN LOSS]: 0.82 | [TRAIN ACC]: 71.1% | [VAL LOSS]: 1.31 | [VAL ACC]: 64.6%\n",
      "[EPOCH]: 16 | [LR]: 0.001 | [TRAIN LOSS]: 0.78 | [TRAIN ACC]: 70.9% | [VAL LOSS]: 1.31 | [VAL ACC]: 64.1%\n",
      "[EPOCH]: 17 | [LR]: 0.001 | [TRAIN LOSS]: 0.75 | [TRAIN ACC]: 71.8% | [VAL LOSS]: 1.29 | [VAL ACC]: 67.1%\n",
      "[EPOCH]: 18 | [LR]: 0.001 | [TRAIN LOSS]: 0.72 | [TRAIN ACC]: 72.7% | [VAL LOSS]: 1.28 | [VAL ACC]: 69.4%\n",
      "[EPOCH]: 19 | [LR]: 0.001 | [TRAIN LOSS]: 0.69 | [TRAIN ACC]: 73.8% | [VAL LOSS]: 1.28 | [VAL ACC]: 69.2%\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "trainer = Trainer(dataset=dataset, model=model, \n",
    "                  model_state_file=args.model_state_file, \n",
    "                  save_dir=args.save_dir, device=args.device,\n",
    "                  shuffle=args.shuffle, num_epochs=args.num_epochs, \n",
    "                  batch_size=args.batch_size, learning_rate=args.learning_rate, \n",
    "                  early_stopping_criteria=args.early_stopping_criteria)\n",
    "trainer.run_train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "0QLZfEyznVpT",
    "outputId": "0f436371-3b96-4e59-a0fd-6868688c59e9",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/0QLZfEyznVpT/pk4qfgosev.png\">"
      ],
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 画出性能\n",
    "trainer.plot_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "BWGzMSaBnYMb",
    "outputId": "a6efc12c-b694-4741-ef6c-1b3c6caadb02",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.32\n",
      "Test Accuracy: 67.7%\n"
     ]
    }
   ],
   "source": [
    "# 测试集上的性能\n",
    "trainer.run_test_loop()\n",
    "print(\"Test loss: {0:.2f}\".format(trainer.train_state['test_loss']))\n",
    "print(\"Test Accuracy: {0:.1f}%\".format(trainer.train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5672VEginYnY",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 保存结果\n",
    "trainer.save_train_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HN1g2vP3nad_",
    "mdEditEnable": false
   },
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Myr8QQjKnZ7k",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Inference(object):\n",
    "    def __init__(self, model, vectorizer):\n",
    "        self.model = model\n",
    "        self.vectorizer = vectorizer\n",
    "  \n",
    "    def predict_nationality(self, surname):\n",
    "        # 向前传播\n",
    "        vectorized_surname = torch.tensor(self.vectorizer.vectorize(surname)).unsqueeze(0)\n",
    "        self.model.eval()\n",
    "        y_pred = self.model(vectorized_surname, apply_softmax=True)\n",
    "\n",
    "        # 可能性最高的国籍\n",
    "        y_prob, indices = y_pred.max(dim=1)\n",
    "        index = indices.item()\n",
    "\n",
    "        # 预测出的国籍\n",
    "        nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
    "        probability = y_prob.item()\n",
    "        return {'nationality': nationality, 'probability': probability}\n",
    "  \n",
    "    def predict_top_k(self, surname, k):\n",
    "        # 向前传播\n",
    "        vectorized_surname = torch.tensor(self.vectorizer.vectorize(surname)).unsqueeze(0)\n",
    "        self.model.eval()\n",
    "        y_pred = self.model(vectorized_surname, apply_softmax=True)\n",
    "\n",
    "        # 最有可能的K个国籍\n",
    "        y_prob, indices = torch.topk(y_pred, k=k)\n",
    "        probabilities = y_prob.detach().numpy()[0]\n",
    "        indices = indices.detach().numpy()[0]\n",
    "\n",
    "        # 结果\n",
    "        results = []\n",
    "        for probability, index in zip(probabilities, indices):\n",
    "            nationality = self.vectorizer.nationality_vocab.lookup_index(index)\n",
    "            results.append({'nationality': nationality, 'probability': probability})\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "vV2SBrXpdllN",
    "outputId": "b46c411d-7de6-4afa-a3bd-5a073a67550c",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.named_modules of SurnameModel(\n",
      "  (conv): ModuleList(\n",
      "    (0): Conv1d(28, 100, kernel_size=(2,), stride=(1,))\n",
      "    (1): Conv1d(28, 100, kernel_size=(3,), stride=(1,))\n",
      "    (2): Conv1d(28, 100, kernel_size=(4,), stride=(1,))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1)\n",
      "  (fc1): Linear(in_features=300, out_features=18, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "dataset = SurnameDataset.load_dataset_and_load_vectorizer(\n",
    "    args.split_data_file, args.vectorizer_file)\n",
    "vectorizer = dataset.vectorizer\n",
    "model = SurnameModel(num_input_channels=len(vectorizer.surname_vocab),\n",
    "                     num_output_channels=args.num_filters,\n",
    "                     num_classes=len(vectorizer.nationality_vocab),\n",
    "                     dropout_p=args.dropout_p)\n",
    "model.load_state_dict(torch.load(args.model_state_file))\n",
    "model = model.to(\"cpu\")\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "TRc5KCZinaBh",
    "outputId": "cdcbf2ad-4576-4a88-9c23-32bae30bc4de",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stream",
     "output_type": "stream",
     "text": [
      "Enter a surname to classify: : giku"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giku → Japanese (p=0.87)\n"
     ]
    }
   ],
   "source": [
    "# 预测\n",
    "inference = Inference(model=model, vectorizer=vectorizer)\n",
    "surname = input(\"Enter a surname to classify: \")\n",
    "prediction = inference.predict_nationality(preprocess_text(surname))\n",
    "print(\"{} → {} (p={:0.2f})\".format(surname, prediction['nationality'], \n",
    "                                    prediction['probability']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "P5slsQKwnZ_H",
    "outputId": "a4dfae91-cb3a-47e3-8d98-71680f12fa5a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giku → Japanese (p=0.87)\n",
      "giku → Russian (p=0.08)\n",
      "giku → Arabic (p=0.02)\n",
      "giku → Czech (p=0.01)\n",
      "giku → Korean (p=0.00)\n",
      "giku → Greek (p=0.00)\n",
      "giku → Vietnamese (p=0.00)\n",
      "giku → Chinese (p=0.00)\n",
      "giku → Polish (p=0.00)\n",
      "giku → Irish (p=0.00)\n",
      "giku → English (p=0.00)\n",
      "giku → Scottish (p=0.00)\n",
      "giku → German (p=0.00)\n",
      "giku → Dutch (p=0.00)\n",
      "giku → French (p=0.00)\n",
      "giku → Italian (p=0.00)\n",
      "giku → Portuguese (p=0.00)\n",
      "giku → Spanish (p=0.00)\n"
     ]
    }
   ],
   "source": [
    "# 可能性最高的k个预测\n",
    "top_k = inference.predict_top_k(preprocess_text(surname), \n",
    "                                k=len(vectorizer.nationality_vocab))\n",
    "for result in top_k:\n",
    "    print (\"{} → {} (p={:0.2f})\".format(surname, result['nationality'], \n",
    "                                         result['probability']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9koMITOdzfZB",
    "mdEditEnable": false
   },
   "source": [
    "## 批标准化 Batch normalization\n",
    "\n",
    "在开始训练前，我们已经对数据进行了诸如零均值，单位方差等标准化操作来帮助模型收敛。输入数据在训练过程中，经过隔层网络和各种非线性变换，显然是一直在变化的。这种情况被称作 内协变量偏移 (internal covariate shift 没有正式的中文翻译)，这种情况的发生会使训练变慢，这时候需要我们降低学习率。通用的解决方案是做 [批标准化处理 batch normalization](https://arxiv.org/abs/1502.03167) (batchnorm)，让标准化成为模型构架的一部分。\n",
    "\n",
    "~$BN = \\frac{a - \\mu_{x}}{\\sqrt{\\sigma^2_{x} + \\epsilon}}  * \\gamma + \\beta~$\n",
    "\n",
    "where:\n",
    "* ~$a~$ = 激活 | ~$\\in \\mathbb{R}^{NXH}~$ (~$N~$ 样本数量, ~$H~$ 隐藏维度)\n",
    "* ~$\\mu_{x}~$ = 隐藏维度的均值 | ~$\\in \\mathbb{R}^{1XH}~$\n",
    "* ~$\\sigma^2_{x}~$ = 隐藏维度的方差 | ~$\\in \\mathbb{R}^{1XH}~$\n",
    "* ~$epsilon~$ = 噪声\n",
    "* ~$\\gamma~$ = 规模参数 (通过学习得到)\n",
    "* ~$\\beta~$ = 偏移参数 (通过学习得到)\n",
    "\n",
    "但是在激活之前将输入标准化又有什么意义呢？批标准化操作只作用于隐藏的维度上( 我们这里是`num_output_channels` )，并不会作用于整个激活矩阵。每一个隐藏维度的均值和方差都会使用当前批所有数据样本进行计算，训练中批标准化还会用到激活矩阵的均值和方差。但是测试中，样本大小会出现偏差，所以一般模型会使用在训练过程中存下来的均值和方差。PyTorch 的 [`BatchNorm`类](https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm1d) 已经帮我们处理好了这个问题。\n",
    "\n",
    "<img src=\"data/layernorm.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RsWdAKVEHvyV",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 带批标准化的模型\n",
    "class SurnameModel_BN(nn.Module):\n",
    "    def __init__(self, num_input_channels, num_output_channels, num_classes, dropout_p):\n",
    "        super(SurnameModel_BN, self).__init__()\n",
    "        \n",
    "        # 卷积权重\n",
    "        self.conv = nn.ModuleList([nn.Conv1d(num_input_channels, num_output_channels, \n",
    "                                             kernel_size=f) for f in [2,3,4]])\n",
    "        self.conv_bn = nn.ModuleList([nn.BatchNorm1d(num_output_channels) # define batchnorms\n",
    "                                      for i in range(3)])\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "       \n",
    "        # 全连接层权重\n",
    "        self.fc1 = nn.Linear(num_output_channels*3, num_classes)\n",
    "\n",
    "    def forward(self, x, channel_first=False, apply_softmax=False):\n",
    "        \n",
    "        # 转换输入格式\n",
    "        if not channel_first:\n",
    "            x = x.transpose(1, 2)\n",
    "            \n",
    "        # 卷积输出\n",
    "        z = [F.relu(conv_bn(conv(x))) for conv, conv_bn in zip(self.conv, self.conv_bn)]\n",
    "        z = [F.max_pool1d(zz, zz.size(2)).squeeze(2) for zz in z]\n",
    "        \n",
    "        # 拼接卷积输出\n",
    "        z = torch.cat(z, 1)\n",
    "        z = self.dropout(z)\n",
    "\n",
    "        # 全连接层\n",
    "        y_pred = self.fc1(z)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            y_pred = F.softmax(y_pred, dim=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "s_QcGx4vN3bQ",
    "outputId": "cf7b1ffa-6520-43ab-f950-b26defc1bcea",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.named_modules of SurnameModel_BN(\n",
      "  (conv): ModuleList(\n",
      "    (0): Conv1d(28, 100, kernel_size=(2,), stride=(1,))\n",
      "    (1): Conv1d(28, 100, kernel_size=(3,), stride=(1,))\n",
      "    (2): Conv1d(28, 100, kernel_size=(4,), stride=(1,))\n",
      "  )\n",
      "  (conv_bn): ModuleList(\n",
      "    (0): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1)\n",
      "  (fc1): Linear(in_features=300, out_features=18, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# 初始化\n",
    "dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.split_data_file)\n",
    "dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.vectorizer\n",
    "model = SurnameModel_BN(num_input_channels=len(vectorizer.surname_vocab),\n",
    "                        num_output_channels=args.num_filters,\n",
    "                        num_classes=len(vectorizer.nationality_vocab),\n",
    "                        dropout_p=args.dropout_p)\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tBXzxtiaxmXi",
    "mdEditEnable": false
   },
   "source": [
    "训练这个带有 batchnorm 的模型后会发现在验证集上的性能提高了大约 2~5 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "ERMGiPgAPssx",
    "outputId": "950c476f-01a3-48b8-9c4c-da4b78eb3f2a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH]: 00 | [LR]: 0.001 | [TRAIN LOSS]: 2.66 | [TRAIN ACC]: 23.2% | [VAL LOSS]: 2.37 | [VAL ACC]: 39.4%\n",
      "[EPOCH]: 01 | [LR]: 0.001 | [TRAIN LOSS]: 2.07 | [TRAIN ACC]: 42.7% | [VAL LOSS]: 1.97 | [VAL ACC]: 46.8%\n",
      "[EPOCH]: 02 | [LR]: 0.001 | [TRAIN LOSS]: 1.67 | [TRAIN ACC]: 49.8% | [VAL LOSS]: 1.73 | [VAL ACC]: 47.8%\n",
      "[EPOCH]: 03 | [LR]: 0.001 | [TRAIN LOSS]: 1.43 | [TRAIN ACC]: 55.6% | [VAL LOSS]: 1.56 | [VAL ACC]: 52.0%\n",
      "[EPOCH]: 04 | [LR]: 0.001 | [TRAIN LOSS]: 1.23 | [TRAIN ACC]: 59.5% | [VAL LOSS]: 1.48 | [VAL ACC]: 56.3%\n",
      "[EPOCH]: 05 | [LR]: 0.001 | [TRAIN LOSS]: 1.09 | [TRAIN ACC]: 62.7% | [VAL LOSS]: 1.41 | [VAL ACC]: 63.9%\n",
      "[EPOCH]: 06 | [LR]: 0.001 | [TRAIN LOSS]: 0.98 | [TRAIN ACC]: 65.4% | [VAL LOSS]: 1.37 | [VAL ACC]: 61.9%\n",
      "[EPOCH]: 07 | [LR]: 0.001 | [TRAIN LOSS]: 0.91 | [TRAIN ACC]: 66.7% | [VAL LOSS]: 1.31 | [VAL ACC]: 65.2%\n",
      "[EPOCH]: 08 | [LR]: 0.001 | [TRAIN LOSS]: 0.83 | [TRAIN ACC]: 68.7% | [VAL LOSS]: 1.32 | [VAL ACC]: 60.7%\n",
      "[EPOCH]: 09 | [LR]: 0.001 | [TRAIN LOSS]: 0.77 | [TRAIN ACC]: 69.5% | [VAL LOSS]: 1.26 | [VAL ACC]: 64.4%\n",
      "[EPOCH]: 10 | [LR]: 0.001 | [TRAIN LOSS]: 0.70 | [TRAIN ACC]: 71.2% | [VAL LOSS]: 1.28 | [VAL ACC]: 65.0%\n",
      "[EPOCH]: 11 | [LR]: 0.001 | [TRAIN LOSS]: 0.67 | [TRAIN ACC]: 72.3% | [VAL LOSS]: 1.28 | [VAL ACC]: 71.1%\n",
      "[EPOCH]: 12 | [LR]: 0.001 | [TRAIN LOSS]: 0.62 | [TRAIN ACC]: 74.6% | [VAL LOSS]: 1.27 | [VAL ACC]: 67.1%\n",
      "[EPOCH]: 13 | [LR]: 0.001 | [TRAIN LOSS]: 0.57 | [TRAIN ACC]: 75.2% | [VAL LOSS]: 1.25 | [VAL ACC]: 68.3%\n",
      "[EPOCH]: 14 | [LR]: 0.001 | [TRAIN LOSS]: 0.55 | [TRAIN ACC]: 75.5% | [VAL LOSS]: 1.31 | [VAL ACC]: 71.7%\n",
      "[EPOCH]: 15 | [LR]: 0.001 | [TRAIN LOSS]: 0.54 | [TRAIN ACC]: 76.9% | [VAL LOSS]: 1.27 | [VAL ACC]: 68.5%\n",
      "[EPOCH]: 16 | [LR]: 0.001 | [TRAIN LOSS]: 0.50 | [TRAIN ACC]: 77.9% | [VAL LOSS]: 1.25 | [VAL ACC]: 72.3%\n",
      "[EPOCH]: 17 | [LR]: 0.001 | [TRAIN LOSS]: 0.49 | [TRAIN ACC]: 78.1% | [VAL LOSS]: 1.25 | [VAL ACC]: 68.6%\n",
      "[EPOCH]: 18 | [LR]: 0.001 | [TRAIN LOSS]: 0.48 | [TRAIN ACC]: 78.3% | [VAL LOSS]: 1.26 | [VAL ACC]: 71.4%\n",
      "[EPOCH]: 19 | [LR]: 0.001 | [TRAIN LOSS]: 0.46 | [TRAIN ACC]: 78.2% | [VAL LOSS]: 1.32 | [VAL ACC]: 71.9%\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "trainer = Trainer(dataset=dataset, model=model, \n",
    "                  model_state_file=args.model_state_file, \n",
    "                  save_dir=args.save_dir, device=args.device,\n",
    "                  shuffle=args.shuffle, num_epochs=args.num_epochs, \n",
    "                  batch_size=args.batch_size, learning_rate=args.learning_rate, \n",
    "                  early_stopping_criteria=args.early_stopping_criteria)\n",
    "trainer.run_train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "iiAW6AL0QAJ8",
    "outputId": "8409aecf-1c40-4a86-9bb9-38a6bf31affd",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/iiAW6AL0QAJ8/pk4r43ho73.png\">"
      ],
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 画出性能\n",
    "trainer.plot_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "GPQH0NVwQAO3",
    "outputId": "5bad94de-2bb3-4e50-f9a3-6a5a521e1eb7",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.34\n",
      "Test Accuracy: 69.7%\n"
     ]
    }
   ],
   "source": [
    "# 测试集性能\n",
    "trainer.run_test_loop()\n",
    "print(\"Test loss: {0:.2f}\".format(trainer.train_state['test_loss']))\n",
    "print(\"Test Accuracy: {0:.1f}%\".format(trainer.train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4EDE4FC55DF403882A3ABF2563F4EFE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
