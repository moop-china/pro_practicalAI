{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OLIxEDq6VhvZ",
    "mdEditEnable": false
   },
   "source": [
    "## 递归神经网络进阶\n",
    "\n",
    "<img src=\"data/logo.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "这个项目中我们会包含许多RNN的进阶知识内容。\n",
    "\n",
    "1. 条件隐藏状态\n",
    "2. 字符级嵌入\n",
    "3. 编码器和解码器\n",
    "4. 注意机制\n",
    "5. 实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "41r7MWJnY0m8",
    "mdEditEnable": false
   },
   "source": [
    "## 配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EDD2A42963B94F9994AB807D6C9B187A",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already up-to-date: torch in /opt/conda/lib/python3.5/site-packages (1.0.0)\n",
      "Requirement already up-to-date: torchvision in /opt/conda/lib/python3.5/site-packages (0.2.1)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.5/site-packages (from torchvision) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /opt/conda/lib/python3.5/site-packages (from torchvision) (1.14.2)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /opt/conda/lib/python3.5/site-packages (from torchvision) (4.2.1)\n",
      "Requirement already satisfied, skipping upgrade: olefile in /opt/conda/lib/python3.5/site-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision --upgrade -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p0FbOd6IZmzX",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "import collections\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bOsqAo4XZpXQ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set Numpy and PyTorch seeds\n",
    "def set_seeds(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "# Creating directories\n",
    "def create_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QHfvEzQ9ZweF",
    "outputId": "a69944ff-021d-4d04-e920-cfc49112a34c",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "# Arguments\n",
    "args = Namespace(\n",
    "    seed=1234,\n",
    "    cuda=True,\n",
    "    batch_size=4,\n",
    "    condition_vocab_size=3, # vocabulary for condition possibilities\n",
    "    embedding_dim=100,\n",
    "    rnn_hidden_dim=100,\n",
    "    hidden_dim=100,\n",
    "    num_layers=1,\n",
    "    bidirectional=False,\n",
    ")\n",
    "\n",
    "# Set seeds\n",
    "set_seeds(seed=args.seed, cuda=args.cuda)\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pKlb9SjfpbED",
    "mdEditEnable": false
   },
   "source": [
    "# 条件递归神经网络\n",
    "给RNN加上条件判断实际上是在为正确的预测添加附加信息。我们可以将这些信息编码(嵌入)然后将它和序列输入数据一起穿进模型。依旧举上个项目中文本分类的例子，假设每篇文章的出版商(NYTimes， ESPN 等)已知。我们就可以用这个信息来帮助预测(比如ESPN发行的新闻更可能是体育新闻)。\n",
    "**注意**: 如果条件信息对序列中每个输入都不一样，那只需将它和每个时间步的数据联系起来就好了\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B419D2C2228C4E738A38BED02C80E68B",
    "mdEditEnable": false
   },
   "source": [
    "\n",
    "**第一步**： 把初始隐藏状态替换为编码后的条件信息，保证它的大小和RNN的隐藏状态大小一样\n",
    "\n",
    "<img src=\"data/conditioned_rnn1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbrlQHx2x8Aa",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cFoiV-fqmvRo",
    "outputId": "9843f756-8d71-4686-b479-b521df9b6f3c",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 100])\n"
     ]
    }
   ],
   "source": [
    "# 条件\n",
    "condition = torch.LongTensor([0, 2, 1, 2]) # 批大小4， 词汇表大小3\n",
    "condition_embeddings = nn.Embedding(\n",
    "    embedding_dim=args.embedding_dim, # 应该和RNN隐藏维一样\n",
    "    num_embeddings=args.condition_vocab_size) # 条件各异\n",
    "\n",
    "# 初始化隐藏状态\n",
    "num_directions = 1\n",
    "if args.bidirectional:\n",
    "    num_directions = 2\n",
    "    \n",
    "# If using multiple layers and directions, the hidden state needs to match that size\n",
    "# 如果使用了多层或者多向网络， 隐藏状态需要匹配大小\n",
    "hidden_t = condition_embeddings(condition).unsqueeze(0).repeat(\n",
    "    args.num_layers * num_directions, 1, 1).to(args.device) # initial state to RNN\n",
    "print (hidden_t.size())\n",
    "\n",
    "# 传入rnn\n",
    "# y_out, _ = self.rnn(x_embedded, hidden_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yUIg5o-dpiZF",
    "mdEditEnable": false
   },
   "source": [
    "\n",
    "**第二步**: 将编码过的信息和每一时间步的隐藏状态关联，注意这里**千万不要**替换隐藏状态，RNN需要它来进行训练\n",
    "\n",
    "<img src=\"data/conditioned_rnn2.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eQ-h28o-pi4X",
    "outputId": "4143190d-c452-48cc-cc96-1a2f0f7fc5ee",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 100])\n"
     ]
    }
   ],
   "source": [
    "# 初始化隐藏状态\n",
    "hidden_t = torch.zeros((args.num_layers * num_directions, args.batch_size, args.rnn_hidden_dim))\n",
    "print (hidden_t.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Z6hYSIdqBQ4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def concat_condition(condition_embeddings, condition, hidden_t, num_layers, num_directions):\n",
    "    condition_t = condition_embeddings(condition).unsqueeze(0).repeat(\n",
    "        num_layers * num_directions, 1, 1)\n",
    "    hidden_t = torch.cat([hidden_t, condition_t], 2)\n",
    "    return hidden_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Tjyzq_s5pixL",
    "outputId": "f4f62742-044e-46ef-cc46-fc21a3c52c78",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 200])\n"
     ]
    }
   ],
   "source": [
    "# 循环输入\n",
    "hiddens = []\n",
    "seq_size = 1\n",
    "for t in range(seq_size):\n",
    "    hidden_t = concat_condition(condition_embeddings, condition, hidden_t, \n",
    "                                args.num_layers, num_directions).to(args.device)\n",
    "    print (hidden_t.size())\n",
    "    \n",
    "    # 传入rnn\n",
    "    # hidden_t = rnn_cell(x_in[t], hidden_t)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-SZgVuwebm_4",
    "mdEditEnable": false
   },
   "source": [
    "## 字符级嵌入\n",
    "\n",
    "卷积操作可能会有句子中包含用字符表示的单词 |  ~$\\in \\mathbb{R}^{NXSXWXE}~$ 而输出是每个词的嵌入(根据具体的卷积操作)。\n",
    "\n",
    "**词嵌入**: 找到相邻token中的暂时关系，这些暂时关系暗示了某些词语代表了相似的意义。比如 \"New Jersey\" 和 \"NJ\" 和 \"Garden State\" 有相似意思，他们都指新泽信这个地方。\n",
    "\n",
    "**字符嵌入**: 创建词语在字符级别的关联。比如 \"toy\" 和 \"toys\" 意思相近。\n",
    "\n",
    "<img src=\"data/char_embeddings.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QOdIvz0G3O8C",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 参数\n",
    "args = Namespace(\n",
    "    seed=1234,\n",
    "    cuda=False,\n",
    "    shuffle=True,\n",
    "    batch_size=64,\n",
    "    vocab_size=20, # 词汇表\n",
    "    seq_size=10, # 句子的最长长度\n",
    "    word_size=15, # 词语的最长长度\n",
    "    embedding_dim=100,\n",
    "    num_filters=100, # 过滤器大小\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "raztXIeYXYJT",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_embeddings, num_input_channels, \n",
    "                 num_output_channels, padding_idx):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # 字符嵌入\n",
    "        self.embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
    "                                       num_embeddings=num_embeddings,\n",
    "                                       padding_idx=padding_idx)\n",
    "        \n",
    "        # 卷积权重\n",
    "        self.conv = nn.ModuleList([nn.Conv1d(num_input_channels, num_output_channels, \n",
    "                                             kernel_size=f) for f in [2,3,4]])\n",
    "\n",
    "    def forward(self, x, channel_first=False, apply_softmax=False):\n",
    "        \n",
    "        # x: (N, seq_len, word_len)\n",
    "        input_shape = x.size()\n",
    "        batch_size, seq_len, word_len = input_shape\n",
    "        x = x.view(-1, word_len) # (N*seq_len, word_len)\n",
    "        \n",
    "        # 嵌入\n",
    "        x = self.embeddings(x) # (N*seq_len, word_len, embedding_dim)\n",
    "        \n",
    "        # 重新排布输入各维位置 (N, embedding_dim, word_len)\n",
    "        if not channel_first:\n",
    "            x = x.transpose(1, 2)\n",
    "        \n",
    "        # 卷积\n",
    "        z = [F.relu(conv(x)) for conv in self.conv]\n",
    "        \n",
    "        # 池化\n",
    "        z = [F.max_pool1d(zz, zz.size(2)).squeeze(2) for zz in z] \n",
    "        z = [zz.view(batch_size, seq_len, -1) for zz in z] # (N, seq_len, embedding_dim)\n",
    "        \n",
    "        # 拼接或者字符级嵌入\n",
    "        z = torch.cat(z, 2) \n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MzHVs8Xe0Zph",
    "outputId": "ff91c1ac-5bc4-446c-9047-8b4b58570e13",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10, 15])\n"
     ]
    }
   ],
   "source": [
    "# 输入\n",
    "input_size = (args.batch_size, args.seq_size, args.word_size)\n",
    "x_in = torch.randint(low=0, high=args.vocab_size, size=input_size).long()\n",
    "print (x_in.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "0B_Xscby2PMQ",
    "outputId": "05b0c3ac-429f-47aa-9526-718e55dfc897",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.named_modules of Model(\n",
      "  (embeddings): Embedding(20, 100, padding_idx=0)\n",
      "  (conv): ModuleList(\n",
      "    (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
      "    (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
      "    (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# 初始化字符级嵌入\n",
    "model = Model(embedding_dim=args.embedding_dim, \n",
    "              num_embeddings=args.vocab_size, \n",
    "              num_input_channels=args.embedding_dim, \n",
    "              num_output_channels=args.num_filters,\n",
    "              padding_idx=0)\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8DIgeEZFXYR2",
    "outputId": "ffdbfabf-5f60-4045-be84-23dfb65fd424",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10, 300])\n"
     ]
    }
   ],
   "source": [
    "# 向前传播得到字符级嵌入\n",
    "z = model(x_in)\n",
    "print (z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nzTscaE10HFA",
    "mdEditEnable": false
   },
   "source": [
    "字符嵌入有以下几种用法:\n",
    "\n",
    "1. 将字符嵌入和词嵌入拼接，然后传入RNN\n",
    "2. 直接传入RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_sixbu74kbJk",
    "mdEditEnable": false
   },
   "source": [
    "## 编码器和解码器\n",
    "\n",
    "到现在为止，我们都在使用RNN来对序列化输入进行 `编码` 得到隐藏状态。然后使用这些隐藏状态去对预测值 `解码`。我们使用的编码器就是RNN而解码器只是几层全连接层接着一层softmax(用于分类)。但其实编码器和解码器都可以有其他的结构。比如解码器也可以是一个RNN。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kfK1mAp1dlpT",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 参数\n",
    "args = Namespace(\n",
    "    batch_size=64,\n",
    "    embedding_dim=100,\n",
    "    rnn_hidden_dim=100,\n",
    "    hidden_dim=100,\n",
    "    num_layers=1,\n",
    "    bidirectional=False,\n",
    "    dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p_OJFyY97bF_",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_embeddings, rnn_hidden_dim, \n",
    "                 num_layers, bidirectional, padding_idx=0):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # 嵌入\n",
    "        self.word_embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
    "                                            num_embeddings=num_embeddings,\n",
    "                                            padding_idx=padding_idx)\n",
    "        \n",
    "        # GRU权重\n",
    "        self.gru = nn.GRU(input_size=embedding_dim, hidden_size=rnn_hidden_dim, \n",
    "                          num_layers=num_layers, batch_first=True, \n",
    "                          bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, x_in, x_lengths):\n",
    "        \n",
    "        # 词级嵌入\n",
    "        z_word = self.word_embeddings(x_in)\n",
    "   \n",
    "        # 传入RNN\n",
    "        out, h_n = self.gru(z)\n",
    "        \n",
    "        # 获取最新的相关隐藏状态\n",
    "        out = gather_last_relevant_hidden(out, x_lengths)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HRXtaGPlpyH7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, rnn_hidden_dim, hidden_dim, output_dim, dropout_p):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # 全连接权重\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, encoder_output, apply_softmax=False):\n",
    "        \n",
    "        # 全连接层\n",
    "        z = self.dropout(encoder_output)\n",
    "        z = self.fc1(z)\n",
    "        z = self.dropout(z)\n",
    "        y_pred = self.fc2(z)\n",
    "\n",
    "        if apply_softmax:\n",
    "            y_pred = F.softmax(y_pred, dim=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SnKyCPVj-OVi",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_embeddings, rnn_hidden_dim, \n",
    "                 hidden_dim, num_layers, bidirectional, output_dim, dropout_p, \n",
    "                 padding_idx=0):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = Encoder(embedding_dim, num_embeddings, rnn_hidden_dim, \n",
    "                               num_layers, bidirectional, padding_idx=0)\n",
    "        self.decoder = Decoder(rnn_hidden_dim, hidden_dim, output_dim, dropout_p)\n",
    "        \n",
    "    def forward(self, x_in, x_lengths, apply_softmax=False):\n",
    "        encoder_outputs = self.encoder(x_in, x_lengths)\n",
    "        y_pred = self.decoder(encoder_outputs, apply_softmax)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "hfeoErsc-Tum",
    "outputId": "8faa37ab-4c38-4ace-bb96-e5dc7e1483bf",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.named_parameters of Model(\n",
      "  (encoder): Encoder(\n",
      "    (word_embeddings): Embedding(1000, 100, padding_idx=0)\n",
      "    (gru): GRU(100, 100, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (dropout): Dropout(p=0.1)\n",
      "    (fc1): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "model = Model(embedding_dim=args.embedding_dim, num_embeddings=1000, \n",
    "              rnn_hidden_dim=args.rnn_hidden_dim, hidden_dim=args.hidden_dim, \n",
    "              num_layers=args.num_layers, bidirectional=args.bidirectional, \n",
    "              output_dim=4, dropout_p=args.dropout, padding_idx=0)\n",
    "print (model.named_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o_jPXuT8xlqd",
    "mdEditEnable": false
   },
   "source": [
    "## 注意力机制 (Attentional mechanisms)\n",
    "\n",
    "我们知道RNN在处理输入序列的时候，在每个时间步被处理的是那一时刻的输入和隐藏状态。在大部分情况下，如果可以访问所有输入然后在每一个时间步只关注某一些被选择出的值时比较优秀的实践。比如在机器翻译时，最好可以访问被翻译内容中的所有值，因为在翻译的过程中，结果和原句并不是严格词与词一一对应的。\n",
    "\n",
    "\n",
    "<img src=\"data/attention1.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "注意力机制听起来可能令人疑惑，所以我们来看看每一个时间步都发生了什么。\n",
    "在时间步 j, 模型已经处理了 ~$x_0, x_1, x_2, ..., x_j~$, 并且生成了隐藏状态 ~$h_0, h_1, h_2, ..., h_j~$。 主要思想其实是，在做预测时用到所有已被处理的隐藏状态，而不是最近的那一个。有几种方法可以达成这样的效果。\n",
    "\n",
    "**软注意 soft attention** 是学习并获得一个浮点数向量(概率)，然后和隐藏状态相乘得到 表示上下文的向量(context vector)\n",
    "\n",
    "\n",
    "比如. [0.1, 0.3, 0.1, 0.4, 0.1]\n",
    "\n",
    "\n",
    "**硬注意 hard attention** 是学习并获得一个进制向量, 然后和隐藏状态相乘得到 表示上下文的向量(context vector)\n",
    "比如. [0, 0, 0, 1, 0]\n",
    "\n",
    "这里我们会主要介绍 soft attention 因为它的使用其实更加广泛，并且对  每个隐藏状态对最终预测的影响  作简单的可视化。也就是它的可解释性更好\n",
    "\n",
    "<img src=\"data/attention2.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "我们将在下面的文本分类任务中使用注意力机制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n38ZJoVZnGaE",
    "mdEditEnable": false
   },
   "source": [
    "## 使用RNN进行文本分类\n",
    "\n",
    "我们会完成和前个项目一样的文本分类任务，但是会使用注意力机制来解读模型。\n",
    "\n",
    "**为什么不用机器翻译模型(machine translation)?** 一般来说在解释注意力机制时大家都会直接拿机器翻译模型做样例，但其实这样做并不是很实际。你并不能经常看到用一个序列来生成另一个序列的实际情景。所以我们会使用文本分类的例子来解读那些输入词条对分类有更强的影响力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fu7HgEqbnGFY",
    "mdEditEnable": false
   },
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "elL6BxtCmNGf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import collections\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DCf2fLmPbKKI",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def set_seeds(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "# Creating directories\n",
    "def create_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TTwkuoZdmMlF",
    "outputId": "291c03d4-6143-4395-b5c9-ab386b061737",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    seed=1234,\n",
    "    cuda=True,\n",
    "    shuffle=True,\n",
    "    data_file=\"data/news.csv\",\n",
    "    split_data_file=\"data/split_news.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"news\",\n",
    "    train_size=0.7,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    pretrained_embeddings=None,\n",
    "    cutoff=25,\n",
    "    num_epochs=5,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=128,\n",
    "    embedding_dim=100,\n",
    "    kernels=[3,5],\n",
    "    num_filters=100,\n",
    "    rnn_hidden_dim=128,\n",
    "    hidden_dim=200,\n",
    "    num_layers=1,\n",
    "    bidirectional=False,\n",
    "    dropout_p=0.25,\n",
    ")\n",
    "\n",
    "# Set seeds\n",
    "set_seeds(seed=args.seed, cuda=args.cuda)\n",
    "\n",
    "# Create save dir\n",
    "create_dirs(args.save_dir)\n",
    "\n",
    "# Expand filepaths\n",
    "args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xfiWhgX5mMQ5",
    "mdEditEnable": false
   },
   "source": [
    "### 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "baAsxXNFmMCF",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3tJi_HyOmLw-",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/data/news.csv\"\n",
    "response = urllib.request.urlopen(url)\n",
    "html = response.read()\n",
    "with open(args.data_file, 'wb') as fp:\n",
    "    fp.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "wrI_df4bmLjB",
    "outputId": "a51463a7-f37e-41e7-aca4-74038c7c6e8e",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                              title\n",
       "0  Business  Wall St. Bears Claw Back Into the Black (Reuters)\n",
       "1  Business  Carlyle Looks Toward Commercial Aerospace (Reu...\n",
       "2  Business    Oil and Economy Cloud Stocks' Outlook (Reuters)\n",
       "3  Business  Iraq Halts Oil Exports from Main Southern Pipe...\n",
       "4  Business  Oil prices soar to all-time record, posing new..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(args.data_file, header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "TreK7nqEmLTN",
    "outputId": "36145f0d-7316-4341-f270-1d8c8037c661",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sports: 30000\n",
      "World: 30000\n",
      "Business: 30000\n",
      "Sci/Tech: 30000\n"
     ]
    }
   ],
   "source": [
    "by_category = collections.defaultdict(list)\n",
    "for _, row in df.iterrows():\n",
    "    by_category[row.category].append(row.to_dict())\n",
    "for category in by_category:\n",
    "    print (\"{0}: {1}\".format(category, len(by_category[category])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "35nb3LxLmLCA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_list = []\n",
    "for _, item_list in sorted(by_category.items()):\n",
    "    if args.shuffle:\n",
    "        np.random.shuffle(item_list)\n",
    "    n = len(item_list)\n",
    "    n_train = int(args.train_size*n)\n",
    "    n_val = int(args.val_size*n)\n",
    "    n_test = int(args.test_size*n)\n",
    "\n",
    "  # Give data point a split attribute\n",
    "    for item in item_list[:n_train]:\n",
    "        item['split'] = 'train'\n",
    "    for item in item_list[n_train:n_train+n_val]:\n",
    "        item['split'] = 'val'\n",
    "    for item in item_list[n_train+n_val:]:\n",
    "        item['split'] = 'test'  \n",
    "\n",
    "    # Add to final list\n",
    "    final_list.extend(item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "Y48IvuSfmK07",
    "outputId": "3b188412-5c0a-4e71-ef50-20c4ba18082b",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    84000\n",
       "test     18000\n",
       "val      18000\n",
       "Name: split, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_df = pd.DataFrame(final_list)\n",
    "split_df[\"split\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RWuNBxAXmKk2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "    \n",
    "split_df.title = split_df.title.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "fG9n77eLmKWB",
    "outputId": "7bb68022-5848-44ac-f90c-7cdf6a7eb988",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>split</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>general electric posts higher rd quarter profit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>lilly to eliminate up to us jobs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>s amp p lowers america west outlook to negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>does rand walk the talk on labor policy ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>housekeeper advocates for changes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category  split                                            title\n",
       "0  Business  train  general electric posts higher rd quarter profit\n",
       "1  Business  train                 lilly to eliminate up to us jobs\n",
       "2  Business  train  s amp p lowers america west outlook to negative\n",
       "3  Business  train        does rand walk the talk on labor policy ?\n",
       "4  Business  train                housekeeper advocates for changes"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_df.to_csv(args.split_data_file, index=False)\n",
    "split_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m-a0OpqhmKJc",
    "mdEditEnable": false
   },
   "source": [
    "### 词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RUMQ_MwumJ8F",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, token_to_idx=None):\n",
    "\n",
    "        # Token to index\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self.token_to_idx = token_to_idx\n",
    "\n",
    "        # Index to token\n",
    "        self.idx_to_token = {idx: token \\\n",
    "                             for token, idx in self.token_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self.token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        return [self.add_token[token] for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        return self.token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        if index not in self.idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self.idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "1LtYf3lpExBb",
    "outputId": "0870e7a9-d843-4549-97ae-d8cf5c3e7e3e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary(size=4)>\n",
      "4\n",
      "0\n",
      "Business\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary instance\n",
    "category_vocab = Vocabulary()\n",
    "for index, row in df.iterrows():\n",
    "    category_vocab.add_token(row.category)\n",
    "print (category_vocab) # __str__\n",
    "print (len(category_vocab)) # __len__\n",
    "index = category_vocab.lookup_token(\"Business\")\n",
    "print (index)\n",
    "print (category_vocab.lookup_index(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QtntaISyE_1c",
    "mdEditEnable": false
   },
   "source": [
    "### 序列词汇表\n",
    "接下来我们将为文章标题创建词汇表类，它由一系列词条构成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ovI8QRefEw_p",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4W3ZouuTEw1_",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self.mask_token = mask_token\n",
    "        self.unk_token = unk_token\n",
    "        self.begin_seq_token = begin_seq_token\n",
    "        self.end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self.mask_token)\n",
    "        self.unk_index = self.add_token(self.unk_token)\n",
    "        self.begin_seq_index = self.add_token(self.begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self.end_seq_token)\n",
    "        \n",
    "        # Index to token\n",
    "        self.idx_to_token = {idx: token \\\n",
    "                             for token, idx in self.token_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self.unk_token,\n",
    "                         'mask_token': self.mask_token,\n",
    "                         'begin_seq_token': self.begin_seq_token,\n",
    "                         'end_seq_token': self.end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        return self.token_to_idx.get(token, self.unk_index)\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        if index not in self.idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the SequenceVocabulary\" % index)\n",
    "        return self.idx_to_token[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"<SequenceVocabulary(size=%d)>\" % len(self.token_to_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "g5UHjpi3El37",
    "outputId": "75875a36-e34f-4e25-aa96-656bdfe4f210",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SequenceVocabulary(size=4400)>\n",
      "4400\n",
      "1092\n",
      "general\n"
     ]
    }
   ],
   "source": [
    "# Get word counts\n",
    "word_counts = Counter()\n",
    "for title in split_df.title:\n",
    "    for token in title.split(\" \"):\n",
    "        if token not in string.punctuation:\n",
    "            word_counts[token] += 1\n",
    "\n",
    "# Create SequenceVocabulary instance\n",
    "title_word_vocab = SequenceVocabulary()\n",
    "for word, word_count in word_counts.items():\n",
    "    if word_count >= args.cutoff:\n",
    "        title_word_vocab.add_token(word)\n",
    "print (title_word_vocab) # __str__\n",
    "print (len(title_word_vocab)) # __len__\n",
    "index = title_word_vocab.lookup_token(\"general\")\n",
    "print (index)\n",
    "print (title_word_vocab.lookup_index(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1_wja0EfQNpA",
    "mdEditEnable": false
   },
   "source": [
    "\n",
    "我们同时会创建SequenceVocabulary实例，它会在字符层级处理输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "5SpfS0BXP9pz",
    "outputId": "383414b5-1274-499a-cd2f-d83cfc17bec6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SequenceVocabulary(size=35)>\n",
      "35\n",
      "4\n",
      "g\n"
     ]
    }
   ],
   "source": [
    "# Create SequenceVocabulary instance\n",
    "title_char_vocab = SequenceVocabulary()\n",
    "for title in split_df.title:\n",
    "    for token in title:\n",
    "        title_char_vocab.add_token(token)\n",
    "print (title_char_vocab) # __str__\n",
    "print (len(title_char_vocab)) # __len__\n",
    "index = title_char_vocab.lookup_token(\"g\")\n",
    "print (index)\n",
    "print (title_char_vocab.lookup_index(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VQIfxcUuKwzz",
    "mdEditEnable": false
   },
   "source": [
    "### 向量化\n",
    "在向量化中这次我们会引入新的操作: 计算输入序列的长度。我们会在为每个输入序列提取最新的相关隐藏状态时用到它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "JtRRXU53El9Y",
    "outputId": "659ad7a1-38a4-46ca-98b8-a72ba0c9fff0",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SequenceVocabulary(size=4404)>\n",
      "<SequenceVocabulary(size=35)>\n",
      "<Vocabulary(size=4)>\n",
      "word_vector: (10,)\n",
      "char_vector: (10, 10)\n",
      "title_length: 10\n",
      "[   2    1 2843 3993 2552    1 3319  410 4236    3]\n",
      "[[ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 7 15  4  5  7  0  0  0  0  0]\n",
      " [21  5 18  5  7  5  7  0  0  0]\n",
      " [26 13  6 16  0  0  0  0  0  0]\n",
      " [12 17  5  0  0  0  0  0  0  0]\n",
      " [26 13 23 25  9  5 18 15  6  0]\n",
      " [12  5  6  6 13 16  0  0  0  0]\n",
      " [12 15 20  7  6  8 23  5  6 12]\n",
      " [30  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "<BEGIN> <UNK> federer wins the <UNK> tennis tournament . <END>\n",
      " roger federer wins the wimbledon tennis tournament .  \n"
     ]
    }
   ],
   "source": [
    "# Vectorizer instance\n",
    "vectorizer = NewsVectorizer.from_dataframe(split_df, cutoff=args.cutoff)\n",
    "print (vectorizer.title_word_vocab)\n",
    "print (vectorizer.title_char_vocab)\n",
    "print (vectorizer.category_vocab)\n",
    "word_vector, char_vector, title_length = vectorizer.vectorize(preprocess_text(\n",
    "    \"Roger Federer wins the Wimbledon tennis tournament.\"))\n",
    "print (\"word_vector:\", np.shape(word_vector))\n",
    "print (\"char_vector:\", np.shape(char_vector))\n",
    "print (\"title_length:\", title_length)\n",
    "print (word_vector)\n",
    "print (char_vector)\n",
    "print (vectorizer.unvectorize_word_vector(word_vector))\n",
    "print (vectorizer.unvectorize_char_vector(char_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uk_QvpVfFM0S",
    "mdEditEnable": false
   },
   "source": [
    "### 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oU7oDdelFMR9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pB7FHmiSFMXA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, df, vectorizer):\n",
    "        self.df = df\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "        # Data splits\n",
    "        self.train_df = self.df[self.df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "        self.val_df = self.df[self.df.split=='val']\n",
    "        self.val_size = len(self.val_df)\n",
    "        self.test_df = self.df[self.df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "        self.lookup_dict = {'train': (self.train_df, self.train_size), \n",
    "                            'val': (self.val_df, self.val_size),\n",
    "                            'test': (self.test_df, self.test_size)}\n",
    "        self.set_split('train')\n",
    "\n",
    "        # Class weights (for imbalances)\n",
    "        class_counts = df.category.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self.vectorizer.category_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, split_data_file, cutoff):\n",
    "        df = pd.read_csv(split_data_file, header=0)\n",
    "        train_df = df[df.split=='train']\n",
    "        return cls(df, NewsVectorizer.from_dataframe(train_df, cutoff))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, split_data_file, vectorizer_filepath):\n",
    "        df = pd.read_csv(split_data_file, header=0)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(df, vectorizer)\n",
    "\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return NewsVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self.vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self.target_split = split\n",
    "        self.target_df, self.target_size = self.lookup_dict[split]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Dataset(split={0}, size={1})\".format(\n",
    "            self.target_split, self.target_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.target_df.iloc[index]\n",
    "        title_word_vector, title_char_vector, title_length = \\\n",
    "            self.vectorizer.vectorize(row.title)\n",
    "        category_index = self.vectorizer.category_vocab.lookup_token(row.category)\n",
    "        return {'title_word_vector': title_word_vector, \n",
    "                'title_char_vector': title_char_vector, \n",
    "                'title_length': title_length, \n",
    "                'category': category_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "\n",
    "    def generate_batches(self, batch_size, collate_fn, shuffle=True, \n",
    "                         drop_last=False, device=\"cpu\"):\n",
    "        dataloader = DataLoader(dataset=self, batch_size=batch_size,\n",
    "                                collate_fn=collate_fn, shuffle=shuffle, \n",
    "                                drop_last=drop_last)\n",
    "        for data_dict in dataloader:\n",
    "            out_data_dict = {}\n",
    "            for name, tensor in data_dict.items():\n",
    "                out_data_dict[name] = data_dict[name].to(device)\n",
    "            yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "_Dpb6ZHJFMeb",
    "outputId": "f87f31eb-c1d1-4269-ea4d-4f93826bd0df",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Dataset(split=train, size=84000)\n",
      "[   2  579    1 1645 2973 2867  522    3]\n",
      "[[ 0  0  0  0  0  0  0  0  0  0]\n",
      " [18  5  9 12  8  0  0  0  0  0]\n",
      " [18 15 18  4  5 16  0  0  0  0]\n",
      " [25  8  6 27  7 20 14 12 11 22]\n",
      " [26 13 12 17  0  0  0  0  0  0]\n",
      " [ 9  8 25 15  7  0  0  0  0  0]\n",
      " [18  5  8  9  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "8\n",
      "0\n",
      "<BEGIN> delta <UNK> bankruptcy with labor deal <END>\n",
      " delta dodges bankruptcy with labor deal  \n",
      "tensor([3.3333e-05, 3.3333e-05, 3.3333e-05, 3.3333e-05])\n"
     ]
    }
   ],
   "source": [
    "# Dataset instance\n",
    "dataset = NewsDataset.load_dataset_and_make_vectorizer(args.split_data_file,\n",
    "                                                       args.cutoff)\n",
    "print (dataset) # __str__\n",
    "input_ = dataset[10] # __getitem__\n",
    "print (input_['title_word_vector'])\n",
    "print (input_['title_char_vector'])\n",
    "print (input_['title_length'])\n",
    "print (input_['category'])\n",
    "print (dataset.vectorizer.unvectorize_word_vector(input_['title_word_vector']))\n",
    "print (dataset.vectorizer.unvectorize_char_vector(input_['title_char_vector']))\n",
    "print (dataset.class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xJV5WlDiFVVz",
    "mdEditEnable": false
   },
   "source": [
    "### 模型\n",
    "embed → encoder → attend → predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rZCzdZZ9FMhm",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c9wipRZt7feC",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NewsEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_word_embeddings, num_char_embeddings,\n",
    "                 kernels, num_input_channels, num_output_channels, \n",
    "                 rnn_hidden_dim, num_layers, bidirectional, \n",
    "                 word_padding_idx=0, char_padding_idx=0):\n",
    "        super(NewsEncoder, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # Embeddings\n",
    "        self.word_embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
    "                                            num_embeddings=num_word_embeddings,\n",
    "                                            padding_idx=word_padding_idx)\n",
    "        self.char_embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
    "                                            num_embeddings=num_char_embeddings,\n",
    "                                            padding_idx=char_padding_idx)\n",
    "        \n",
    "        # Conv weights\n",
    "        self.conv = nn.ModuleList([nn.Conv1d(num_input_channels, \n",
    "                                             num_output_channels, \n",
    "                                             kernel_size=f) for f in kernels])\n",
    "        \n",
    "        \n",
    "        # GRU weights\n",
    "        self.gru = nn.GRU(input_size=embedding_dim*(len(kernels)+1), \n",
    "                          hidden_size=rnn_hidden_dim, num_layers=num_layers, \n",
    "                          batch_first=True, bidirectional=bidirectional)\n",
    "        \n",
    "    def initialize_hidden_state(self, batch_size, rnn_hidden_dim, device):\n",
    "        \"\"\"Modify this to condition the RNN.\"\"\"\n",
    "        num_directions = 1\n",
    "        if self.bidirectional:\n",
    "            num_directions = 2\n",
    "        hidden_t = torch.zeros(self.num_layers * num_directions, \n",
    "                               batch_size, rnn_hidden_dim).to(device)\n",
    "        \n",
    "    def get_char_level_embeddings(self, x):\n",
    "        # x: (N, seq_len, word_len)\n",
    "        input_shape = x.size()\n",
    "        batch_size, seq_len, word_len = input_shape\n",
    "        x = x.view(-1, word_len) # (N*seq_len, word_len)\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.char_embeddings(x) # (N*seq_len, word_len, embedding_dim)\n",
    "        \n",
    "        # Rearrange input so num_input_channels is in dim 1 (N, embedding_dim, word_len)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Convolution\n",
    "        z = [F.relu(conv(x)) for conv in self.conv]\n",
    "        \n",
    "        # Pooling\n",
    "        z = [F.max_pool1d(zz, zz.size(2)).squeeze(2) for zz in z] \n",
    "        z = [zz.view(batch_size, seq_len, -1) for zz in z] # (N, seq_len, embedding_dim)\n",
    "        \n",
    "        # Concat to get char-level embeddings\n",
    "        z = torch.cat(z, 2) # join conv outputs\n",
    "        \n",
    "        return z\n",
    "        \n",
    "    def forward(self, x_word, x_char, x_lengths, device):\n",
    "        \"\"\"\n",
    "        x_word: word level representation (N, seq_size)\n",
    "        x_char: char level representation (N, seq_size, word_len)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Word level embeddings\n",
    "        z_word = self.word_embeddings(x_word)\n",
    "        \n",
    "        # Char level embeddings\n",
    "        z_char = self.get_char_level_embeddings(x=x_char)\n",
    "        \n",
    "        # Concatenate\n",
    "        z = torch.cat([z_word, z_char], 2)\n",
    "        \n",
    "        # Feed into RNN\n",
    "        initial_h = self.initialize_hidden_state(\n",
    "            batch_size=z.size(0), rnn_hidden_dim=self.gru.hidden_size,\n",
    "            device=device)\n",
    "        out, h_n = self.gru(z, initial_h)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zeEcdA287gz4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NewsDecoder(nn.Module):\n",
    "    def __init__(self, rnn_hidden_dim, hidden_dim, output_dim, dropout_p):\n",
    "        super(NewsDecoder, self).__init__()\n",
    "        \n",
    "        # 注意力全连接模型\n",
    "        self.fc_attn = nn.Linear(rnn_hidden_dim, rnn_hidden_dim)\n",
    "        self.v = nn.Parameter(torch.rand(rnn_hidden_dim))\n",
    "        \n",
    "        # 全连接权重\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, encoder_outputs, apply_softmax=False):\n",
    "        \n",
    "        # Attention\n",
    "        z = torch.tanh(self.fc_attn(encoder_outputs))\n",
    "        z = z.transpose(2,1) # [B*H*T]\n",
    "        v = self.v.repeat(encoder_outputs.size(0),1).unsqueeze(1) #[B*1*H]\n",
    "        z = torch.bmm(v,z).squeeze(1) # [B*T]\n",
    "        attn_scores = F.softmax(z, dim=1)\n",
    "        context = torch.matmul(encoder_outputs.transpose(-2, -1), \n",
    "                               attn_scores.unsqueeze(dim=2)).squeeze()\n",
    "        if len(context.size()) == 1:\n",
    "            context = context.unsqueeze(0)\n",
    "        \n",
    "        # 全连接层\n",
    "        z = self.dropout(context)\n",
    "        z = self.fc1(z)\n",
    "        z = self.dropout(z)\n",
    "        y_pred = self.fc2(z)\n",
    "\n",
    "        if apply_softmax:\n",
    "            y_pred = F.softmax(y_pred, dim=1)\n",
    "        return attn_scores, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yVDftS-G7gwy",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NewsModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_word_embeddings, num_char_embeddings,\n",
    "                 kernels, num_input_channels, num_output_channels, \n",
    "                 rnn_hidden_dim, hidden_dim, output_dim, num_layers, \n",
    "                 bidirectional, dropout_p, word_padding_idx, char_padding_idx):\n",
    "        super(NewsModel, self).__init__()\n",
    "        self.encoder = NewsEncoder(embedding_dim, num_word_embeddings,\n",
    "                                   num_char_embeddings, kernels, \n",
    "                                   num_input_channels, num_output_channels, \n",
    "                                   rnn_hidden_dim, num_layers, bidirectional, \n",
    "                                   word_padding_idx, char_padding_idx)\n",
    "        self.decoder = NewsDecoder(rnn_hidden_dim, hidden_dim, output_dim, \n",
    "                                   dropout_p)\n",
    "        \n",
    "    def forward(self, x_word, x_char, x_lengths, device, apply_softmax=False):\n",
    "        encoder_outputs = self.encoder(x_word, x_char, x_lengths, device)\n",
    "        y_pred = self.decoder(encoder_outputs, apply_softmax)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jHPYCPd7Fl3M",
    "mdEditEnable": false
   },
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D3seBMA7FlcC",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HnRKWLekFlnM",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, dataset, model, model_state_file, save_dir, device, \n",
    "                 shuffle, num_epochs, batch_size, learning_rate, \n",
    "                 early_stopping_criteria):\n",
    "        self.dataset = dataset\n",
    "        self.class_weights = dataset.class_weights.to(device)\n",
    "        self.device = device\n",
    "        self.model = model.to(device)\n",
    "        self.save_dir = save_dir\n",
    "        self.device = device\n",
    "        self.shuffle = shuffle\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_func = nn.CrossEntropyLoss(self.class_weights)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer=self.optimizer, mode='min', factor=0.5, patience=1)\n",
    "        self.train_state = {\n",
    "            'stop_early': False, \n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'early_stopping_criteria': early_stopping_criteria,\n",
    "            'learning_rate': learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': model_state_file}\n",
    "    \n",
    "    def update_train_state(self):\n",
    "\n",
    "        # Verbose\n",
    "        print (\"[EPOCH]: {0:02d} | [LR]: {1} | [TRAIN LOSS]: {2:.2f} | [TRAIN ACC]: {3:.1f}% | [VAL LOSS]: {4:.2f} | [VAL ACC]: {5:.1f}%\".format(\n",
    "          self.train_state['epoch_index'], self.train_state['learning_rate'], \n",
    "            self.train_state['train_loss'][-1], self.train_state['train_acc'][-1], \n",
    "            self.train_state['val_loss'][-1], self.train_state['val_acc'][-1]))\n",
    "\n",
    "        # Save one model at least\n",
    "        if self.train_state['epoch_index'] == 0:\n",
    "            torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
    "            self.train_state['stop_early'] = False\n",
    "\n",
    "        # Save model if performance improved\n",
    "        elif self.train_state['epoch_index'] >= 1:\n",
    "            loss_tm1, loss_t = self.train_state['val_loss'][-2:]\n",
    "\n",
    "            # If loss worsened\n",
    "            if loss_t >= self.train_state['early_stopping_best_val']:\n",
    "                # Update step\n",
    "                self.train_state['early_stopping_step'] += 1\n",
    "\n",
    "            # Loss decreased\n",
    "            else:\n",
    "                # Save the best model\n",
    "                if loss_t < self.train_state['early_stopping_best_val']:\n",
    "                    torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
    "\n",
    "                # Reset early stopping step\n",
    "                self.train_state['early_stopping_step'] = 0\n",
    "\n",
    "            # Stop early ?\n",
    "            self.train_state['stop_early'] = self.train_state['early_stopping_step'] \\\n",
    "              >= self.train_state['early_stopping_criteria']\n",
    "        return self.train_state\n",
    "  \n",
    "    def compute_accuracy(self, y_pred, y_target):\n",
    "        _, y_pred_indices = y_pred.max(dim=1)\n",
    "        n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "        return n_correct / len(y_pred_indices) * 100\n",
    "    \n",
    "    def pad_word_seq(self, seq, length):\n",
    "        vector = np.zeros(length, dtype=np.int64)\n",
    "        vector[:len(seq)] = seq\n",
    "        vector[len(seq):] = self.dataset.vectorizer.title_word_vocab.mask_index\n",
    "        return vector\n",
    "    \n",
    "    def pad_char_seq(self, seq, seq_length, word_length):\n",
    "        vector = np.zeros((seq_length, word_length), dtype=np.int64)\n",
    "        vector.fill(self.dataset.vectorizer.title_char_vocab.mask_index)\n",
    "        for i in range(len(seq)):\n",
    "            char_padding = np.zeros(word_length-len(seq[i]), dtype=np.int64)\n",
    "            vector[i] = np.concatenate((seq[i], char_padding), axis=None)\n",
    "        return vector\n",
    "        \n",
    "    def collate_fn(self, batch):\n",
    "        \n",
    "        # Make a deep copy\n",
    "        batch_copy = copy.deepcopy(batch)\n",
    "        processed_batch = {\"title_word_vector\": [], \"title_char_vector\": [], \n",
    "                           \"title_length\": [], \"category\": []}\n",
    "             \n",
    "        # Max lengths\n",
    "        get_seq_length = lambda sample: len(sample[\"title_word_vector\"])\n",
    "        get_word_length = lambda sample: len(sample[\"title_char_vector\"][0])\n",
    "        max_seq_length = max(map(get_seq_length, batch))\n",
    "        max_word_length = max(map(get_word_length, batch))\n",
    "\n",
    "\n",
    "        # Pad\n",
    "        for i, sample in enumerate(batch_copy):\n",
    "            padded_word_seq = self.pad_word_seq(\n",
    "                sample[\"title_word_vector\"], max_seq_length)\n",
    "            padded_char_seq = self.pad_char_seq(\n",
    "                sample[\"title_char_vector\"], max_seq_length, max_word_length)\n",
    "            processed_batch[\"title_word_vector\"].append(padded_word_seq)\n",
    "            processed_batch[\"title_char_vector\"].append(padded_char_seq)\n",
    "            processed_batch[\"title_length\"].append(sample[\"title_length\"])\n",
    "            processed_batch[\"category\"].append(sample[\"category\"])\n",
    "            \n",
    "        # Convert to appropriate tensor types\n",
    "        processed_batch[\"title_word_vector\"] = torch.LongTensor(\n",
    "            processed_batch[\"title_word_vector\"])\n",
    "        processed_batch[\"title_char_vector\"] = torch.LongTensor(\n",
    "            processed_batch[\"title_char_vector\"])\n",
    "        processed_batch[\"title_length\"] = torch.LongTensor(\n",
    "            processed_batch[\"title_length\"])\n",
    "        processed_batch[\"category\"] = torch.LongTensor(\n",
    "            processed_batch[\"category\"])\n",
    "        \n",
    "        return processed_batch  \n",
    "  \n",
    "    def run_train_loop(self):\n",
    "        for epoch_index in range(self.num_epochs):\n",
    "            self.train_state['epoch_index'] = epoch_index\n",
    "      \n",
    "            # Iterate over train dataset\n",
    "\n",
    "            # initialize batch generator, set loss and acc to 0, set train mode on\n",
    "            self.dataset.set_split('train')\n",
    "            batch_generator = self.dataset.generate_batches(\n",
    "                batch_size=self.batch_size, collate_fn=self.collate_fn, \n",
    "                shuffle=self.shuffle, device=self.device)\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            self.model.train()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "                # zero the gradients\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # compute the output\n",
    "                _, y_pred = self.model(x_word=batch_dict['title_word_vector'],\n",
    "                                       x_char=batch_dict['title_char_vector'],\n",
    "                                       x_lengths=batch_dict['title_length'],\n",
    "                                       device=self.device)\n",
    "                \n",
    "                # compute the loss\n",
    "                loss = self.loss_func(y_pred, batch_dict['category'])\n",
    "                loss_t = loss.item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # compute gradients using loss\n",
    "                loss.backward()\n",
    "\n",
    "                # use optimizer to take a gradient step\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # compute the accuracy\n",
    "                acc_t = self.compute_accuracy(y_pred, batch_dict['category'])\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            self.train_state['train_loss'].append(running_loss)\n",
    "            self.train_state['train_acc'].append(running_acc)\n",
    "\n",
    "            # Iterate over val dataset\n",
    "\n",
    "            # initialize batch generator, set loss and acc to 0, set eval mode on\n",
    "            self.dataset.set_split('val')\n",
    "            batch_generator = self.dataset.generate_batches(\n",
    "                batch_size=self.batch_size, collate_fn=self.collate_fn, \n",
    "                shuffle=self.shuffle, device=self.device)\n",
    "            running_loss = 0.\n",
    "            running_acc = 0.\n",
    "            self.model.eval()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "                # compute the output\n",
    "                _, y_pred = self.model(x_word=batch_dict['title_word_vector'],\n",
    "                                       x_char=batch_dict['title_char_vector'],\n",
    "                                       x_lengths=batch_dict['title_length'],\n",
    "                                       device=self.device)\n",
    "\n",
    "                # compute the loss\n",
    "                loss = self.loss_func(y_pred, batch_dict['category'])\n",
    "                loss_t = loss.to(\"cpu\").item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # compute the accuracy\n",
    "                acc_t = self.compute_accuracy(y_pred, batch_dict['category'])\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            self.train_state['val_loss'].append(running_loss)\n",
    "            self.train_state['val_acc'].append(running_acc)\n",
    "\n",
    "            self.train_state = self.update_train_state()\n",
    "            self.scheduler.step(self.train_state['val_loss'][-1])\n",
    "            if self.train_state['stop_early']:\n",
    "                break\n",
    "          \n",
    "    def run_test_loop(self):\n",
    "        # initialize batch generator, set loss and acc to 0, set eval mode on\n",
    "        self.dataset.set_split('test')\n",
    "        batch_generator = self.dataset.generate_batches(\n",
    "            batch_size=self.batch_size, collate_fn=self.collate_fn, \n",
    "            shuffle=self.shuffle, device=self.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        self.model.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # compute the output\n",
    "            _, y_pred = self.model(x_word=batch_dict['title_word_vector'],\n",
    "                                   x_char=batch_dict['title_char_vector'],\n",
    "                                   x_lengths=batch_dict['title_length'],\n",
    "                                   device=self.device)\n",
    "\n",
    "            # compute the loss\n",
    "            loss = self.loss_func(y_pred, batch_dict['category'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = self.compute_accuracy(y_pred, batch_dict['category'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "        self.train_state['test_loss'] = running_loss\n",
    "        self.train_state['test_acc'] = running_acc\n",
    "    \n",
    "    def plot_performance(self):\n",
    "        # Figure size\n",
    "        plt.figure(figsize=(15,5))\n",
    "\n",
    "        # Plot Loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Loss\")\n",
    "        plt.plot(trainer.train_state[\"train_loss\"], label=\"train\")\n",
    "        plt.plot(trainer.train_state[\"val_loss\"], label=\"val\")\n",
    "        plt.legend(loc='upper right')\n",
    "\n",
    "        # Plot Accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.plot(trainer.train_state[\"train_acc\"], label=\"train\")\n",
    "        plt.plot(trainer.train_state[\"val_acc\"], label=\"val\")\n",
    "        plt.legend(loc='lower right')\n",
    "\n",
    "        # Save figure\n",
    "        plt.savefig(os.path.join(self.save_dir, \"performance.png\"))\n",
    "\n",
    "        # Show plots\n",
    "        plt.show()\n",
    "    \n",
    "    def save_train_state(self):\n",
    "        with open(os.path.join(self.save_dir, \"train_state.json\"), \"w\") as fp:\n",
    "            json.dump(self.train_state, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "ICkiOaGtFlk-",
    "outputId": "18174034-ce3e-444a-a968-aba51eb03b3e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.named_modules of NewsModel(\n",
      "  (encoder): NewsEncoder(\n",
      "    (word_embeddings): Embedding(3406, 100, padding_idx=0)\n",
      "    (char_embeddings): Embedding(35, 100, padding_idx=0)\n",
      "    (conv): ModuleList(\n",
      "      (0): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
      "      (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
      "    )\n",
      "    (gru): GRU(300, 128, batch_first=True)\n",
      "  )\n",
      "  (decoder): NewsDecoder(\n",
      "    (fc_attn): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.25)\n",
      "    (fc1): Linear(in_features=128, out_features=200, bias=True)\n",
      "    (fc2): Linear(in_features=200, out_features=4, bias=True)\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# Initialization\n",
    "dataset = NewsDataset.load_dataset_and_make_vectorizer(args.split_data_file,\n",
    "                                                       args.cutoff)\n",
    "dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.vectorizer\n",
    "model = NewsModel(embedding_dim=args.embedding_dim, \n",
    "                  num_word_embeddings=len(vectorizer.title_word_vocab), \n",
    "                  num_char_embeddings=len(vectorizer.title_char_vocab),\n",
    "                  kernels=args.kernels,\n",
    "                  num_input_channels=args.embedding_dim,\n",
    "                  num_output_channels=args.num_filters,\n",
    "                  rnn_hidden_dim=args.rnn_hidden_dim,\n",
    "                  hidden_dim=args.hidden_dim,\n",
    "                  output_dim=len(vectorizer.category_vocab),\n",
    "                  num_layers=args.num_layers,\n",
    "                  bidirectional=args.bidirectional,\n",
    "                  dropout_p=args.dropout_p, \n",
    "                  word_padding_idx=vectorizer.title_word_vocab.mask_index,\n",
    "                  char_padding_idx=vectorizer.title_char_vocab.mask_index)\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "tuaRZ4DiFlh1",
    "outputId": "6496aa05-de58-4913-a56a-9885bd60d9ad",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH]: 00 | [LR]: 0.001 | [TRAIN LOSS]: 0.78 | [TRAIN ACC]: 69.2% | [VAL LOSS]: 0.54 | [VAL ACC]: 80.1%\n",
      "[EPOCH]: 01 | [LR]: 0.001 | [TRAIN LOSS]: 0.50 | [TRAIN ACC]: 82.2% | [VAL LOSS]: 0.50 | [VAL ACC]: 82.0%\n",
      "[EPOCH]: 02 | [LR]: 0.001 | [TRAIN LOSS]: 0.43 | [TRAIN ACC]: 84.6% | [VAL LOSS]: 0.46 | [VAL ACC]: 83.2%\n",
      "[EPOCH]: 03 | [LR]: 0.001 | [TRAIN LOSS]: 0.39 | [TRAIN ACC]: 86.2% | [VAL LOSS]: 0.46 | [VAL ACC]: 83.7%\n",
      "[EPOCH]: 04 | [LR]: 0.001 | [TRAIN LOSS]: 0.35 | [TRAIN ACC]: 87.5% | [VAL LOSS]: 0.44 | [VAL ACC]: 84.3%\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "trainer = Trainer(dataset=dataset, model=model, \n",
    "                  model_state_file=args.model_state_file, \n",
    "                  save_dir=args.save_dir, device=args.device,\n",
    "                  shuffle=args.shuffle, num_epochs=args.num_epochs, \n",
    "                  batch_size=args.batch_size, learning_rate=args.learning_rate, \n",
    "                  early_stopping_criteria=args.early_stopping_criteria)\n",
    "trainer.run_train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "mzRJIz88Flfe",
    "outputId": "dece6240-57ab-4abc-f9cc-ecd11dabcdc6",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/mzRJIz88Flfe/pk8fxyf7ap.png\">"
      ],
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot performance\n",
    "trainer.plot_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4EmFhiX-FMaV",
    "outputId": "29ef6d38-6258-429b-841f-7345b7cd0695",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.44\n",
      "Test Accuracy: 84.6%\n"
     ]
    }
   ],
   "source": [
    "# Test performance\n",
    "trainer.run_test_loop()\n",
    "print(\"Test loss: {0:.2f}\".format(trainer.train_state['test_loss']))\n",
    "print(\"Test Accuracy: {0:.1f}%\".format(trainer.train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zVU1zakYFMVF",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Save all results\n",
    "trainer.save_train_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qLoKfjSpFw7t",
    "mdEditEnable": false
   },
   "source": [
    "### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ANrPcS7Hp_CP",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Inference(object):\n",
    "    def __init__(self, model, vectorizer):\n",
    "        self.model = model\n",
    "        self.vectorizer = vectorizer\n",
    "  \n",
    "    def predict_category(self, title):\n",
    "        # Vectorize\n",
    "        word_vector, char_vector, title_length = self.vectorizer.vectorize(title)\n",
    "        title_word_vector = torch.tensor(word_vector).unsqueeze(0)\n",
    "        title_char_vector = torch.tensor(char_vector).unsqueeze(0)\n",
    "        title_length = torch.tensor([title_length]).long()        \n",
    "        \n",
    "        # Forward pass\n",
    "        self.model.eval()\n",
    "        attn_scores, y_pred = self.model(x_word=title_word_vector, \n",
    "                                         x_char=title_char_vector,\n",
    "                                         x_lengths=title_length, \n",
    "                                         device=\"cpu\",\n",
    "                                         apply_softmax=True)\n",
    "\n",
    "        # Top category\n",
    "        y_prob, indices = y_pred.max(dim=1)\n",
    "        index = indices.item()\n",
    "\n",
    "        # Predicted category\n",
    "        category = vectorizer.category_vocab.lookup_index(index)\n",
    "        probability = y_prob.item()\n",
    "        return {'category': category, 'probability': probability, \n",
    "                'attn_scores': attn_scores}\n",
    "    \n",
    "    def predict_top_k(self, title, k):\n",
    "        # Vectorize\n",
    "        word_vector, char_vector, title_length = self.vectorizer.vectorize(title)\n",
    "        title_word_vector = torch.tensor(word_vector).unsqueeze(0)\n",
    "        title_char_vector = torch.tensor(char_vector).unsqueeze(0)\n",
    "        title_length = torch.tensor([title_length]).long()\n",
    "        \n",
    "         # Forward pass\n",
    "        self.model.eval()\n",
    "        _, y_pred = self.model(x_word=title_word_vector,\n",
    "                               x_char=title_char_vector,\n",
    "                               x_lengths=title_length, \n",
    "                               device=\"cpu\",\n",
    "                               apply_softmax=True)\n",
    "        \n",
    "        # Top k categories\n",
    "        y_prob, indices = torch.topk(y_pred, k=k)\n",
    "        probabilities = y_prob.detach().numpy()[0]\n",
    "        indices = indices.detach().numpy()[0]\n",
    "\n",
    "        # Results\n",
    "        results = []\n",
    "        for probability, index in zip(probabilities, indices):\n",
    "            category = self.vectorizer.category_vocab.lookup_index(index)\n",
    "            results.append({'category': category, 'probability': probability})\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "W6wr68o2p_Eh",
    "outputId": "87886e24-350d-433e-981d-b2907b0c95cf",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.named_modules of NewsModel(\n",
      "  (encoder): NewsEncoder(\n",
      "    (word_embeddings): Embedding(3406, 100, padding_idx=0)\n",
      "    (char_embeddings): Embedding(35, 100, padding_idx=0)\n",
      "    (conv): ModuleList(\n",
      "      (0): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
      "      (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
      "    )\n",
      "    (gru): GRU(300, 128, batch_first=True)\n",
      "  )\n",
      "  (decoder): NewsDecoder(\n",
      "    (fc_attn): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.25)\n",
      "    (fc1): Linear(in_features=128, out_features=200, bias=True)\n",
      "    (fc2): Linear(in_features=200, out_features=4, bias=True)\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "dataset = NewsDataset.load_dataset_and_load_vectorizer(\n",
    "    args.split_data_file, args.vectorizer_file)\n",
    "vectorizer = dataset.vectorizer\n",
    "model = NewsModel(embedding_dim=args.embedding_dim, \n",
    "                  num_word_embeddings=len(vectorizer.title_word_vocab), \n",
    "                  num_char_embeddings=len(vectorizer.title_char_vocab),\n",
    "                  kernels=args.kernels,\n",
    "                  num_input_channels=args.embedding_dim,\n",
    "                  num_output_channels=args.num_filters,\n",
    "                  rnn_hidden_dim=args.rnn_hidden_dim,\n",
    "                  hidden_dim=args.hidden_dim,\n",
    "                  output_dim=len(vectorizer.category_vocab),\n",
    "                  num_layers=args.num_layers,\n",
    "                  bidirectional=args.bidirectional,\n",
    "                  dropout_p=args.dropout_p, \n",
    "                  word_padding_idx=vectorizer.title_word_vocab.mask_index,\n",
    "                  char_padding_idx=vectorizer.title_char_vocab.mask_index)\n",
    "model.load_state_dict(torch.load(args.model_state_file))\n",
    "model = model.to(\"cpu\")\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "JPKgHxsfN954",
    "outputId": "0445e3a7-24a9-4c77-829d-a25681768ab1",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stream",
     "output_type": "stream",
     "text": [
      "Enter a title to classify: : shanghai daily"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shanghai daily → Sports (p=0.51)\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "inference = Inference(model=model, vectorizer=vectorizer)\n",
    "title = input(\"Enter a title to classify: \")\n",
    "prediction = inference.predict_category(preprocess_text(title))\n",
    "print(\"{} → {} (p={:0.2f})\".format(title, prediction['category'], \n",
    "                                   prediction['probability']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "JRdz4wzuQR4N",
    "outputId": "f2c91b24-a36a-4e35-b06a-f6618497d64f",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shanghai daily: \n",
      "Sports (p=0.51)\n",
      "World (p=0.25)\n",
      "Business (p=0.19)\n",
      "Sci/Tech (p=0.04)\n"
     ]
    }
   ],
   "source": [
    "# Top-k inference\n",
    "top_k = inference.predict_top_k(preprocess_text(title), k=len(vectorizer.category_vocab))\n",
    "print (\"{}: \".format(title))\n",
    "for result in top_k:\n",
    "    print (\"{} (p={:0.2f})\".format(result['category'], \n",
    "                                   result['probability']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qrAieHoHxOt2",
    "mdEditEnable": false
   },
   "source": [
    "## 可解释性\n",
    "\n",
    "\n",
    "我们可查看每个时间步生成的概率向量，然后对   前面每一个隐藏状态对某一时间步的预测结果的影响  做可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k6uZY4J8vYgw",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "id": "2PNuY7GLoEi4",
    "outputId": "24b2e48f-da5b-4251-c2eb-81e72603a6f4",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/2PNuY7GLoEi4/pk8jfud727.png\">"
      ],
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "attn_matrix = prediction['attn_scores'].detach().numpy()\n",
    "ax = sns.heatmap(attn_matrix, linewidths=2, square=True)\n",
    "tokens = [\"<BEGIN>\"]+preprocess_text(title).split(\" \")+[\"<END>\"]\n",
    "ax.set_xticklabels(tokens, rotation=45)\n",
    "ax.set_xlabel(\"Token\")\n",
    "ax.set_ylabel(\"Importance\\n\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "594F42ADC95F4DB58117247C59E53206"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
