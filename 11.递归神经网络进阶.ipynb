{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OLIxEDq6VhvZ",
    "mdEditEnable": false
   },
   "source": [
    "## 递归神经网络进阶\n",
    "\n",
    "<img src=\"data/logo.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "这个项目中我们会包含许多RNN的进阶知识内容。\n",
    "\n",
    "1. 条件隐藏状态\n",
    "2. 字符级嵌入\n",
    "3. 编码器和解码器\n",
    "4. 注意机制\n",
    "5. 实践"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "41r7MWJnY0m8",
    "mdEditEnable": false
   },
   "source": [
    "## 配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p0FbOd6IZmzX",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "import collections\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bOsqAo4XZpXQ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set Numpy and PyTorch seeds\n",
    "def set_seeds(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "# Creating directories\n",
    "def create_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QHfvEzQ9ZweF",
    "outputId": "a69944ff-021d-4d04-e920-cfc49112a34c",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "# Arguments\n",
    "args = Namespace(\n",
    "    seed=1234,\n",
    "    cuda=True,\n",
    "    batch_size=4,\n",
    "    condition_vocab_size=3, # vocabulary for condition possibilities\n",
    "    embedding_dim=100,\n",
    "    rnn_hidden_dim=100,\n",
    "    hidden_dim=100,\n",
    "    num_layers=1,\n",
    "    bidirectional=False,\n",
    ")\n",
    "\n",
    "# Set seeds\n",
    "set_seeds(seed=args.seed, cuda=args.cuda)\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pKlb9SjfpbED",
    "mdEditEnable": false
   },
   "source": [
    "# 条件递归神经网络\n",
    "给RNN加上条件判断实际上是在为正确的预测添加附加信息。我们可以将这些信息编码(嵌入)然后将它和序列输入数据一起穿进模型。依旧举上个项目中文本分类的例子，假设每篇文章的出版商(NYTimes， ESPN 等)已知。我们就可以用这个信息来帮助预测(比如ESPN发行的新闻更可能是体育新闻)。\n",
    "**注意**: 如果条件信息对序列中每个输入都不一样，那只需将它和每个时间步的数据联系起来就好了\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B419D2C2228C4E738A38BED02C80E68B",
    "mdEditEnable": false
   },
   "source": [
    "\n",
    "**第一步**： 把初始隐藏状态替换为编码后的条件信息，保证它的大小和RNN的隐藏状态大小一样\n",
    "\n",
    "<img src=\"data/conditioned_rnn1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbrlQHx2x8Aa",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cFoiV-fqmvRo",
    "outputId": "9843f756-8d71-4686-b479-b521df9b6f3c",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 100])\n"
     ]
    }
   ],
   "source": [
    "# 条件\n",
    "condition = torch.LongTensor([0, 2, 1, 2]) # 批大小4， 词汇表大小3\n",
    "condition_embeddings = nn.Embedding(\n",
    "    embedding_dim=args.embedding_dim, # 应该和RNN隐藏维一样\n",
    "    num_embeddings=args.condition_vocab_size) # 条件各异\n",
    "\n",
    "# 初始化隐藏状态\n",
    "num_directions = 1\n",
    "if args.bidirectional:\n",
    "    num_directions = 2\n",
    "    \n",
    "# If using multiple layers and directions, the hidden state needs to match that size\n",
    "# 如果使用了多层或者多向网络， 隐藏状态需要匹配大小\n",
    "hidden_t = condition_embeddings(condition).unsqueeze(0).repeat(\n",
    "    args.num_layers * num_directions, 1, 1).to(args.device) # initial state to RNN\n",
    "print (hidden_t.size())\n",
    "\n",
    "# 传入rnn\n",
    "# y_out, _ = self.rnn(x_embedded, hidden_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yUIg5o-dpiZF",
    "mdEditEnable": false
   },
   "source": [
    "\n",
    "**第二步**: 将编码过的信息和每一时间步的隐藏状态关联，注意这里**千万不要**替换隐藏状态，RNN需要它来进行训练\n",
    "\n",
    "<img src=\"data/conditioned_rnn2.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eQ-h28o-pi4X",
    "outputId": "4143190d-c452-48cc-cc96-1a2f0f7fc5ee",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 100])\n"
     ]
    }
   ],
   "source": [
    "# 初始化隐藏状态\n",
    "hidden_t = torch.zeros((args.num_layers * num_directions, args.batch_size, args.rnn_hidden_dim))\n",
    "print (hidden_t.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Z6hYSIdqBQ4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def concat_condition(condition_embeddings, condition, hidden_t, num_layers, num_directions):\n",
    "    condition_t = condition_embeddings(condition).unsqueeze(0).repeat(\n",
    "        num_layers * num_directions, 1, 1)\n",
    "    hidden_t = torch.cat([hidden_t, condition_t], 2)\n",
    "    return hidden_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Tjyzq_s5pixL",
    "outputId": "f4f62742-044e-46ef-cc46-fc21a3c52c78",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 200])\n"
     ]
    }
   ],
   "source": [
    "# 循环输入\n",
    "hiddens = []\n",
    "seq_size = 1\n",
    "for t in range(seq_size):\n",
    "    hidden_t = concat_condition(condition_embeddings, condition, hidden_t, \n",
    "                                args.num_layers, num_directions).to(args.device)\n",
    "    print (hidden_t.size())\n",
    "    \n",
    "    # 传入rnn\n",
    "    # hidden_t = rnn_cell(x_in[t], hidden_t)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-SZgVuwebm_4",
    "mdEditEnable": false
   },
   "source": [
    "## 字符级嵌入\n",
    "\n",
    "卷积操作可能会有句子中包含用字符表示的单词 |  ~$\\in \\mathbb{R}^{NXSXWXE}~$ 而输出是每个词的嵌入(根据具体的卷积操作)。\n",
    "\n",
    "**词嵌入**: 找到相邻token中的暂时关系，这些暂时关系暗示了某些词语代表了相似的意义。比如 \"New Jersey\" 和 \"NJ\" 和 \"Garden State\" 有相似意思，他们都指新泽信这个地方。\n",
    "\n",
    "**字符嵌入**: 创建词语在字符级别的关联。比如 \"toy\" 和 \"toys\" 意思相近。\n",
    "\n",
    "<img src=\"data/char_embeddings.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QOdIvz0G3O8C",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 参数\n",
    "args = Namespace(\n",
    "    seed=1234,\n",
    "    cuda=False,\n",
    "    shuffle=True,\n",
    "    batch_size=64,\n",
    "    vocab_size=20, # 词汇表\n",
    "    seq_size=10, # 句子的最长长度\n",
    "    word_size=15, # 词语的最长长度\n",
    "    embedding_dim=100,\n",
    "    num_filters=100, # 过滤器大小\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "raztXIeYXYJT",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_embeddings, num_input_channels, \n",
    "                 num_output_channels, padding_idx):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # 字符嵌入\n",
    "        self.embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
    "                                       num_embeddings=num_embeddings,\n",
    "                                       padding_idx=padding_idx)\n",
    "        \n",
    "        # 卷积权重\n",
    "        self.conv = nn.ModuleList([nn.Conv1d(num_input_channels, num_output_channels, \n",
    "                                             kernel_size=f) for f in [2,3,4]])\n",
    "\n",
    "    def forward(self, x, channel_first=False, apply_softmax=False):\n",
    "        \n",
    "        # x: (N, seq_len, word_len)\n",
    "        input_shape = x.size()\n",
    "        batch_size, seq_len, word_len = input_shape\n",
    "        x = x.view(-1, word_len) # (N*seq_len, word_len)\n",
    "        \n",
    "        # 嵌入\n",
    "        x = self.embeddings(x) # (N*seq_len, word_len, embedding_dim)\n",
    "        \n",
    "        # 重新排布输入各维位置 (N, embedding_dim, word_len)\n",
    "        if not channel_first:\n",
    "            x = x.transpose(1, 2)\n",
    "        \n",
    "        # 卷积\n",
    "        z = [F.relu(conv(x)) for conv in self.conv]\n",
    "        \n",
    "        # 池化\n",
    "        z = [F.max_pool1d(zz, zz.size(2)).squeeze(2) for zz in z] \n",
    "        z = [zz.view(batch_size, seq_len, -1) for zz in z] # (N, seq_len, embedding_dim)\n",
    "        \n",
    "        # 拼接或者字符级嵌入\n",
    "        z = torch.cat(z, 2) \n",
    "        \n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MzHVs8Xe0Zph",
    "outputId": "ff91c1ac-5bc4-446c-9047-8b4b58570e13",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10, 15])\n"
     ]
    }
   ],
   "source": [
    "# 输入\n",
    "input_size = (args.batch_size, args.seq_size, args.word_size)\n",
    "x_in = torch.randint(low=0, high=args.vocab_size, size=input_size).long()\n",
    "print (x_in.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "0B_Xscby2PMQ",
    "outputId": "05b0c3ac-429f-47aa-9526-718e55dfc897",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.named_modules of Model(\n",
      "  (embeddings): Embedding(20, 100, padding_idx=0)\n",
      "  (conv): ModuleList(\n",
      "    (0): Conv1d(100, 100, kernel_size=(2,), stride=(1,))\n",
      "    (1): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
      "    (2): Conv1d(100, 100, kernel_size=(4,), stride=(1,))\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# 初始化字符级嵌入\n",
    "model = Model(embedding_dim=args.embedding_dim, \n",
    "              num_embeddings=args.vocab_size, \n",
    "              num_input_channels=args.embedding_dim, \n",
    "              num_output_channels=args.num_filters,\n",
    "              padding_idx=0)\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8DIgeEZFXYR2",
    "outputId": "ffdbfabf-5f60-4045-be84-23dfb65fd424",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10, 300])\n"
     ]
    }
   ],
   "source": [
    "# 向前传播得到字符级嵌入\n",
    "z = model(x_in)\n",
    "print (z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nzTscaE10HFA",
    "mdEditEnable": false
   },
   "source": [
    "字符嵌入有以下几种用法:\n",
    "\n",
    "1. 将字符嵌入和词嵌入拼接，然后传入RNN\n",
    "2. 直接传入RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_sixbu74kbJk",
    "mdEditEnable": false
   },
   "source": [
    "## 编码器和解码器\n",
    "\n",
    "到现在为止，我们都在使用RNN来对序列化输入进行 `编码` 得到隐藏状态。然后使用这些隐藏状态去对预测值 `解码`。我们使用的编码器就是RNN而解码器只是几层全连接层接着一层softmax(用于分类)。但其实编码器和解码器都可以有其他的结构。比如解码器也可以是一个RNN。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kfK1mAp1dlpT",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 参数\n",
    "args = Namespace(\n",
    "    batch_size=64,\n",
    "    embedding_dim=100,\n",
    "    rnn_hidden_dim=100,\n",
    "    hidden_dim=100,\n",
    "    num_layers=1,\n",
    "    bidirectional=False,\n",
    "    dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p_OJFyY97bF_",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_embeddings, rnn_hidden_dim, \n",
    "                 num_layers, bidirectional, padding_idx=0):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # 嵌入\n",
    "        self.word_embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
    "                                            num_embeddings=num_embeddings,\n",
    "                                            padding_idx=padding_idx)\n",
    "        \n",
    "        # GRU权重\n",
    "        self.gru = nn.GRU(input_size=embedding_dim, hidden_size=rnn_hidden_dim, \n",
    "                          num_layers=num_layers, batch_first=True, \n",
    "                          bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, x_in, x_lengths):\n",
    "        \n",
    "        # 词级嵌入\n",
    "        z_word = self.word_embeddings(x_in)\n",
    "   \n",
    "        # 传入RNN\n",
    "        out, h_n = self.gru(z)\n",
    "        \n",
    "        # 获取最新的相关隐藏状态\n",
    "        out = gather_last_relevant_hidden(out, x_lengths)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HRXtaGPlpyH7",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, rnn_hidden_dim, hidden_dim, output_dim, dropout_p):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # 全连接权重\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, encoder_output, apply_softmax=False):\n",
    "        \n",
    "        # 全连接层\n",
    "        z = self.dropout(encoder_output)\n",
    "        z = self.fc1(z)\n",
    "        z = self.dropout(z)\n",
    "        y_pred = self.fc2(z)\n",
    "\n",
    "        if apply_softmax:\n",
    "            y_pred = F.softmax(y_pred, dim=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SnKyCPVj-OVi",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_embeddings, rnn_hidden_dim, \n",
    "                 hidden_dim, num_layers, bidirectional, output_dim, dropout_p, \n",
    "                 padding_idx=0):\n",
    "        super(Model, self).__init__()\n",
    "        self.encoder = Encoder(embedding_dim, num_embeddings, rnn_hidden_dim, \n",
    "                               num_layers, bidirectional, padding_idx=0)\n",
    "        self.decoder = Decoder(rnn_hidden_dim, hidden_dim, output_dim, dropout_p)\n",
    "        \n",
    "    def forward(self, x_in, x_lengths, apply_softmax=False):\n",
    "        encoder_outputs = self.encoder(x_in, x_lengths)\n",
    "        y_pred = self.decoder(encoder_outputs, apply_softmax)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "hfeoErsc-Tum",
    "outputId": "8faa37ab-4c38-4ace-bb96-e5dc7e1483bf",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.named_parameters of Model(\n",
      "  (encoder): Encoder(\n",
      "    (word_embeddings): Embedding(1000, 100, padding_idx=0)\n",
      "    (gru): GRU(100, 100, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (dropout): Dropout(p=0.1)\n",
      "    (fc1): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "model = Model(embedding_dim=args.embedding_dim, num_embeddings=1000, \n",
    "              rnn_hidden_dim=args.rnn_hidden_dim, hidden_dim=args.hidden_dim, \n",
    "              num_layers=args.num_layers, bidirectional=args.bidirectional, \n",
    "              output_dim=4, dropout_p=args.dropout, padding_idx=0)\n",
    "print (model.named_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o_jPXuT8xlqd",
    "mdEditEnable": false
   },
   "source": [
    "## 注意力机制 (Attentional mechanisms)\n",
    "\n",
    "我们知道RNN在处理输入序列的时候，在每个时间步被处理的是那一时刻的输入和隐藏状态。在大部分情况下，如果可以访问所有输入然后在每一个时间步只关注某一些被选择出的值时比较优秀的实践。比如在机器翻译时，最好可以访问被翻译内容中的所有值，因为在翻译的过程中，结果和原句并不是严格词与词一一对应的。\n",
    "\n",
    "\n",
    "<img src=\"data/attention1.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "注意力机制听起来可能令人疑惑，所以我们来看看每一个时间步都发生了什么。\n",
    "在时间步 j, 模型已经处理了 ~$x_0, x_1, x_2, ..., x_j~$, 并且生成了隐藏状态 ~$h_0, h_1, h_2, ..., h_j~$。 主要思想其实是，在做预测时用到所有已被处理的隐藏状态，而不是最近的那一个。有几种方法可以达成这样的效果。\n",
    "\n",
    "**软注意 soft attention** 是学习并获得一个浮点数向量(概率)，然后和隐藏状态相乘得到 表示上下文的向量(context vector)\n",
    "\n",
    "\n",
    "比如. [0.1, 0.3, 0.1, 0.4, 0.1]\n",
    "\n",
    "\n",
    "**硬注意 hard attention** 是学习并获得一个进制向量, 然后和隐藏状态相乘得到 表示上下文的向量(context vector)\n",
    "比如. [0, 0, 0, 1, 0]\n",
    "\n",
    "这里我们会主要介绍 soft attention 因为它的使用其实更加广泛，并且对  每个隐藏状态对最终预测的影响  作简单的可视化。也就是它的可解释性更好\n",
    "\n",
    "<img src=\"data/attention2.jpg\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "我们将在下面的文本分类任务中使用注意力机制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n38ZJoVZnGaE",
    "mdEditEnable": false
   },
   "source": [
    "## 使用RNN进行文本分类\n",
    "\n",
    "我们会完成和前个项目一样的文本分类任务，但是会使用注意力机制来解读模型。\n",
    "\n",
    "**为什么不用机器翻译模型(machine translation)?** 一般来说在解释注意力机制时大家都会直接拿机器翻译模型做样例，但其实这样做并不是很实际。你并不能经常看到用一个序列来生成另一个序列的实际情景。所以我们会使用文本分类的例子来解读那些输入词条对分类有更强的影响力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fu7HgEqbnGFY",
    "mdEditEnable": false
   },
   "source": [
    "### Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "elL6BxtCmNGf",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import collections\n",
    "import copy\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DCf2fLmPbKKI",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def set_seeds(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "# Creating directories\n",
    "def create_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TTwkuoZdmMlF",
    "outputId": "291c03d4-6143-4395-b5c9-ab386b061737",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    seed=1234,\n",
    "    cuda=True,\n",
    "    shuffle=True,\n",
    "    data_file=\"data/news.csv\",\n",
    "    split_data_file=\"data/split_news.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"news\",\n",
    "    train_size=0.7,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    pretrained_embeddings=None,\n",
    "    cutoff=25,\n",
    "    num_epochs=5,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=128,\n",
    "    embedding_dim=100,\n",
    "    kernels=[3,5],\n",
    "    num_filters=100,\n",
    "    rnn_hidden_dim=128,\n",
    "    hidden_dim=200,\n",
    "    num_layers=1,\n",
    "    bidirectional=False,\n",
    "    dropout_p=0.25,\n",
    ")\n",
    "\n",
    "# Set seeds\n",
    "set_seeds(seed=args.seed, cuda=args.cuda)\n",
    "\n",
    "# Create save dir\n",
    "create_dirs(args.save_dir)\n",
    "\n",
    "# Expand filepaths\n",
    "args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xfiWhgX5mMQ5",
    "mdEditEnable": false
   },
   "source": [
    "### 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "baAsxXNFmMCF",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "wrI_df4bmLjB",
    "outputId": "a51463a7-f37e-41e7-aca4-74038c7c6e8e",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                              title\n",
       "0  Business  Wall St. Bears Claw Back Into the Black (Reuters)\n",
       "1  Business  Carlyle Looks Toward Commercial Aerospace (Reu...\n",
       "2  Business    Oil and Economy Cloud Stocks' Outlook (Reuters)\n",
       "3  Business  Iraq Halts Oil Exports from Main Southern Pipe...\n",
       "4  Business  Oil prices soar to all-time record, posing new..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(args.data_file, header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "TreK7nqEmLTN",
    "outputId": "36145f0d-7316-4341-f270-1d8c8037c661",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business: 30000\n",
      "Sci/Tech: 30000\n",
      "Sports: 30000\n",
      "World: 30000\n"
     ]
    }
   ],
   "source": [
    "by_category = collections.defaultdict(list)\n",
    "for _, row in df.iterrows():\n",
    "    by_category[row.category].append(row.to_dict())\n",
    "for category in by_category:\n",
    "    print (\"{0}: {1}\".format(category, len(by_category[category])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "35nb3LxLmLCA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_list = []\n",
    "for _, item_list in sorted(by_category.items()):\n",
    "    if args.shuffle:\n",
    "        np.random.shuffle(item_list)\n",
    "    n = len(item_list)\n",
    "    n_train = int(args.train_size*n)\n",
    "    n_val = int(args.val_size*n)\n",
    "    n_test = int(args.test_size*n)\n",
    "\n",
    "  # Give data point a split attribute\n",
    "    for item in item_list[:n_train]:\n",
    "        item['split'] = 'train'\n",
    "    for item in item_list[n_train:n_train+n_val]:\n",
    "        item['split'] = 'val'\n",
    "    for item in item_list[n_train+n_val:]:\n",
    "        item['split'] = 'test'  \n",
    "\n",
    "    # Add to final list\n",
    "    final_list.extend(item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "Y48IvuSfmK07",
    "outputId": "3b188412-5c0a-4e71-ef50-20c4ba18082b",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    84000\n",
       "val      18000\n",
       "test     18000\n",
       "Name: split, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_df = pd.DataFrame(final_list)\n",
    "split_df[\"split\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RWuNBxAXmKk2",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "    \n",
    "split_df.title = split_df.title.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "fG9n77eLmKWB",
    "outputId": "7bb68022-5848-44ac-f90c-7cdf6a7eb988",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>split</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>general electric posts higher rd quarter profit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>lilly to eliminate up to us jobs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>s amp p lowers america west outlook to negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>does rand walk the talk on labor policy ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>train</td>\n",
       "      <td>housekeeper advocates for changes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category  split                                            title\n",
       "0  Business  train  general electric posts higher rd quarter profit\n",
       "1  Business  train                 lilly to eliminate up to us jobs\n",
       "2  Business  train  s amp p lowers america west outlook to negative\n",
       "3  Business  train        does rand walk the talk on labor policy ?\n",
       "4  Business  train                housekeeper advocates for changes"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_df.to_csv(args.split_data_file, index=False)\n",
    "split_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m-a0OpqhmKJc",
    "mdEditEnable": false
   },
   "source": [
    "### 词汇表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RUMQ_MwumJ8F",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, token_to_idx=None):\n",
    "\n",
    "        # Token to index\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self.token_to_idx = token_to_idx\n",
    "\n",
    "        # Index to token\n",
    "        self.idx_to_token = {idx: token \\\n",
    "                             for token, idx in self.token_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self.token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        return [self.add_token[token] for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        return self.token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        if index not in self.idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self.idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "1LtYf3lpExBb",
    "outputId": "0870e7a9-d843-4549-97ae-d8cf5c3e7e3e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary(size=4)>\n",
      "4\n",
      "0\n",
      "Business\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary instance\n",
    "category_vocab = Vocabulary()\n",
    "for index, row in df.iterrows():\n",
    "    category_vocab.add_token(row.category)\n",
    "print (category_vocab) # __str__\n",
    "print (len(category_vocab)) # __len__\n",
    "index = category_vocab.lookup_token(\"Business\")\n",
    "print (index)\n",
    "print (category_vocab.lookup_index(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QtntaISyE_1c",
    "mdEditEnable": false
   },
   "source": [
    "### 序列词汇表\n",
    "接下来我们将为文章标题创建词汇表类，它由一系列词条构成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ovI8QRefEw_p",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4W3ZouuTEw1_",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    "\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "\n",
    "        self.mask_token = mask_token\n",
    "        self.unk_token = unk_token\n",
    "        self.begin_seq_token = begin_seq_token\n",
    "        self.end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self.mask_token)\n",
    "        self.unk_index = self.add_token(self.unk_token)\n",
    "        self.begin_seq_index = self.add_token(self.begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self.end_seq_token)\n",
    "        \n",
    "        # Index to token\n",
    "        self.idx_to_token = {idx: token \\\n",
    "                             for token, idx in self.token_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update({'unk_token': self.unk_token,\n",
    "                         'mask_token': self.mask_token,\n",
    "                         'begin_seq_token': self.begin_seq_token,\n",
    "                         'end_seq_token': self.end_seq_token})\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        return self.token_to_idx.get(token, self.unk_index)\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        if index not in self.idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the SequenceVocabulary\" % index)\n",
    "        return self.idx_to_token[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"<SequenceVocabulary(size=%d)>\" % len(self.token_to_idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "g5UHjpi3El37",
    "outputId": "75875a36-e34f-4e25-aa96-656bdfe4f210",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SequenceVocabulary(size=4400)>\n",
      "4400\n",
      "4\n",
      "general\n"
     ]
    }
   ],
   "source": [
    "# Get word counts\n",
    "word_counts = Counter()\n",
    "for title in split_df.title:\n",
    "    for token in title.split(\" \"):\n",
    "        if token not in string.punctuation:\n",
    "            word_counts[token] += 1\n",
    "\n",
    "# Create SequenceVocabulary instance\n",
    "title_word_vocab = SequenceVocabulary()\n",
    "for word, word_count in word_counts.items():\n",
    "    if word_count >= args.cutoff:\n",
    "        title_word_vocab.add_token(word)\n",
    "print (title_word_vocab) # __str__\n",
    "print (len(title_word_vocab)) # __len__\n",
    "index = title_word_vocab.lookup_token(\"general\")\n",
    "print (index)\n",
    "print (title_word_vocab.lookup_index(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1_wja0EfQNpA",
    "mdEditEnable": false
   },
   "source": [
    "\n",
    "我们同时会创建SequenceVocabulary实例，它会在字符层级处理输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "5SpfS0BXP9pz",
    "outputId": "383414b5-1274-499a-cd2f-d83cfc17bec6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SequenceVocabulary(size=35)>\n",
      "35\n",
      "4\n",
      "g\n"
     ]
    }
   ],
   "source": [
    "# Create SequenceVocabulary instance\n",
    "title_char_vocab = SequenceVocabulary()\n",
    "for title in split_df.title:\n",
    "    for token in title:\n",
    "        title_char_vocab.add_token(token)\n",
    "print (title_char_vocab) # __str__\n",
    "print (len(title_char_vocab)) # __len__\n",
    "index = title_char_vocab.lookup_token(\"g\")\n",
    "print (index)\n",
    "print (title_char_vocab.lookup_index(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VQIfxcUuKwzz",
    "mdEditEnable": false
   },
   "source": [
    "### 向量化\n",
    "在向量化中这次我们会引入新的操作: 计算输入序列的长度。我们会在为每个输入序列提取最新的相关隐藏状态时用到它"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsVectorizer(object):\n",
    "    def __init__(self, title_word_vocab, title_char_vocab, category_vocab):\n",
    "        self.title_word_vocab = title_word_vocab\n",
    "        self.title_char_vocab = title_char_vocab\n",
    "        self.category_vocab = category_vocab\n",
    "\n",
    "    def vectorize(self, title):\n",
    "       \n",
    "        # Word-level vectorization\n",
    "        word_indices = [self.title_word_vocab.lookup_token(token) for token in title.split(\" \")]\n",
    "        word_indices = [self.title_word_vocab.begin_seq_index] + word_indices + \\\n",
    "            [self.title_word_vocab.end_seq_index]\n",
    "        title_length = len(word_indices)\n",
    "        word_vector = np.zeros(title_length, dtype=np.int64)\n",
    "        word_vector[:len(word_indices)] = word_indices\n",
    "        \n",
    "        # Char-level vectorization\n",
    "        word_length = max([len(word) for word in title.split(\" \")])\n",
    "        char_vector = np.zeros((len(word_vector), word_length), dtype=np.int64)\n",
    "        char_vector[0, :] = self.title_word_vocab.mask_index # <BEGIN>\n",
    "        char_vector[-1, :] = self.title_word_vocab.mask_index # <END>\n",
    "        for i, word in enumerate(title.split(\" \")):\n",
    "            char_vector[i+1,:len(word)] = [title_char_vocab.lookup_token(char) \\\n",
    "                                           for char in word] # i+1 b/c of <BEGIN> token\n",
    "                \n",
    "        return word_vector, char_vector, len(word_indices)\n",
    "    \n",
    "    def unvectorize_word_vector(self, word_vector):\n",
    "        tokens = [self.title_word_vocab.lookup_index(index) for index in word_vector]\n",
    "        title = \" \".join(token for token in tokens)\n",
    "        return title\n",
    "    \n",
    "    def unvectorize_char_vector(self, char_vector):\n",
    "        title = \"\"\n",
    "        for word_vector in char_vector:\n",
    "            for index in word_vector:\n",
    "                if index == self.title_char_vocab.mask_index:\n",
    "                    break\n",
    "                title += self.title_char_vocab.lookup_index(index)\n",
    "            title += \" \"\n",
    "        return title\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df, cutoff):\n",
    "        \n",
    "        # Create class vocab\n",
    "        category_vocab = Vocabulary()        \n",
    "        for category in sorted(set(df.category)):\n",
    "            category_vocab.add_token(category)\n",
    "\n",
    "        # Get word counts\n",
    "        word_counts = Counter()\n",
    "        for title in df.title:\n",
    "            for token in title.split(\" \"):\n",
    "                word_counts[token] += 1\n",
    "        \n",
    "        # Create title vocab (word level)\n",
    "        title_word_vocab = SequenceVocabulary()\n",
    "        for word, word_count in word_counts.items():\n",
    "            if word_count >= cutoff:\n",
    "                title_word_vocab.add_token(word)\n",
    "                \n",
    "        # Create title vocab (char level)\n",
    "        title_char_vocab = SequenceVocabulary()\n",
    "        for title in df.title:\n",
    "            for token in title:\n",
    "                title_char_vocab.add_token(token)\n",
    "        \n",
    "        return cls(title_word_vocab, title_char_vocab, category_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        title_word_vocab = SequenceVocabulary.from_serializable(contents['title_word_vocab'])\n",
    "        title_char_vocab = SequenceVocabulary.from_serializable(contents['title_char_vocab'])\n",
    "        category_vocab = Vocabulary.from_serializable(contents['category_vocab'])\n",
    "        return cls(title_word_vocab=title_word_vocab, \n",
    "                   title_char_vocab=title_char_vocab, \n",
    "                   category_vocab=category_vocab)\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {'title_word_vocab': self.title_word_vocab.to_serializable(),\n",
    "                'title_char_vocab': self.title_char_vocab.to_serializable(),\n",
    "                'category_vocab': self.category_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "JtRRXU53El9Y",
    "outputId": "659ad7a1-38a4-46ca-98b8-a72ba0c9fff0",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SequenceVocabulary(size=4404)>\n",
      "<SequenceVocabulary(size=35)>\n",
      "<Vocabulary(size=4)>\n",
      "word_vector: (10,)\n",
      "char_vector: (10, 10)\n",
      "title_length: 10\n",
      "[   2    1 4151 1231   25    1 2392 4076   38    3]\n",
      "[[ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 7 15  4  5  7  0  0  0  0  0]\n",
      " [21  5 18  5  7  5  7  0  0  0]\n",
      " [26 13  6 16  0  0  0  0  0  0]\n",
      " [12 17  5  0  0  0  0  0  0  0]\n",
      " [26 13 23 25  9  5 18 15  6  0]\n",
      " [12  5  6  6 13 16  0  0  0  0]\n",
      " [12 15 20  7  6  8 23  5  6 12]\n",
      " [30  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "<BEGIN> <UNK> federer wins the <UNK> tennis tournament . <END>\n",
      " roger federer wins the wimbledon tennis tournament .  \n"
     ]
    }
   ],
   "source": [
    "# Vectorizer instance\n",
    "vectorizer = NewsVectorizer.from_dataframe(split_df, cutoff=args.cutoff)\n",
    "print (vectorizer.title_word_vocab)\n",
    "print (vectorizer.title_char_vocab)\n",
    "print (vectorizer.category_vocab)\n",
    "word_vector, char_vector, title_length = vectorizer.vectorize(preprocess_text(\n",
    "    \"Roger Federer wins the Wimbledon tennis tournament.\"))\n",
    "print (\"word_vector:\", np.shape(word_vector))\n",
    "print (\"char_vector:\", np.shape(char_vector))\n",
    "print (\"title_length:\", title_length)\n",
    "print (word_vector)\n",
    "print (char_vector)\n",
    "print (vectorizer.unvectorize_word_vector(word_vector))\n",
    "print (vectorizer.unvectorize_char_vector(char_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uk_QvpVfFM0S",
    "mdEditEnable": false
   },
   "source": [
    "### 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oU7oDdelFMR9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pB7FHmiSFMXA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, df, vectorizer):\n",
    "        self.df = df\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "        # Data splits\n",
    "        self.train_df = self.df[self.df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "        self.val_df = self.df[self.df.split=='val']\n",
    "        self.val_size = len(self.val_df)\n",
    "        self.test_df = self.df[self.df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "        self.lookup_dict = {'train': (self.train_df, self.train_size), \n",
    "                            'val': (self.val_df, self.val_size),\n",
    "                            'test': (self.test_df, self.test_size)}\n",
    "        self.set_split('train')\n",
    "\n",
    "        # Class weights (for imbalances)\n",
    "        class_counts = df.category.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self.vectorizer.category_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, split_data_file, cutoff):\n",
    "        df = pd.read_csv(split_data_file, header=0)\n",
    "        train_df = df[df.split=='train']\n",
    "        return cls(df, NewsVectorizer.from_dataframe(train_df, cutoff))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, split_data_file, vectorizer_filepath):\n",
    "        df = pd.read_csv(split_data_file, header=0)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(df, vectorizer)\n",
    "\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return NewsVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self.vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self.target_split = split\n",
    "        self.target_df, self.target_size = self.lookup_dict[split]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Dataset(split={0}, size={1})\".format(\n",
    "            self.target_split, self.target_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.target_df.iloc[index]\n",
    "        title_word_vector, title_char_vector, title_length = \\\n",
    "            self.vectorizer.vectorize(row.title)\n",
    "        category_index = self.vectorizer.category_vocab.lookup_token(row.category)\n",
    "        return {'title_word_vector': title_word_vector, \n",
    "                'title_char_vector': title_char_vector, \n",
    "                'title_length': title_length, \n",
    "                'category': category_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "\n",
    "    def generate_batches(self, batch_size, collate_fn, shuffle=True, \n",
    "                         drop_last=False, device=\"cpu\"):\n",
    "        dataloader = DataLoader(dataset=self, batch_size=batch_size,\n",
    "                                collate_fn=collate_fn, shuffle=shuffle, \n",
    "                                drop_last=drop_last)\n",
    "        for data_dict in dataloader:\n",
    "            out_data_dict = {}\n",
    "            for name, tensor in data_dict.items():\n",
    "                out_data_dict[name] = data_dict[name].to(device)\n",
    "            yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "_Dpb6ZHJFMeb",
    "outputId": "f87f31eb-c1d1-4269-ea4d-4f93826bd0df",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Dataset(split=train, size=84000)\n",
      "[ 2 51  1 52 53 26 54  3]\n",
      "[[ 0  0  0  0  0  0  0  0  0  0]\n",
      " [18  5  9 12  8  0  0  0  0  0]\n",
      " [18 15 18  4  5 16  0  0  0  0]\n",
      " [25  8  6 27  7 20 14 12 11 22]\n",
      " [26 13 12 17  0  0  0  0  0  0]\n",
      " [ 9  8 25 15  7  0  0  0  0  0]\n",
      " [18  5  8  9  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "8\n",
      "0\n",
      "<BEGIN> delta <UNK> bankruptcy with labor deal <END>\n",
      " delta dodges bankruptcy with labor deal  \n",
      "tensor([3.3333e-05, 3.3333e-05, 3.3333e-05, 3.3333e-05])\n"
     ]
    }
   ],
   "source": [
    "# Dataset instance\n",
    "dataset = NewsDataset.load_dataset_and_make_vectorizer(args.split_data_file,\n",
    "                                                       args.cutoff)\n",
    "print (dataset) # __str__\n",
    "input_ = dataset[10] # __getitem__\n",
    "print (input_['title_word_vector'])\n",
    "print (input_['title_char_vector'])\n",
    "print (input_['title_length'])\n",
    "print (input_['category'])\n",
    "print (dataset.vectorizer.unvectorize_word_vector(input_['title_word_vector']))\n",
    "print (dataset.vectorizer.unvectorize_char_vector(input_['title_char_vector']))\n",
    "print (dataset.class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xJV5WlDiFVVz",
    "mdEditEnable": false
   },
   "source": [
    "### 模型\n",
    "embed → encoder → attend → predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rZCzdZZ9FMhm",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c9wipRZt7feC",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NewsEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_word_embeddings, num_char_embeddings,\n",
    "                 kernels, num_input_channels, num_output_channels, \n",
    "                 rnn_hidden_dim, num_layers, bidirectional, \n",
    "                 word_padding_idx=0, char_padding_idx=0):\n",
    "        super(NewsEncoder, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        # Embeddings\n",
    "        self.word_embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
    "                                            num_embeddings=num_word_embeddings,\n",
    "                                            padding_idx=word_padding_idx)\n",
    "        self.char_embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
    "                                            num_embeddings=num_char_embeddings,\n",
    "                                            padding_idx=char_padding_idx)\n",
    "        \n",
    "        # Conv weights\n",
    "        self.conv = nn.ModuleList([nn.Conv1d(num_input_channels, \n",
    "                                             num_output_channels, \n",
    "                                             kernel_size=f) for f in kernels])\n",
    "        \n",
    "        \n",
    "        # GRU weights\n",
    "        self.gru = nn.GRU(input_size=embedding_dim*(len(kernels)+1), \n",
    "                          hidden_size=rnn_hidden_dim, num_layers=num_layers, \n",
    "                          batch_first=True, bidirectional=bidirectional)\n",
    "        \n",
    "    def initialize_hidden_state(self, batch_size, rnn_hidden_dim, device):\n",
    "        \"\"\"Modify this to condition the RNN.\"\"\"\n",
    "        num_directions = 1\n",
    "        if self.bidirectional:\n",
    "            num_directions = 2\n",
    "        hidden_t = torch.zeros(self.num_layers * num_directions, \n",
    "                               batch_size, rnn_hidden_dim).to(device)\n",
    "        \n",
    "    def get_char_level_embeddings(self, x):\n",
    "        # x: (N, seq_len, word_len)\n",
    "        input_shape = x.size()\n",
    "        batch_size, seq_len, word_len = input_shape\n",
    "        x = x.view(-1, word_len) # (N*seq_len, word_len)\n",
    "        \n",
    "        # Embedding\n",
    "        x = self.char_embeddings(x) # (N*seq_len, word_len, embedding_dim)\n",
    "        \n",
    "        # Rearrange input so num_input_channels is in dim 1 (N, embedding_dim, word_len)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Convolution\n",
    "        z = [F.relu(conv(x)) for conv in self.conv]\n",
    "        \n",
    "        # Pooling\n",
    "        z = [F.max_pool1d(zz, zz.size(2)).squeeze(2) for zz in z] \n",
    "        z = [zz.view(batch_size, seq_len, -1) for zz in z] # (N, seq_len, embedding_dim)\n",
    "        \n",
    "        # Concat to get char-level embeddings\n",
    "        z = torch.cat(z, 2) # join conv outputs\n",
    "        \n",
    "        return z\n",
    "        \n",
    "    def forward(self, x_word, x_char, x_lengths, device):\n",
    "        \"\"\"\n",
    "        x_word: word level representation (N, seq_size)\n",
    "        x_char: char level representation (N, seq_size, word_len)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Word level embeddings\n",
    "        z_word = self.word_embeddings(x_word)\n",
    "        \n",
    "        # Char level embeddings\n",
    "        z_char = self.get_char_level_embeddings(x=x_char)\n",
    "        \n",
    "        # Concatenate\n",
    "        z = torch.cat([z_word, z_char], 2)\n",
    "        \n",
    "        # Feed into RNN\n",
    "        initial_h = self.initialize_hidden_state(\n",
    "            batch_size=z.size(0), rnn_hidden_dim=self.gru.hidden_size,\n",
    "            device=device)\n",
    "        out, h_n = self.gru(z, initial_h)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zeEcdA287gz4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NewsDecoder(nn.Module):\n",
    "    def __init__(self, rnn_hidden_dim, hidden_dim, output_dim, dropout_p):\n",
    "        super(NewsDecoder, self).__init__()\n",
    "        \n",
    "        # 注意力全连接模型\n",
    "        self.fc_attn = nn.Linear(rnn_hidden_dim, rnn_hidden_dim)\n",
    "        self.v = nn.Parameter(torch.rand(rnn_hidden_dim))\n",
    "        \n",
    "        # 全连接权重\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, encoder_outputs, apply_softmax=False):\n",
    "        \n",
    "        # Attention\n",
    "        z = torch.tanh(self.fc_attn(encoder_outputs))\n",
    "        z = z.transpose(2,1) # [B*H*T]\n",
    "        v = self.v.repeat(encoder_outputs.size(0),1).unsqueeze(1) #[B*1*H]\n",
    "        z = torch.bmm(v,z).squeeze(1) # [B*T]\n",
    "        attn_scores = F.softmax(z, dim=1)\n",
    "        context = torch.matmul(encoder_outputs.transpose(-2, -1), \n",
    "                               attn_scores.unsqueeze(dim=2)).squeeze()\n",
    "        if len(context.size()) == 1:\n",
    "            context = context.unsqueeze(0)\n",
    "        \n",
    "        # 全连接层\n",
    "        z = self.dropout(context)\n",
    "        z = self.fc1(z)\n",
    "        z = self.dropout(z)\n",
    "        y_pred = self.fc2(z)\n",
    "\n",
    "        if apply_softmax:\n",
    "            y_pred = F.softmax(y_pred, dim=1)\n",
    "        return attn_scores, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yVDftS-G7gwy",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NewsModel(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_word_embeddings, num_char_embeddings,\n",
    "                 kernels, num_input_channels, num_output_channels, \n",
    "                 rnn_hidden_dim, hidden_dim, output_dim, num_layers, \n",
    "                 bidirectional, dropout_p, word_padding_idx, char_padding_idx):\n",
    "        super(NewsModel, self).__init__()\n",
    "        self.encoder = NewsEncoder(embedding_dim, num_word_embeddings,\n",
    "                                   num_char_embeddings, kernels, \n",
    "                                   num_input_channels, num_output_channels, \n",
    "                                   rnn_hidden_dim, num_layers, bidirectional, \n",
    "                                   word_padding_idx, char_padding_idx)\n",
    "        self.decoder = NewsDecoder(rnn_hidden_dim, hidden_dim, output_dim, \n",
    "                                   dropout_p)\n",
    "        \n",
    "    def forward(self, x_word, x_char, x_lengths, device, apply_softmax=False):\n",
    "        encoder_outputs = self.encoder(x_word, x_char, x_lengths, device)\n",
    "        y_pred = self.decoder(encoder_outputs, apply_softmax)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jHPYCPd7Fl3M",
    "mdEditEnable": false
   },
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D3seBMA7FlcC",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HnRKWLekFlnM",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, dataset, model, model_state_file, save_dir, device, \n",
    "                 shuffle, num_epochs, batch_size, learning_rate, \n",
    "                 early_stopping_criteria):\n",
    "        self.dataset = dataset\n",
    "        self.class_weights = dataset.class_weights.to(device)\n",
    "        self.device = device\n",
    "        self.model = model.to(device)\n",
    "        self.save_dir = save_dir\n",
    "        self.device = device\n",
    "        self.shuffle = shuffle\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_func = nn.CrossEntropyLoss(self.class_weights)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer=self.optimizer, mode='min', factor=0.5, patience=1)\n",
    "        self.train_state = {\n",
    "            'stop_early': False, \n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'early_stopping_criteria': early_stopping_criteria,\n",
    "            'learning_rate': learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': model_state_file}\n",
    "    \n",
    "    def update_train_state(self):\n",
    "\n",
    "        # Verbose\n",
    "        print (\"[EPOCH]: {0:02d} | [LR]: {1} | [TRAIN LOSS]: {2:.2f} | [TRAIN ACC]: {3:.1f}% | [VAL LOSS]: {4:.2f} | [VAL ACC]: {5:.1f}%\".format(\n",
    "          self.train_state['epoch_index'], self.train_state['learning_rate'], \n",
    "            self.train_state['train_loss'][-1], self.train_state['train_acc'][-1], \n",
    "            self.train_state['val_loss'][-1], self.train_state['val_acc'][-1]))\n",
    "\n",
    "        # Save one model at least\n",
    "        if self.train_state['epoch_index'] == 0:\n",
    "            torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
    "            self.train_state['stop_early'] = False\n",
    "\n",
    "        # Save model if performance improved\n",
    "        elif self.train_state['epoch_index'] >= 1:\n",
    "            loss_tm1, loss_t = self.train_state['val_loss'][-2:]\n",
    "\n",
    "            # If loss worsened\n",
    "            if loss_t >= self.train_state['early_stopping_best_val']:\n",
    "                # Update step\n",
    "                self.train_state['early_stopping_step'] += 1\n",
    "\n",
    "            # Loss decreased\n",
    "            else:\n",
    "                # Save the best model\n",
    "                if loss_t < self.train_state['early_stopping_best_val']:\n",
    "                    torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
    "\n",
    "                # Reset early stopping step\n",
    "                self.train_state['early_stopping_step'] = 0\n",
    "\n",
    "            # Stop early ?\n",
    "            self.train_state['stop_early'] = self.train_state['early_stopping_step'] \\\n",
    "              >= self.train_state['early_stopping_criteria']\n",
    "        return self.train_state\n",
    "  \n",
    "    def compute_accuracy(self, y_pred, y_target):\n",
    "        _, y_pred_indices = y_pred.max(dim=1)\n",
    "        n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "        return n_correct / len(y_pred_indices) * 100\n",
    "    \n",
    "    def pad_word_seq(self, seq, length):\n",
    "        vector = np.zeros(length, dtype=np.int64)\n",
    "        vector[:len(seq)] = seq\n",
    "        vector[len(seq):] = self.dataset.vectorizer.title_word_vocab.mask_index\n",
    "        return vector\n",
    "    \n",
    "    def pad_char_seq(self, seq, seq_length, word_length):\n",
    "        vector = np.zeros((seq_length, word_length), dtype=np.int64)\n",
    "        vector.fill(self.dataset.vectorizer.title_char_vocab.mask_index)\n",
    "        for i in range(len(seq)):\n",
    "            char_padding = np.zeros(word_length-len(seq[i]), dtype=np.int64)\n",
    "            vector[i] = np.concatenate((seq[i], char_padding), axis=None)\n",
    "        return vector\n",
    "        \n",
    "    def collate_fn(self, batch):\n",
    "        \n",
    "        # Make a deep copy\n",
    "        batch_copy = copy.deepcopy(batch)\n",
    "        processed_batch = {\"title_word_vector\": [], \"title_char_vector\": [], \n",
    "                           \"title_length\": [], \"category\": []}\n",
    "             \n",
    "        # Max lengths\n",
    "        get_seq_length = lambda sample: len(sample[\"title_word_vector\"])\n",
    "        get_word_length = lambda sample: len(sample[\"title_char_vector\"][0])\n",
    "        max_seq_length = max(map(get_seq_length, batch))\n",
    "        max_word_length = max(map(get_word_length, batch))\n",
    "\n",
    "\n",
    "        # Pad\n",
    "        for i, sample in enumerate(batch_copy):\n",
    "            padded_word_seq = self.pad_word_seq(\n",
    "                sample[\"title_word_vector\"], max_seq_length)\n",
    "            padded_char_seq = self.pad_char_seq(\n",
    "                sample[\"title_char_vector\"], max_seq_length, max_word_length)\n",
    "            processed_batch[\"title_word_vector\"].append(padded_word_seq)\n",
    "            processed_batch[\"title_char_vector\"].append(padded_char_seq)\n",
    "            processed_batch[\"title_length\"].append(sample[\"title_length\"])\n",
    "            processed_batch[\"category\"].append(sample[\"category\"])\n",
    "            \n",
    "        # Convert to appropriate tensor types\n",
    "        processed_batch[\"title_word_vector\"] = torch.LongTensor(\n",
    "            processed_batch[\"title_word_vector\"])\n",
    "        processed_batch[\"title_char_vector\"] = torch.LongTensor(\n",
    "            processed_batch[\"title_char_vector\"])\n",
    "        processed_batch[\"title_length\"] = torch.LongTensor(\n",
    "            processed_batch[\"title_length\"])\n",
    "        processed_batch[\"category\"] = torch.LongTensor(\n",
    "            processed_batch[\"category\"])\n",
    "        \n",
    "        return processed_batch  \n",
    "  \n",
    "    def run_train_loop(self):\n",
    "        for epoch_index in range(self.num_epochs):\n",
    "            self.train_state['epoch_index'] = epoch_index\n",
    "      \n",
    "            # Iterate over train dataset\n",
    "\n",
    "            # initialize batch generator, set loss and acc to 0, set train mode on\n",
    "            self.dataset.set_split('train')\n",
    "            batch_generator = self.dataset.generate_batches(\n",
    "                batch_size=self.batch_size, collate_fn=self.collate_fn, \n",
    "                shuffle=self.shuffle, device=self.device)\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            self.model.train()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "                # zero the gradients\n",
    "                self.optimizer.zero_grad()\n",
    "                \n",
    "                # compute the output\n",
    "                _, y_pred = self.model(x_word=batch_dict['title_word_vector'],\n",
    "                                       x_char=batch_dict['title_char_vector'],\n",
    "                                       x_lengths=batch_dict['title_length'],\n",
    "                                       device=self.device)\n",
    "                \n",
    "                # compute the loss\n",
    "                loss = self.loss_func(y_pred, batch_dict['category'])\n",
    "                loss_t = loss.item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # compute gradients using loss\n",
    "                loss.backward()\n",
    "\n",
    "                # use optimizer to take a gradient step\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # compute the accuracy\n",
    "                acc_t = self.compute_accuracy(y_pred, batch_dict['category'])\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            self.train_state['train_loss'].append(running_loss)\n",
    "            self.train_state['train_acc'].append(running_acc)\n",
    "\n",
    "            # Iterate over val dataset\n",
    "\n",
    "            # initialize batch generator, set loss and acc to 0, set eval mode on\n",
    "            self.dataset.set_split('val')\n",
    "            batch_generator = self.dataset.generate_batches(\n",
    "                batch_size=self.batch_size, collate_fn=self.collate_fn, \n",
    "                shuffle=self.shuffle, device=self.device)\n",
    "            running_loss = 0.\n",
    "            running_acc = 0.\n",
    "            self.model.eval()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "                # compute the output\n",
    "                _, y_pred = self.model(x_word=batch_dict['title_word_vector'],\n",
    "                                       x_char=batch_dict['title_char_vector'],\n",
    "                                       x_lengths=batch_dict['title_length'],\n",
    "                                       device=self.device)\n",
    "\n",
    "                # compute the loss\n",
    "                loss = self.loss_func(y_pred, batch_dict['category'])\n",
    "                loss_t = loss.to(\"cpu\").item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # compute the accuracy\n",
    "                acc_t = self.compute_accuracy(y_pred, batch_dict['category'])\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            self.train_state['val_loss'].append(running_loss)\n",
    "            self.train_state['val_acc'].append(running_acc)\n",
    "\n",
    "            self.train_state = self.update_train_state()\n",
    "            self.scheduler.step(self.train_state['val_loss'][-1])\n",
    "            if self.train_state['stop_early']:\n",
    "                break\n",
    "          \n",
    "    def run_test_loop(self):\n",
    "        # initialize batch generator, set loss and acc to 0, set eval mode on\n",
    "        self.dataset.set_split('test')\n",
    "        batch_generator = self.dataset.generate_batches(\n",
    "            batch_size=self.batch_size, collate_fn=self.collate_fn, \n",
    "            shuffle=self.shuffle, device=self.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        self.model.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # compute the output\n",
    "            _, y_pred = self.model(x_word=batch_dict['title_word_vector'],\n",
    "                                   x_char=batch_dict['title_char_vector'],\n",
    "                                   x_lengths=batch_dict['title_length'],\n",
    "                                   device=self.device)\n",
    "\n",
    "            # compute the loss\n",
    "            loss = self.loss_func(y_pred, batch_dict['category'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = self.compute_accuracy(y_pred, batch_dict['category'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "        self.train_state['test_loss'] = running_loss\n",
    "        self.train_state['test_acc'] = running_acc\n",
    "    \n",
    "    def plot_performance(self):\n",
    "        # Figure size\n",
    "        plt.figure(figsize=(15,5))\n",
    "\n",
    "        # Plot Loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Loss\")\n",
    "        plt.plot(trainer.train_state[\"train_loss\"], label=\"train\")\n",
    "        plt.plot(trainer.train_state[\"val_loss\"], label=\"val\")\n",
    "        plt.legend(loc='upper right')\n",
    "\n",
    "        # Plot Accuracy\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.plot(trainer.train_state[\"train_acc\"], label=\"train\")\n",
    "        plt.plot(trainer.train_state[\"val_acc\"], label=\"val\")\n",
    "        plt.legend(loc='lower right')\n",
    "\n",
    "        # Save figure\n",
    "        plt.savefig(os.path.join(self.save_dir, \"performance.png\"))\n",
    "\n",
    "        # Show plots\n",
    "        plt.show()\n",
    "    \n",
    "    def save_train_state(self):\n",
    "        with open(os.path.join(self.save_dir, \"train_state.json\"), \"w\") as fp:\n",
    "            json.dump(self.train_state, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "ICkiOaGtFlk-",
    "outputId": "18174034-ce3e-444a-a968-aba51eb03b3e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.named_modules of NewsModel(\n",
      "  (encoder): NewsEncoder(\n",
      "    (word_embeddings): Embedding(3406, 100, padding_idx=0)\n",
      "    (char_embeddings): Embedding(35, 100, padding_idx=0)\n",
      "    (conv): ModuleList(\n",
      "      (0): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
      "      (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
      "    )\n",
      "    (gru): GRU(300, 128, batch_first=True)\n",
      "  )\n",
      "  (decoder): NewsDecoder(\n",
      "    (fc_attn): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.25)\n",
      "    (fc1): Linear(in_features=128, out_features=200, bias=True)\n",
      "    (fc2): Linear(in_features=200, out_features=4, bias=True)\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# Initialization\n",
    "dataset = NewsDataset.load_dataset_and_make_vectorizer(args.split_data_file,\n",
    "                                                       args.cutoff)\n",
    "dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.vectorizer\n",
    "model = NewsModel(embedding_dim=args.embedding_dim, \n",
    "                  num_word_embeddings=len(vectorizer.title_word_vocab), \n",
    "                  num_char_embeddings=len(vectorizer.title_char_vocab),\n",
    "                  kernels=args.kernels,\n",
    "                  num_input_channels=args.embedding_dim,\n",
    "                  num_output_channels=args.num_filters,\n",
    "                  rnn_hidden_dim=args.rnn_hidden_dim,\n",
    "                  hidden_dim=args.hidden_dim,\n",
    "                  output_dim=len(vectorizer.category_vocab),\n",
    "                  num_layers=args.num_layers,\n",
    "                  bidirectional=args.bidirectional,\n",
    "                  dropout_p=args.dropout_p, \n",
    "                  word_padding_idx=vectorizer.title_word_vocab.mask_index,\n",
    "                  char_padding_idx=vectorizer.title_char_vocab.mask_index)\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "tuaRZ4DiFlh1",
    "outputId": "6496aa05-de58-4913-a56a-9885bd60d9ad",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH]: 00 | [LR]: 0.001 | [TRAIN LOSS]: 0.77 | [TRAIN ACC]: 68.8% | [VAL LOSS]: 0.55 | [VAL ACC]: 79.8%\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "trainer = Trainer(dataset=dataset, model=model, \n",
    "                  model_state_file=args.model_state_file, \n",
    "                  save_dir=args.save_dir, device=args.device,\n",
    "                  shuffle=args.shuffle, num_epochs=args.num_epochs, \n",
    "                  batch_size=args.batch_size, learning_rate=args.learning_rate, \n",
    "                  early_stopping_criteria=args.early_stopping_criteria)\n",
    "trainer.run_train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "mzRJIz88Flfe",
    "outputId": "dece6240-57ab-4abc-f9cc-ecd11dabcdc6",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot performance\n",
    "trainer.plot_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "4EmFhiX-FMaV",
    "outputId": "29ef6d38-6258-429b-841f-7345b7cd0695",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test performance\n",
    "trainer.run_test_loop()\n",
    "print(\"Test loss: {0:.2f}\".format(trainer.train_state['test_loss']))\n",
    "print(\"Test Accuracy: {0:.1f}%\".format(trainer.train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zVU1zakYFMVF",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Save all results\n",
    "trainer.save_train_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qLoKfjSpFw7t",
    "mdEditEnable": false
   },
   "source": [
    "### 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ANrPcS7Hp_CP",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Inference(object):\n",
    "    def __init__(self, model, vectorizer):\n",
    "        self.model = model\n",
    "        self.vectorizer = vectorizer\n",
    "  \n",
    "    def predict_category(self, title):\n",
    "        # Vectorize\n",
    "        word_vector, char_vector, title_length = self.vectorizer.vectorize(title)\n",
    "        title_word_vector = torch.tensor(word_vector).unsqueeze(0)\n",
    "        title_char_vector = torch.tensor(char_vector).unsqueeze(0)\n",
    "        title_length = torch.tensor([title_length]).long()        \n",
    "        \n",
    "        # Forward pass\n",
    "        self.model.eval()\n",
    "        attn_scores, y_pred = self.model(x_word=title_word_vector, \n",
    "                                         x_char=title_char_vector,\n",
    "                                         x_lengths=title_length, \n",
    "                                         device=\"cpu\",\n",
    "                                         apply_softmax=True)\n",
    "\n",
    "        # Top category\n",
    "        y_prob, indices = y_pred.max(dim=1)\n",
    "        index = indices.item()\n",
    "\n",
    "        # Predicted category\n",
    "        category = vectorizer.category_vocab.lookup_index(index)\n",
    "        probability = y_prob.item()\n",
    "        return {'category': category, 'probability': probability, \n",
    "                'attn_scores': attn_scores}\n",
    "    \n",
    "    def predict_top_k(self, title, k):\n",
    "        # Vectorize\n",
    "        word_vector, char_vector, title_length = self.vectorizer.vectorize(title)\n",
    "        title_word_vector = torch.tensor(word_vector).unsqueeze(0)\n",
    "        title_char_vector = torch.tensor(char_vector).unsqueeze(0)\n",
    "        title_length = torch.tensor([title_length]).long()\n",
    "        \n",
    "         # Forward pass\n",
    "        self.model.eval()\n",
    "        _, y_pred = self.model(x_word=title_word_vector,\n",
    "                               x_char=title_char_vector,\n",
    "                               x_lengths=title_length, \n",
    "                               device=\"cpu\",\n",
    "                               apply_softmax=True)\n",
    "        \n",
    "        # Top k categories\n",
    "        y_prob, indices = torch.topk(y_pred, k=k)\n",
    "        probabilities = y_prob.detach().numpy()[0]\n",
    "        indices = indices.detach().numpy()[0]\n",
    "\n",
    "        # Results\n",
    "        results = []\n",
    "        for probability, index in zip(probabilities, indices):\n",
    "            category = self.vectorizer.category_vocab.lookup_index(index)\n",
    "            results.append({'category': category, 'probability': probability})\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "W6wr68o2p_Eh",
    "outputId": "87886e24-350d-433e-981d-b2907b0c95cf",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.named_modules of NewsModel(\n",
      "  (encoder): NewsEncoder(\n",
      "    (word_embeddings): Embedding(3406, 100, padding_idx=0)\n",
      "    (char_embeddings): Embedding(35, 100, padding_idx=0)\n",
      "    (conv): ModuleList(\n",
      "      (0): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
      "      (1): Conv1d(100, 100, kernel_size=(5,), stride=(1,))\n",
      "    )\n",
      "    (gru): GRU(300, 128, batch_first=True)\n",
      "  )\n",
      "  (decoder): NewsDecoder(\n",
      "    (fc_attn): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (dropout): Dropout(p=0.25)\n",
      "    (fc1): Linear(in_features=128, out_features=200, bias=True)\n",
      "    (fc2): Linear(in_features=200, out_features=4, bias=True)\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "dataset = NewsDataset.load_dataset_and_load_vectorizer(\n",
    "    args.split_data_file, args.vectorizer_file)\n",
    "vectorizer = dataset.vectorizer\n",
    "model = NewsModel(embedding_dim=args.embedding_dim, \n",
    "                  num_word_embeddings=len(vectorizer.title_word_vocab), \n",
    "                  num_char_embeddings=len(vectorizer.title_char_vocab),\n",
    "                  kernels=args.kernels,\n",
    "                  num_input_channels=args.embedding_dim,\n",
    "                  num_output_channels=args.num_filters,\n",
    "                  rnn_hidden_dim=args.rnn_hidden_dim,\n",
    "                  hidden_dim=args.hidden_dim,\n",
    "                  output_dim=len(vectorizer.category_vocab),\n",
    "                  num_layers=args.num_layers,\n",
    "                  bidirectional=args.bidirectional,\n",
    "                  dropout_p=args.dropout_p, \n",
    "                  word_padding_idx=vectorizer.title_word_vocab.mask_index,\n",
    "                  char_padding_idx=vectorizer.title_char_vocab.mask_index)\n",
    "model.load_state_dict(torch.load(args.model_state_file))\n",
    "model = model.to(\"cpu\")\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "JPKgHxsfN954",
    "outputId": "0445e3a7-24a9-4c77-829d-a25681768ab1",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a title to classify: asfasdfsdaf\n",
      "asfasdfsdaf → Sports (p=0.48)\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "inference = Inference(model=model, vectorizer=vectorizer)\n",
    "title = input(\"Enter a title to classify: \")\n",
    "prediction = inference.predict_category(preprocess_text(title))\n",
    "print(\"{} → {} (p={:0.2f})\".format(title, prediction['category'], \n",
    "                                   prediction['probability']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "JRdz4wzuQR4N",
    "outputId": "f2c91b24-a36a-4e35-b06a-f6618497d64f",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asfasdfsdaf: \n",
      "Sports (p=0.48)\n",
      "Business (p=0.22)\n",
      "Sci/Tech (p=0.20)\n",
      "World (p=0.10)\n"
     ]
    }
   ],
   "source": [
    "# Top-k inference\n",
    "top_k = inference.predict_top_k(preprocess_text(title), k=len(vectorizer.category_vocab))\n",
    "print (\"{}: \".format(title))\n",
    "for result in top_k:\n",
    "    print (\"{} (p={:0.2f})\".format(result['category'], \n",
    "                                   result['probability']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qrAieHoHxOt2",
    "mdEditEnable": false
   },
   "source": [
    "## 可解释性\n",
    "\n",
    "\n",
    "我们可查看每个时间步生成的概率向量，然后对   前面每一个隐藏状态对某一时间步的预测结果的影响  做可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k6uZY4J8vYgw",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 330
    },
    "colab_type": "code",
    "id": "2PNuY7GLoEi4",
    "outputId": "24b2e48f-da5b-4251-c2eb-81e72603a6f4",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attn_matrix = prediction['attn_scores'].detach().numpy()\n",
    "ax = sns.heatmap(attn_matrix, linewidths=2, square=True)\n",
    "tokens = [\"<BEGIN>\"]+preprocess_text(title).split(\" \")+[\"<END>\"]\n",
    "ax.set_xticklabels(tokens, rotation=45)\n",
    "ax.set_xlabel(\"Token\")\n",
    "ax.set_ylabel(\"Importance\\n\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "594F42ADC95F4DB58117247C59E53206"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
