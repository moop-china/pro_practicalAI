{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OLIxEDq6VhvZ",
    "mdEditEnable": false
   },
   "source": [
    "## 面向对象机器学习 (Object-Oriented Machine Learning)\n",
    "\n",
    "<img src=\"data/logo.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "在这个项目中，我们会学习如何创建和运用Python的类和函数来解决机器学习问题。今后的课程项目中我们都会按照这样的实现方式和代码结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f88fhoHFLIKg",
    "mdEditEnable": false
   },
   "source": [
    "## Overview\n",
    "\n",
    "下面是我们将会见到的词汇和他们的意义:\n",
    "\n",
    "* **词汇表(Vocabulary)**: 原始输入和数字形式的转换字典。一般用在自然语言处理任务中。\n",
    "* **向量器(Vectorizer)**: 输入和输出的词汇表类实例，并为模型的训练和运行把数据向量化\n",
    "* **数据集(Dataset)**: 用于处理和切分数据集的向量器\n",
    "* **模型(Model)**: 用于处理和返回预测值的模型，由于我们主要使用PyTorch建模，这里指PyTorch模型\n",
    "\n",
    "感谢[PyTorch的开发者们](https://github.com/joosthub/pytorch/graphs/contributors) ！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b6B0GSOuY_ty",
    "mdEditEnable": false
   },
   "source": [
    "## 配置\n",
    "首先，我们需要配置可重复现的环境、参数、随机种子等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WnV34uLSY4bZ",
    "outputId": "923141ef-0d88-47ba-8b6a-ad14e50b789d",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already up-to-date: torch in /opt/conda/lib/python3.5/site-packages (1.0.0)\n",
      "\u001b[33mYou are using pip version 18.0, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# 安装PyTorch \n",
    "!pip3 install torch --upgrade -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0-dXQiLlTIgz",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "import collections\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hnID97KCXuT-",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 设置 Numpy 和 PyTorch 种子\n",
    "def set_seeds(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "# 创建目录\n",
    "def create_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wmpwPJr5VtKP",
    "outputId": "4ede8242-2dcd-4f30-aaa2-88090713b1a6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n"
     ]
    }
   ],
   "source": [
    "# 参数\n",
    "args = Namespace(\n",
    "    seed=1234,\n",
    "    cuda=False,\n",
    "    shuffle=True,\n",
    "    data_file=\"data/names.csv\",\n",
    "    split_data_file=\"data/split_names.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"names\",\n",
    "    train_size=0.7,\n",
    "    val_size=0.15,\n",
    "    test_size=0.15,\n",
    "    num_epochs=20,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=64,\n",
    "    hidden_dim=300,\n",
    "    dropout_p=0.1,\n",
    ")\n",
    "\n",
    "# 设置种子\n",
    "set_seeds(seed=args.seed, cuda=args.cuda)\n",
    "\n",
    "# 新建保存文件路径\n",
    "create_dirs(args.save_dir)\n",
    "\n",
    "# 展开文件路径\n",
    "args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "# 检查GPU可用性\n",
    "if torch.cuda.is_available():\n",
    "    args.cuda = True\n",
    "else:\n",
    "    args.cuda = False\n",
    "    \n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_Fg-TslU2Bs",
    "mdEditEnable": false
   },
   "source": [
    "## 数据\n",
    "\n",
    "我们的任务是得到输入姓名对应的可能国籍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M7H6_m8XZkFX",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vq8Ooa6RZWhX",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "url = \"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/data/surnames.csv\"\n",
    "response = urllib.request.urlopen(url)\n",
    "html = response.read()\n",
    "with open(args.data_file, 'wb') as fp:\n",
    "    fp.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "QkkdVRivZKd8",
    "outputId": "79d7d550-f7e0-4eb7-9a62-ebac2c514fd1",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>surname</th>\n",
       "      <th>nationality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Woodford</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coté</td>\n",
       "      <td>French</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kore</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Koury</td>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lebzak</td>\n",
       "      <td>Russian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    surname nationality\n",
       "0  Woodford     English\n",
       "1      Coté      French\n",
       "2      Kore     English\n",
       "3     Koury      Arabic\n",
       "4    Lebzak     Russian"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 原始数据\n",
    "df = pd.read_csv(args.data_file, header=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "diz8O5KKBbwj",
    "outputId": "08fe865a-7220-4036-d07f-e43725637f04",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: 2972\n",
      "French: 229\n",
      "Arabic: 1603\n",
      "Russian: 2373\n",
      "Japanese: 775\n",
      "Chinese: 220\n",
      "Italian: 600\n",
      "Czech: 414\n",
      "Irish: 183\n",
      "German: 576\n",
      "Greek: 156\n",
      "Spanish: 258\n",
      "Polish: 120\n",
      "Dutch: 236\n",
      "Vietnamese: 58\n",
      "Korean: 77\n",
      "Portuguese: 55\n",
      "Scottish: 75\n"
     ]
    }
   ],
   "source": [
    "# 按国籍切分数据\n",
    "by_nationality = collections.defaultdict(list)\n",
    "for _, row in df.iterrows():\n",
    "    by_nationality[row.nationality].append(row.to_dict())\n",
    "for nationality in by_nationality:\n",
    "    print (\"{0}: {1}\".format(nationality, len(by_nationality[nationality])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PYwA2DD7BoNv",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 新建切分数据集\n",
    "final_list = []\n",
    "for _, item_list in sorted(by_nationality.items()):\n",
    "    if args.shuffle:\n",
    "        np.random.shuffle(item_list)\n",
    "    n = len(item_list)\n",
    "    n_train = int(args.train_size*n)\n",
    "    n_val = int(args.val_size*n)\n",
    "    n_test = int(args.test_size*n)\n",
    "\n",
    "  # 给每个数据点添加子集属性\n",
    "    for item in item_list[:n_train]:\n",
    "        item['split'] = 'train'\n",
    "    for item in item_list[n_train:n_train+n_val]:\n",
    "        item['split'] = 'val'\n",
    "    for item in item_list[n_train+n_val:]:\n",
    "        item['split'] = 'test'  \n",
    "\n",
    "    # 添加到最新的列表\n",
    "    final_list.extend(item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "oxYU4gnLCzcV",
    "outputId": "56762991-d349-423a-8dfd-74e36f513508",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    7680\n",
       "test     1660\n",
       "val      1640\n",
       "Name: split, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 切分后的数据集dataframe\n",
    "split_df = pd.DataFrame(final_list)\n",
    "split_df[\"split\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k_erGyQ24Tnb",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 预处理\n",
    "def preprocess_text(text):\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text\n",
    "    \n",
    "split_df.surname = split_df.surname.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "BnLGAU_7Dl7y",
    "outputId": "d59eff3a-c6f9-4f47-fdc6-847f140b931b",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nationality</th>\n",
       "      <th>split</th>\n",
       "      <th>surname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "      <td>bishara</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "      <td>nahas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "      <td>ghanem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "      <td>tannous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arabic</td>\n",
       "      <td>train</td>\n",
       "      <td>mikhail</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  nationality  split  surname\n",
       "0      Arabic  train  bishara\n",
       "1      Arabic  train    nahas\n",
       "2      Arabic  train   ghanem\n",
       "3      Arabic  train  tannous\n",
       "4      Arabic  train  mikhail"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 存入CSV文件\n",
    "split_df.to_csv(args.split_data_file, index=False)\n",
    "split_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sj7DzE_YvjSr",
    "mdEditEnable": false
   },
   "source": [
    "## 词汇表\n",
    "\n",
    "对国籍(nationality)和姓(surname)创建 Vocabulary 类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QDFTRZ4nDzxp",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "\n",
    "        # 令牌(Token)到索引\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self.token_to_idx = token_to_idx\n",
    "\n",
    "        # 索引到Token\n",
    "        self.idx_to_token = {idx: token \\\n",
    "                             for token, idx in self.token_to_idx.items()}\n",
    "        \n",
    "        # 添加未知Token\n",
    "        self.add_unk = add_unk\n",
    "        self.unk_token = unk_token\n",
    "        if self.add_unk:\n",
    "            self.unk_index = self.add_token(self.unk_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'token_to_idx': self.token_to_idx,\n",
    "                'add_unk': self.add_unk, 'unk_token': self.unk_token}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token in self.token_to_idx:\n",
    "            index = self.token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self.token_to_idx)\n",
    "            self.token_to_idx[token] = index\n",
    "            self.idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        return [self.add_token[token] for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        if self.add_unk:\n",
    "            index = self.token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            index =  self.token_to_idx[token]\n",
    "        return index\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        if index not in self.idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self.idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "PJ56RLko0prE",
    "outputId": "06161be2-181f-4436-b310-258bce6f349e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary(size=18)>\n",
      "18\n",
      "0\n",
      "English\n"
     ]
    }
   ],
   "source": [
    "# 词汇表实例\n",
    "nationality_vocab = Vocabulary(add_unk=False)\n",
    "for index, row in df.iterrows():\n",
    "    nationality_vocab.add_token(row.nationality)\n",
    "print (nationality_vocab) # __str__\n",
    "print (len(nationality_vocab)) # __len__\n",
    "index = nationality_vocab.lookup_token(\"English\")\n",
    "print (index)\n",
    "print (nationality_vocab.lookup_index(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wt6sCsBu238H",
    "mdEditEnable": false
   },
   "source": [
    "## 向量器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lFSaEs4L2TkY",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class SurnameVectorizer(object):\n",
    "    def __init__(self, surname_vocab, nationality_vocab):\n",
    "        self.surname_vocab = surname_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "\n",
    "    def vectorize(self, surname):\n",
    "        one_hot = np.zeros(len(self.surname_vocab), dtype=np.float32)\n",
    "        for token in surname:\n",
    "            one_hot[self.surname_vocab.lookup_token(token)] = 1\n",
    "        return one_hot\n",
    "\n",
    "    def unvectorize(self, one_hot):\n",
    "        surname = [vectorizer.surname_vocab.lookup_index(index) \\\n",
    "            for index in np.where(one_hot==1)[0]]\n",
    "        return surname\n",
    "        \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, df):\n",
    "        surname_vocab = Vocabulary(add_unk=True)\n",
    "        nationality_vocab = Vocabulary(add_unk=False)\n",
    "\n",
    "        # 创建词汇表\n",
    "        for index, row in df.iterrows():\n",
    "            for letter in row.surname: # char-level tokenization\n",
    "                surname_vocab.add_token(letter)\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "        return cls(surname_vocab, nationality_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        surname_vocab = Vocabulary.from_serializable(contents['surname_vocab'])\n",
    "        nationality_vocab =  Vocabulary.from_serializable(contents['nationality_vocab'])\n",
    "        return cls(surname_vocab, nationality_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'surname_vocab': self.surname_vocab.to_serializable(),\n",
    "                'nationality_vocab': self.nationality_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "IBf2a0nz4Ji_",
    "outputId": "199a097e-bfa2-4205-e168-c643c6c1bc66",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary(size=28)>\n",
      "<Vocabulary(size=18)>\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "['g', 'o', 'u', 'k']\n"
     ]
    }
   ],
   "source": [
    "# 向量器实例\n",
    "vectorizer = SurnameVectorizer.from_dataframe(split_df)\n",
    "print (vectorizer.surname_vocab)\n",
    "print (vectorizer.nationality_vocab)\n",
    "one_hot = vectorizer.vectorize(preprocess_text(\"goku\"))\n",
    "print (one_hot)\n",
    "print (vectorizer.unvectorize(one_hot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N0QYigLGZJ6G",
    "mdEditEnable": false
   },
   "source": [
    "**注意**: 对独热编码形式的输入进行向量化操作时， 我们丢失了所有和名称相关的结构信息。这是用独热编码表示文本的主要缺点，但是接下来我们会展示更多可保留语义结构的编码方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vT7q4sh558yh",
    "mdEditEnable": false
   },
   "source": [
    "## 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4fYxY_Eq-Tso",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XFQf4ikx5pp1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, df, vectorizer):\n",
    "        self.df = df\n",
    "        self.vectorizer = vectorizer\n",
    "\n",
    "        # 切分数据集\n",
    "        self.train_df = self.df[self.df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    "        self.val_df = self.df[self.df.split=='val']\n",
    "        self.val_size = len(self.val_df)\n",
    "        self.test_df = self.df[self.df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    "        self.lookup_dict = {'train': (self.train_df, self.train_size), \n",
    "                            'val': (self.val_df, self.val_size),\n",
    "                            'test': (self.test_df, self.test_size)}\n",
    "        self.set_split('train')\n",
    "\n",
    "        # 类权重(不平衡)\n",
    "        class_counts = df.nationality.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self.vectorizer.nationality_vocab.lookup_token(item[0])\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, split_data_file):\n",
    "        df = pd.read_csv(split_data_file, header=0)\n",
    "        train_df = df[df.split=='train']\n",
    "        return cls(df, SurnameVectorizer.from_dataframe(train_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, split_data_file, vectorizer_filepath):\n",
    "        df = pd.read_csv(split_data_file, header=0)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(df, vectorizer)\n",
    "\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self.vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self.target_split = split\n",
    "        self.target_df, self.target_size = self.lookup_dict[split]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Dataset(split={0}, size={1})\".format(\n",
    "            self.target_split, self.target_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.target_df.iloc[index]\n",
    "        surname_vector = self.vectorizer.vectorize(row.surname)\n",
    "        nationality_index = self.vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
    "        return {'surname': surname_vector, 'nationality': nationality_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size\n",
    "\n",
    "    def generate_batches(self, batch_size, shuffle=True, drop_last=True, device=args.device):\n",
    "        dataloader = DataLoader(dataset=self, batch_size=batch_size, \n",
    "                                shuffle=shuffle, drop_last=drop_last)\n",
    "        for data_dict in dataloader:\n",
    "            out_data_dict = {}\n",
    "            for name, tensor in data_dict.items():\n",
    "                out_data_dict[name] = data_dict[name].to(device)\n",
    "            yield out_data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "QhAJn2H3-vvu",
    "outputId": "a9aefac3-9827-4251-e099-ab22d12ecc62",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Dataset(split=train, size=7680)\n",
      "{'surname': array([0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), 'nationality': 0}\n",
      "tensor([0.0006, 0.0045, 0.0024, 0.0042, 0.0003, 0.0044, 0.0017, 0.0064, 0.0055,\n",
      "        0.0017, 0.0013, 0.0130, 0.0083, 0.0182, 0.0004, 0.0133, 0.0039, 0.0172])\n"
     ]
    }
   ],
   "source": [
    "# 数据集实例\n",
    "dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.split_data_file)\n",
    "print (dataset) # __str__\n",
    "print (dataset[5]) # __getitem__\n",
    "print (dataset.class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q8CAcVWRCVtm",
    "mdEditEnable": false
   },
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K4yDxHIe_hGv",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_bbJqIPRCbuZ",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class SurnameModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_p):\n",
    "        super(SurnameModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        z = F.relu(self.fc1(x_in))\n",
    "        z = self.dropout(z)\n",
    "        y_pred = self.fc2(z)\n",
    "\n",
    "        if apply_softmax:\n",
    "            y_pred = F.softmax(y_pred, dim=1)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p0Hr9OohDmPI",
    "mdEditEnable": false
   },
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UWYC2MfiKh8o",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TKlstCszC_PT",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, dataset, model, model_state_file, save_dir, device, shuffle, \n",
    "               num_epochs, batch_size, learning_rate, early_stopping_criteria):\n",
    "        self.dataset = dataset\n",
    "        self.class_weights = dataset.class_weights.to(device)\n",
    "        self.model = model.to(device)\n",
    "        self.save_dir = save_dir\n",
    "        self.device = device\n",
    "        self.shuffle = shuffle\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_func = nn.CrossEntropyLoss(self.class_weights)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer=self.optimizer, mode='min', factor=0.5, patience=1)\n",
    "        self.train_state = {\n",
    "            'stop_early': False, \n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'early_stopping_criteria': early_stopping_criteria,\n",
    "            'learning_rate': learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': model_state_file}\n",
    "    \n",
    "    def update_train_state(self):\n",
    "\n",
    "        # 打印checkpoint 信息\n",
    "        print (\"[EPOCH]: {0:02d} | [LR]: {1} | [TRAIN LOSS]: {2:.2f} | [TRAIN ACC]: {3:.1f}% | [VAL LOSS]: {4:.2f} | [VAL ACC]: {5:.1f}%\".format(\n",
    "          self.train_state['epoch_index'], self.train_state['learning_rate'], \n",
    "            self.train_state['train_loss'][-1], self.train_state['train_acc'][-1], \n",
    "            self.train_state['val_loss'][-1], self.train_state['val_acc'][-1]))\n",
    "\n",
    "        # 至少保存一次模型\n",
    "        if self.train_state['epoch_index'] == 0:\n",
    "            torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
    "            self.train_state['stop_early'] = False\n",
    "\n",
    "        # 如果模型性能表现有提升，再次保存\n",
    "        elif self.train_state['epoch_index'] >= 1:\n",
    "            loss_tm1, loss_t = self.train_state['val_loss'][-2:]\n",
    "\n",
    "            # 如果损失增大\n",
    "            if loss_t >= self.train_state['early_stopping_best_val']:\n",
    "                # 更新步数\n",
    "                self.train_state['early_stopping_step'] += 1\n",
    "\n",
    "            # 损失变小\n",
    "            else:\n",
    "                # 保存最优的模型\n",
    "                if loss_t < self.train_state['early_stopping_best_val']:\n",
    "                    torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
    "\n",
    "                # 重置early stopping 的步数\n",
    "                self.train_state['early_stopping_step'] = 0\n",
    "\n",
    "            # 是否需要early stopping ?\n",
    "            self.train_state['stop_early'] = self.train_state['early_stopping_step'] \\\n",
    "              >= self.train_state['early_stopping_criteria']\n",
    "        return self.train_state\n",
    "  \n",
    "    def compute_accuracy(self, y_pred, y_target):\n",
    "        _, y_pred_indices = y_pred.max(dim=1)\n",
    "        n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "        return n_correct / len(y_pred_indices) * 100\n",
    "  \n",
    "    def run_train_loop(self):\n",
    "        for epoch_index in range(self.num_epochs):\n",
    "            self.train_state['epoch_index'] = epoch_index\n",
    "      \n",
    "            # 遍历训练集\n",
    "            # 初始化批生成器, 将损失和准确率归零，设置为训练模式\n",
    "            self.dataset.set_split('train')\n",
    "            batch_generator = self.dataset.generate_batches(\n",
    "                batch_size=self.batch_size, shuffle=self.shuffle, \n",
    "                device=self.device)\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "            self.model.train()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "                # 梯度归零\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                # 计算输出\n",
    "                y_pred = self.model(batch_dict['surname'])\n",
    "\n",
    "                # 计算损失\n",
    "                loss = self.loss_func(y_pred, batch_dict['nationality'])\n",
    "                loss_t = loss.item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # 反向传播\n",
    "                loss.backward()\n",
    "\n",
    "                # 更新梯度\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                # 计算准确率\n",
    "                acc_t = self.compute_accuracy(y_pred, batch_dict['nationality'])\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            self.train_state['train_loss'].append(running_loss)\n",
    "            self.train_state['train_acc'].append(running_acc)\n",
    "\n",
    "            # 遍历 val 集上\n",
    "\n",
    "            # 初始化批生成器, 将损失和准确率归零，设置为训练模式\n",
    "            self.dataset.set_split('val')\n",
    "            batch_generator = self.dataset.generate_batches(\n",
    "                batch_size=self.batch_size, shuffle=self.shuffle, device=self.device)\n",
    "            running_loss = 0.\n",
    "            running_acc = 0.\n",
    "            self.model.eval()\n",
    "\n",
    "            for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "                # 计算输出\n",
    "                y_pred =  self.model(batch_dict['surname'])\n",
    "\n",
    "                # 计算损失\n",
    "                loss = self.loss_func(y_pred, batch_dict['nationality'])\n",
    "                loss_t = loss.to(args.device).item()\n",
    "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "                # 计算准确率\n",
    "                acc_t = self.compute_accuracy(y_pred, batch_dict['nationality'])\n",
    "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            self.train_state['val_loss'].append(running_loss)\n",
    "            self.train_state['val_acc'].append(running_acc)\n",
    "\n",
    "            self.train_state = self.update_train_state()\n",
    "            self.scheduler.step(self.train_state['val_loss'][-1])\n",
    "            if self.train_state['stop_early']:\n",
    "                break\n",
    "          \n",
    "    def run_test_loop(self):\n",
    "        # 初始化批生成器, 将损失和准确率归零，设置为训练模式\n",
    "        self.dataset.set_split('test')\n",
    "        batch_generator = self.dataset.generate_batches(\n",
    "            batch_size=self.batch_size, shuffle=self.shuffle, device=self.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        self.model.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # 计算输出\n",
    "            y_pred =  self.model(batch_dict['surname'])\n",
    "\n",
    "            # 计算损失\n",
    "            loss = self.loss_func(y_pred, batch_dict['nationality'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # 计算准确率\n",
    "            acc_t = self.compute_accuracy(y_pred, batch_dict['nationality'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "        self.train_state['test_loss'] = running_loss\n",
    "        self.train_state['test_acc'] = running_acc\n",
    "    \n",
    "    def plot_performance(self):\n",
    "        # 设置图大小\n",
    "        plt.figure(figsize=(15,5))\n",
    "\n",
    "        # 画出损失\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"Loss\")\n",
    "        plt.plot(trainer.train_state[\"train_loss\"], label=\"train\")\n",
    "        plt.plot(trainer.train_state[\"val_loss\"], label=\"val\")\n",
    "        plt.legend(loc='upper right')\n",
    "\n",
    "        # 画出准确率\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.plot(trainer.train_state[\"train_acc\"], label=\"train\")\n",
    "        plt.plot(trainer.train_state[\"val_acc\"], label=\"val\")\n",
    "        plt.legend(loc='lower right')\n",
    "\n",
    "        # 存图\n",
    "        plt.savefig(os.path.join(self.save_dir, \"performance.png\"))\n",
    "\n",
    "        # 展示图\n",
    "        plt.show()\n",
    "    \n",
    "    def save_train_state(self):\n",
    "        with open(os.path.join(self.save_dir, \"train_state.json\"), \"w\") as fp:\n",
    "            json.dump(self.train_state, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "O1_A24sGHslh",
    "outputId": "10f14aa7-0092-4096-b37d-adcee95a26d1",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.named_modules of SurnameModel(\n",
      "  (fc1): Linear(in_features=28, out_features=300, bias=True)\n",
      "  (dropout): Dropout(p=0.1)\n",
      "  (fc2): Linear(in_features=300, out_features=18, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# 初始化\n",
    "dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.split_data_file)\n",
    "dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.vectorizer\n",
    "model = SurnameModel(input_dim=len(vectorizer.surname_vocab), \n",
    "                     hidden_dim=args.hidden_dim, \n",
    "                     output_dim=len(vectorizer.nationality_vocab),\n",
    "                     dropout_p=args.dropout_p)\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "-5RrfYBpJFkg",
    "outputId": "54348c27-4452-42aa-befc-05ae3de66c46",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda\n",
      "[EPOCH]: 00 | [LR]: 0.001 | [TRAIN LOSS]: 1.56 | [TRAIN ACC]: 45.7% | [VAL LOSS]: 1.84 | [VAL ACC]: 42.4%\n",
      "[EPOCH]: 01 | [LR]: 0.001 | [TRAIN LOSS]: 1.54 | [TRAIN ACC]: 45.2% | [VAL LOSS]: 1.81 | [VAL ACC]: 41.6%\n",
      "[EPOCH]: 02 | [LR]: 0.001 | [TRAIN LOSS]: 1.52 | [TRAIN ACC]: 45.7% | [VAL LOSS]: 1.85 | [VAL ACC]: 41.6%\n",
      "[EPOCH]: 03 | [LR]: 0.001 | [TRAIN LOSS]: 1.49 | [TRAIN ACC]: 46.3% | [VAL LOSS]: 1.86 | [VAL ACC]: 40.8%\n",
      "[EPOCH]: 04 | [LR]: 0.001 | [TRAIN LOSS]: 1.46 | [TRAIN ACC]: 47.2% | [VAL LOSS]: 1.82 | [VAL ACC]: 42.3%\n",
      "[EPOCH]: 05 | [LR]: 0.001 | [TRAIN LOSS]: 1.45 | [TRAIN ACC]: 46.7% | [VAL LOSS]: 1.81 | [VAL ACC]: 42.9%\n",
      "[EPOCH]: 06 | [LR]: 0.001 | [TRAIN LOSS]: 1.43 | [TRAIN ACC]: 47.9% | [VAL LOSS]: 1.82 | [VAL ACC]: 43.3%\n",
      "[EPOCH]: 07 | [LR]: 0.001 | [TRAIN LOSS]: 1.44 | [TRAIN ACC]: 48.0% | [VAL LOSS]: 1.80 | [VAL ACC]: 43.3%\n",
      "[EPOCH]: 08 | [LR]: 0.001 | [TRAIN LOSS]: 1.42 | [TRAIN ACC]: 47.6% | [VAL LOSS]: 1.83 | [VAL ACC]: 42.2%\n",
      "[EPOCH]: 09 | [LR]: 0.001 | [TRAIN LOSS]: 1.42 | [TRAIN ACC]: 48.8% | [VAL LOSS]: 1.83 | [VAL ACC]: 42.7%\n",
      "[EPOCH]: 10 | [LR]: 0.001 | [TRAIN LOSS]: 1.41 | [TRAIN ACC]: 48.1% | [VAL LOSS]: 1.81 | [VAL ACC]: 43.2%\n",
      "[EPOCH]: 11 | [LR]: 0.001 | [TRAIN LOSS]: 1.41 | [TRAIN ACC]: 47.9% | [VAL LOSS]: 1.79 | [VAL ACC]: 43.8%\n",
      "[EPOCH]: 12 | [LR]: 0.001 | [TRAIN LOSS]: 1.40 | [TRAIN ACC]: 48.8% | [VAL LOSS]: 1.83 | [VAL ACC]: 42.6%\n",
      "[EPOCH]: 13 | [LR]: 0.001 | [TRAIN LOSS]: 1.41 | [TRAIN ACC]: 48.0% | [VAL LOSS]: 1.82 | [VAL ACC]: 43.1%\n",
      "[EPOCH]: 14 | [LR]: 0.001 | [TRAIN LOSS]: 1.40 | [TRAIN ACC]: 48.6% | [VAL LOSS]: 1.79 | [VAL ACC]: 43.1%\n",
      "[EPOCH]: 15 | [LR]: 0.001 | [TRAIN LOSS]: 1.41 | [TRAIN ACC]: 48.5% | [VAL LOSS]: 1.83 | [VAL ACC]: 43.7%\n",
      "[EPOCH]: 16 | [LR]: 0.001 | [TRAIN LOSS]: 1.40 | [TRAIN ACC]: 48.5% | [VAL LOSS]: 1.80 | [VAL ACC]: 43.2%\n",
      "[EPOCH]: 17 | [LR]: 0.001 | [TRAIN LOSS]: 1.39 | [TRAIN ACC]: 48.7% | [VAL LOSS]: 1.83 | [VAL ACC]: 43.3%\n",
      "[EPOCH]: 18 | [LR]: 0.001 | [TRAIN LOSS]: 1.39 | [TRAIN ACC]: 48.7% | [VAL LOSS]: 1.82 | [VAL ACC]: 43.4%\n",
      "[EPOCH]: 19 | [LR]: 0.001 | [TRAIN LOSS]: 1.39 | [TRAIN ACC]: 48.4% | [VAL LOSS]: 1.80 | [VAL ACC]: 43.5%\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "print(\"training on\", args.device)\n",
    "trainer = Trainer(dataset=dataset, model=model, \n",
    "                  model_state_file=args.model_state_file, \n",
    "                  save_dir=args.save_dir, device=args.device,\n",
    "                  shuffle=args.shuffle, num_epochs=args.num_epochs, \n",
    "                  batch_size=args.batch_size, learning_rate=args.learning_rate, \n",
    "                  early_stopping_criteria=args.early_stopping_criteria)\n",
    "trainer.run_train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "fgfpWoNRN_wu",
    "outputId": "91cf38b2-c040-4d9f-b596-bee9d1d41bc8",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://cdn.kesci.com/rt_upload/fgfpWoNRN_wu/pk2gqhylwa.png\">"
      ],
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 画出训练过程\n",
    "trainer.plot_performance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "RAQROqCgfcQM",
    "outputId": "ef6c7f89-50b8-4823-ab44-4203bafba3f0",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.89\n",
      "Test Accuracy: 42.8%\n"
     ]
    }
   ],
   "source": [
    "# 测试集上的性能\n",
    "trainer.run_test_loop()\n",
    "print(\"Test loss: {0:.2f}\".format(trainer.train_state['test_loss']))\n",
    "print(\"Test Accuracy: {0:.1f}%\".format(trainer.train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bT0hnmy2vS26",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 保存结果\n",
    "trainer.save_train_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EvWFdAz0jh4B",
    "mdEditEnable": false
   },
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "76p9UmpYjjp5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Inference(object):\n",
    "    def __init__(self, model, vectorizer):\n",
    "        self.model = model\n",
    "        self.vectorizer = vectorizer\n",
    "  \n",
    "    def predict_nationality(self, surname):\n",
    "        # 向前传播\n",
    "        vectorized_surname = torch.tensor(self.vectorizer.vectorize(surname)).view(1, -1)\n",
    "        self.model.eval()\n",
    "        y_pred = self.model(vectorized_surname, apply_softmax=True)\n",
    "\n",
    "        # 可能性最高的国籍\n",
    "        y_prob, indices = y_pred.max(dim=1)\n",
    "        index = indices.item()\n",
    "\n",
    "        # 预测出的国籍\n",
    "        nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
    "        probability = y_prob.item()\n",
    "        return {'nationality': nationality, 'probability': probability}\n",
    "  \n",
    "    def predict_top_k(self, surname, k):\n",
    "        # 向前传播\n",
    "        vectorized_surname = torch.tensor(self.vectorizer.vectorize(surname)).view(1, -1)\n",
    "        self.model.eval()\n",
    "        y_pred = self.model(vectorized_surname, apply_softmax=True)\n",
    "\n",
    "        # 最有可能的K个国籍\n",
    "        y_prob, indices = torch.topk(y_pred, k=k)\n",
    "        probabilities = y_prob.detach().numpy()[0]\n",
    "        indices = indices.detach().numpy()[0]\n",
    "\n",
    "        # 结果\n",
    "        results = []\n",
    "        for probability, index in zip(probabilities, indices):\n",
    "            nationality = self.vectorizer.nationality_vocab.lookup_index(index)\n",
    "            results.append({'nationality': nationality, 'probability': probability})\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "9zlIp2uJcYHM",
    "outputId": "844f4e9a-c565-48ab-f159-ccf487e4c348",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cuda\n",
      "<bound method Module.named_modules of SurnameModel(\n",
      "  (fc1): Linear(in_features=28, out_features=300, bias=True)\n",
      "  (dropout): Dropout(p=0.1)\n",
      "  (fc2): Linear(in_features=300, out_features=18, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "dataset = SurnameDataset.load_dataset_and_load_vectorizer(\n",
    "    args.split_data_file,args.vectorizer_file)\n",
    "vectorizer = dataset.vectorizer\n",
    "model = SurnameModel(input_dim=len(vectorizer.surname_vocab), \n",
    "                     hidden_dim=args.hidden_dim, \n",
    "                     output_dim=len(vectorizer.nationality_vocab),\n",
    "                     dropout_p=args.dropout_p)\n",
    "model.load_state_dict(torch.load(args.model_state_file))\n",
    "model = model.to(\"cpu\")\n",
    "print(\"running on device\", args.device)\n",
    "print(model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "7YND9nlvjjtK",
    "outputId": "ce62853e-623d-47cb-b951-43f0b11ebb86",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stream",
     "output_type": "stream",
     "text": [
      "Enter a surname to classify: : goku"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goku: Korean → p=0.40)\n"
     ]
    }
   ],
   "source": [
    "# 预测\n",
    "inference = Inference(model=model, vectorizer=vectorizer)\n",
    "surname = input(\"Enter a surname to classify: \")\n",
    "prediction = inference.predict_nationality(surname)\n",
    "print(\"{}: {} → p={:0.2f})\".format(surname, prediction['nationality'], \n",
    "                                   prediction['probability']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "8JXPtHs_pQEC",
    "outputId": "f7060ecc-7846-4846-b93b-3f269eb4c0c3",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goku: \n",
      "Korean → (p=0.40)\n",
      "Chinese → (p=0.18)\n",
      "Japanese → (p=0.17)\n"
     ]
    }
   ],
   "source": [
    "# 可能性最高的k个国籍\n",
    "top_k = inference.predict_top_k(surname, k=3)\n",
    "print (\"{}: \".format(surname))\n",
    "for result in top_k:\n",
    "    print (\"{} → (p={:0.2f})\".format(result['nationality'], \n",
    "                                     result['probability']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
